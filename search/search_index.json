{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>PREPOBS-BGC provides a set of modules to process and map biogeochemical data.</p>"},{"location":"#requirements","title":"Requirements","text":"<p>In order to execute the scripts of this project, It is necessary to have conda installed to be able to create and use the virtual environements needed.</p> How to install conda ? <p>Conda installing guide</p> <p>Having GNU Make installed can also simplify the project's setup.</p> How to install GNU Make ? UbuntuWindowsmacOS <p>More informations on installing GNU make for ubuntu systems.</p> <p>More informations on installing GNU make for windows systems.</p> <p>More informations on installing GNU make for macOS systems using Homebrew.</p>"},{"location":"#building-the-virtual-environment","title":"Building the virtual environment","text":"With makeWithout make <pre><code>make all\n</code></pre> <p><pre><code>conda env create --file environment.yml --prefix ./.venv\n</code></pre> <pre><code>conda activate ./.venv\n</code></pre> <pre><code>poetry install\n</code></pre></p> <p>More details on the virtual environment</p>"},{"location":"#configuration-files","title":"Configuration files","text":"<p>Each scripts has an associated configuration file to set up all necessary parameters. By default, these configuration don't exists but can be created from a 'default configuration' existing in config/default. If these copies don't exist, the following command will create the files:</p> With makeWithout make <pre><code>make copy-default-config\n</code></pre> <p>Manually copy/paste all scripts from config/default into config. Or use the following command: <pre><code>for name in config/default/*.toml; do cp config/default/$(basename ${name}) config/_$(basename ${name}) ; done\n</code></pre></p> <p>Before running any script, one has to verify that all parameters indicated in the configuration are relevant. For example, one has to fill in all <code>PATH</code> parameters in the <code>providers.toml</code> configuration file to indicate where the providers data can be find.</p>"},{"location":"#running-the-scripts","title":"Running the Scripts","text":"<p>To run a script from the folder <code>scripts</code> and named <code>script_1.py</code>, one can use the following command:</p> With makeWithout make <pre><code>make run-script-1 # (1)!\n</code></pre> <ol> <li>make will install the correct environment and run the script</li> </ol> <p>Any <code>.py</code> file located in the folder <code>./scripts/</code> can be run with a make rule starting by <code>run-</code> and ending with the name of the file (without the <code>.py</code> extension) and where all underscores ('_') ar replaced by hyphens ('-'). These rules are created dynamically so if a new script is added, there is no modification to apply to the Makefile to use the corresponding rule</p> <p>Virtual environment must have been built, see here <pre><code>conda activate ./.venv  # (1)!\n</code></pre></p> <ol> <li>Activate virtual environment</li> </ol> <pre><code>poetry install --without dev,docs  # (1)!\n</code></pre> <ol> <li>Install all required libraries</li> </ol> <pre><code>python scripts/script_1.py # (1)!\n</code></pre> <ol> <li>Run script.</li> </ol> <p>All the details about the scripts, their execution, their input parameters and their output can be found in the Scripts section of this documentation.</p>"},{"location":"#make-rules-cheatsheet","title":"Make rules cheatsheet","text":""},{"location":"#installation","title":"Installation","text":"<p>Main</p> <code>make all</code>  Create the environment, install main libraries and copy the configuration files (if needed).  <code>make copy-default-config</code>  Copy default configuration files to the config folder if default files have been modified or if the configuration file doesn't exist.  <code>make pre-commit</code>  Install git pre-commit hooks to ensure that the code meets editing standards before committing to github.  <p>Development</p> <code>make install-dev</code>  Install the environment as <code>make all</code> does with additional development libraries and installs git hooks to ensure that the code meets editing standards before committing to github.  <code>make hooks</code>  Install git hooks to ensure that the code meets editing standards before committing to github."},{"location":"#cleaning","title":"Cleaning","text":"<code>make clean</code>  'Clean' the repository environment: remove virtual environment folder and git hooks.  <code>make clean-dirs</code>  'Clean' the outputs: remove both bgc_fig and bgc_data directories if existing."},{"location":"#documentation","title":"Documentation","text":"<code>make view-docs</code>  Create the environment, install documentation-related libraries and build the documentation website locally. The documentation is then accessible from a browser at the <code>localhost:8000</code> adress. See MkDocs documentation on <code>mkdocs serve</code> for more informations.  <code>make build-docs</code>  Create the environment, install documentation-related libraries and build the documentation website into the 'site' folder. See MkDocs documentation on <code>mkdocs build</code> for more informations.  <code>make deploy-docs</code>  Create the environment, install documentation-related libraries and deploy documentation to a github branch. See MkDocs documentation on <code>mkdocs deploy</code> for more informations."},{"location":"#running-scripts","title":"Running scripts","text":"<code>make run-any-script</code>  Create the environment, install scripts-running-related libraries and runs the <code>scripts/any_script.py</code> python script. 'any-script' can be replaced by anything in order to run a script. For example, calling <code>make run-another-script</code> will run the <code>scripts/another_script.py</code> python script (if it exists). To make this rule work, the following syntax must be respected: <ol> <li>script must be a python script</li> <li>script must be in the folder <code>scripts</code></li> <li>underscores ('_') must be replaced by hyphens ('-') in the script name</li> <li>extension ('.py') must be removed from the script's name</li> <li>rule must start with the `run-` prefix</li> </ol>"},{"location":"contributing/","title":"How to contribute to this project ?","text":"<p>A few precautions must be taken when contributing to this project.</p>"},{"location":"contributing/#adding-a-new-variable","title":"Adding a new variable","text":"<p>In order to register a new variable, one must create the variable entry in the <code>config/variables.toml</code> configuration file. In order for this addition to be permanent, the change must be done as well in the <code>config/default/variables.toml</code> file since <code>config/variables.toml</code> is only local. Once the variable is created, one must manually add this variable to all the loaders defined in every file of <code>providers</code>. The variable template is automatically loaded in the <code>VARS</code> dictionnary if it has been properly defined in <code>config/variables.toml</code>.</p>"},{"location":"contributing/#example","title":"Example","text":"<p>CMEMS's orginal loader's definition: src/bgc_data_processing/providers/cmems.py<pre><code>\"\"\"Specific parameters to load CMEMS-provided data.\"\"\"\n\nfrom pathlib import Path\n\nimport numpy as np\n\nfrom bgc_data_processing import units\nfrom bgc_data_processing.core.sources import DataSource\nfrom bgc_data_processing.core.variables.sets import SourceVariableSet\nfrom bgc_data_processing.defaults import PROVIDERS_CONFIG, VARS\nfrom bgc_data_processing.utils.patterns import FileNamePattern\n\nloader = DataSource(\n    provider_name=\"CMEMS\",\n    data_format=\"netcdf\",\n    dirin=Path(PROVIDERS_CONFIG[\"CMEMS\"][\"PATH\"]),\n    data_category=PROVIDERS_CONFIG[\"CMEMS\"][\"CATEGORY\"],\n    excluded_files=PROVIDERS_CONFIG[\"CMEMS\"][\"EXCLUDE\"],\n    files_pattern=FileNamePattern(\".*.nc\"),\n    variable_ensemble=SourceVariableSet(\n        provider=VARS[\"provider\"].not_in_file(),\n        expocode=VARS[\"expocode\"].not_in_file(),\n        date=VARS[\"date\"].in_file_as(\"TIME\"),\n        year=VARS[\"year\"].not_in_file(),\n        month=VARS[\"month\"].not_in_file(),\n        day=VARS[\"day\"].not_in_file(),\n        hour=VARS[\"hour\"].not_in_file(),\n        longitude=VARS[\"longitude\"].in_file_as(\"LONGITUDE\"),\n        latitude=VARS[\"latitude\"].in_file_as(\"LATITUDE\"),\n        depth=VARS[\"depth\"]\n        .in_file_as(\"DEPH\", \"PRES\")\n        .remove_when_nan()\n        .correct_with(lambda x: -np.abs(x)),\n        temperature=VARS[\"temperature\"].in_file_as((\"TEMP\", \"TEMP_QC\", [1])),\n        salinity=VARS[\"salinity\"].in_file_as((\"PSAL\", \"PSL_QC\", [1])),\n        oxygen=VARS[\"oxygen\"]\n        .in_file_as(\"DOX1\")\n        .correct_with(units.convert_doxy_ml_by_l_to_mmol_by_m3),\n        phosphate=VARS[\"phosphate\"]\n        .in_file_as((\"PHOS\", \"PHOS_QC\", [1]))\n        .remove_when_all_nan(),\n        nitrate=VARS[\"nitrate\"]\n        .in_file_as((\"NTRA\", \"NTRA_QC\", [1]))\n        .remove_when_all_nan(),\n        silicate=VARS[\"silicate\"]\n        .in_file_as((\"SLCA\", \"SLCA_QC\", [1]))\n        .remove_when_all_nan(),\n        chlorophyll=VARS[\"chlorophyll\"]\n        .in_file_as((\"CPHL\", \"CPHL_QC\", [1]))\n        .remove_when_all_nan(),\n    ),\n)\n</code></pre></p> <p>Creating the new variable 'carbon': config/variables.toml<pre><code>...\n[carbon]\n#? carbon.NAME: str: variable name\nNAME = \"CARB\"\n#? carbon.UNIT: str: variable unit\nUNIT = \"[umol/l]\"\n#? carbon.TYPE: str: variable type (among ['int', 'float', 'str', 'datetime64[ns]'])\nTYPE = \"float\"\n#? carbon.DEFAULT: str | int | float: default variable value if nan or not existing\nDEFAULT = nan\n#? carbon.NAME_FORMAT: str: format to use to save the name and unit of the variable as text\nNAME_FORMAT = \"%-10s\"\n#? carbon.VALUE_FORMAT: str: format to use to save the values of the variable as text\nVALUE_FORMAT = \"%10.3f\"\n</code></pre></p> <p>Updating the loader by adding the variable (supposedly not in the file here):</p> bgc_data_processing/providers/cmems.py<pre><code>\"\"\"Specific parameters to load CMEMS-provided data.\"\"\"\nfrom pathlib import Path\n\nimport numpy as np\n\nfrom bgc_data_processing import units\nfrom bgc_data_processing.core.sources import DataSource\nfrom bgc_data_processing.core.variables.sets import SourceVariableSet\nfrom bgc_data_processing.defaults import PROVIDERS_CONFIG, VARS\nfrom bgc_data_processing.utils.patterns import FileNamePattern\n\nloader = DataSource(\n    provider_name=\"CMEMS\",\n    data_format=\"netcdf\",\n    dirin=Path(PROVIDERS_CONFIG[\"CMEMS\"][\"PATH\"]),\n    data_category=PROVIDERS_CONFIG[\"CMEMS\"][\"CATEGORY\"],\n    excluded_files=PROVIDERS_CONFIG[\"CMEMS\"][\"EXCLUDE\"],\n    files_pattern=FileNamePattern(\".*.nc\"),\n    variable_ensemble=SourceVariableSet(\n        provider=VARS[\"provider\"].not_in_file(),\n        expocode=VARS[\"expocode\"].not_in_file(),\n        date=VARS[\"date\"].in_file_as(\"TIME\"),\n        year=VARS[\"year\"].not_in_file(),\n        month=VARS[\"month\"].not_in_file(),\n        day=VARS[\"day\"].not_in_file(),\n        hour=VARS[\"hour\"].not_in_file(),\n        longitude=VARS[\"longitude\"].in_file_as(\"LONGITUDE\"),\n        latitude=VARS[\"latitude\"].in_file_as(\"LATITUDE\"),\n        depth=VARS[\"depth\"]\n        .in_file_as(\"DEPH\", \"PRES\")\n        .remove_when_nan()\n        .correct_with(lambda x: -np.abs(x)),\n        temperature=VARS[\"temperature\"].in_file_as((\"TEMP\", \"TEMP_QC\", [1])),\n        salinity=VARS[\"salinity\"].in_file_as((\"PSAL\", \"PSL_QC\", [1])),\n        oxygen=VARS[\"oxygen\"]\n        .in_file_as(\"DOX1\")\n        .correct_with(units.convert_doxy_ml_by_l_to_mmol_by_m3),\n        phosphate=VARS[\"phosphate\"]\n        .in_file_as((\"PHOS\", \"PHOS_QC\", [1]))\n        .remove_when_all_nan(),\n        nitrate=VARS[\"nitrate\"]\n        .in_file_as((\"NTRA\", \"NTRA_QC\", [1]))\n        .remove_when_all_nan(),\n        silicate=VARS[\"silicate\"]\n        .in_file_as((\"SLCA\", \"SLCA_QC\", [1]))\n        .remove_when_all_nan(),\n        chlorophyll=VARS[\"chlorophyll\"]\n        .in_file_as((\"CPHL\", \"CPHL_QC\", [1]))\n        .remove_when_all_nan(),\ncarbon=DEFAULT_VARS[\"carbon\"].not_in_file(), # (1)!\n),\n)\n</code></pre> <ol> <li>Additional row to use the 'carbon' variable</li> </ol> <p>Warning</p> <p>The new variable must be defined in every loader's definition file.</p>"},{"location":"contributing/#adding-a-new-provider","title":"Adding a new provider","text":"<p>In order to register a new provider, one must be create a new entry in the <code>config/providers.toml</code> configuration file. In order for this addition to be permanent, the change must be done as well in the <code>config/default/providers.toml</code> file since <code>config/providers.toml</code> is only local. Once the entry is created, one must manually create a file to define this provider's loader in <code>providers</code>. All the available variables must be properly defined in the loader's VariablesStorer (proper names, correction functions, flag informations...).</p>"},{"location":"contributing/#example_1","title":"Example","text":"<p>Creating a new provider entry:</p> config/providers.toml<pre><code>...\n[BGC_PROVIDER]\n#? BGC_PROVIDER.PATH: str: path to the folder containing the data\nPATH = \"/path/to/data/directory\"\n#? BGC_PROVIDER.CATEGORY: str: data category, either 'float' or 'in_situ'\nCATEGORY = \"in_situ\"\n#? BGC_PROVIDER.EXCLUDE: list[str]: files to exclude from loading\nEXCLUDE = []\n</code></pre> <p>Creating a new file in <code>providers</code> :</p> bgc_data_processing/providers/bgc_provider.py<pre><code>\"\"\"Specific parameters to load BGC_PROVIDER-provided data.\"\"\"\nfrom pathlib import Path\n\nfrom bgc_data_processing.core.sources import DataSource\nfrom bgc_data_processing.core.variables.sets import SourceVariableSet\nfrom bgc_data_processing.defaults import PROVIDERS_CONFIG, VARS\nfrom bgc_data_processing.utils.patterns import FileNamePattern\n\nloader = DataSource(\n    provider_name=\"BGC_PROVIDER\",\n    data_format=\"csv\",\n    dirin=PROVIDERS_CONFIG[\"BGC_PROVIDER\"][\"PATH\"],\n    category=PROVIDERS_CONFIG[\"BGC_PROVIDER\"][\"CATEGORY\"],\n    excluded_files=PROVIDERS_CONFIG[\"BGC_PROVIDER\"][\"EXCLUDE\"],\n    files_pattern=FileNamePattern(\".*.csv\"),                                             # (1)!\n    variables=SourceVariableSet(\n        provider=VARS[\"provider\"].in_file_as(\"provider\"),\n        expocode=VARS[\"expocode\"].not_in_file(),\n        date=VARS[\"date\"].in_file_as(\"time\"),\n        year=VARS[\"year\"].not_in_file(),\n        month=VARS[\"month\"].not_in_file(),\n        day=VARS[\"day\"].not_in_file(),\n        hour=VARS[\"hour\"].not_in_file(),\n        longitude=VARS[\"longitude\"].in_file_as(\"longitude\"),\n        latitude=VARS[\"latitude\"].in_file_as(\"latitude\"),\n        depth=VARS[\"depth\"]\n        .in_file_as(\"DEPH\")\n        .remove_when_nan(),\n        temperature=VARS[\"temperature\"].in_file_as(\"temperature\"),\n        salinity=VARS[\"salinity\"].in_file_as(\"salinity\"),\n        oxygen=VARS[\"oxygen\"].in_file_as(\"doxygen\"),\n        phosphate=VARS[\"phosphate\"]\n        .in_file_as((\"PHOS\", \"PHOS_QC\", [1]))\n        .remove_when_all_nan(),\n        nitrate=VARS[\"nitrate\"]\n        .in_file_as((\"NTRA\", \"NTRA_QC\", [1]))\n        .remove_when_all_nan(),\n        silicate=VARS[\"silicate\"]\n        .in_file_as((\"SLCA\", \"SLCA_QC\", [1]))\n        .remove_when_all_nan(),\n        chlorophyll=VARS[\"chlorophyll\"]\n        .in_file_as((\"CPHL\", \"CPHL_QC\", [1]))\n        .remove_when_all_nan(),\n    ),\n)\n</code></pre> <ol> <li>File pattern must be updated as well if possible</li> </ol>"},{"location":"examples/","title":"Examples","text":"<p>More examples on how to use this library.</p>"},{"location":"examples/#manually-setting-up-boundaries","title":"Manually setting up boundaries","text":"<p>See example script here</p>"},{"location":"examples/#mapping-data-stored-in-files","title":"Mapping data stored in files","text":"<p>See example script here</p>"},{"location":"examples/boundaries/","title":"Defining Boundaries","text":"<p>Loading script to load year, latitude, longitude, phosphate and nitrate variables from 2 providers, 'provider1' and 'provider2'. Phosphate variable is not measured by provider1 and nitrate is not measured by provider2.  Therefore, template are created to store basic informations on variables and are then instanciated in order to create relevant ExistingVar or NotExistingVar depending on provider.  Finally, latitude and longitude are applied in order to load the data only on a certain area. These are define using through the <code>Constraints</code> objects. Passing a Constraints object to the data source's <code>load_all</code> magic method will load the constraints to apply to the object and apply them when loading (or plotting) the data.</p> <pre><code>import datetime as dt\n\nimport bgc_data_processing as bgc_dp\n\n# Boundaries definition\nlatitude_min = 50\nlatitude_max = 89\nlongitude_min = -40\nlongitude_max = 40\n# Variables definition\nyear_var = bgc_dp.variables.TemplateVar(\n    name = \"YEAR\",\n    unit = \"[]\",\n    var_type = int,\n    name_format = \"%-4s\",\n    value_format = \"%4f\",\n)\nlatitude_var = bgc_dp.variables.TemplateVar(\n    name = \"LATITUDE\",\n    unit = \"[deg_N]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\nlongitude_var = bgc_dp.variables.TemplateVar(\n    name = \"LONGITUDE\",\n    unit = \"[deg_E]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\nphos_var = bgc_dp.variables.TemplateVar(\n    name=\"PHOS\",\n    unit=\"[umol/l]\",\n    var_type=float,\n    name_format=\"%-10s\",\n    value_format=\"%10.3f\",\n)\nntra_var = bgc_dp.variables.TemplateVar(\n    name=\"NTRA\",\n    unit=\"[umol/l]\",\n    var_type=float,\n    name_format=\"%-10s\",\n    value_format=\"%10.3f\",\n)\n# loaders definition\ndata_source1 = bgc_dp.DataSource(\n    provider_name=\"provider1\",\n    data_format=\"csv\",\n    dirin=\"~/provider1/data\",\n    category=\"in_situ\",\n    excluded_files=[],\n    files_pattern=bgc_dp.utils.patterns.FileNamePattern(\"prov1_data_{years}.csv\"),\n    variables=bgc_dp.SourceVariableSet(\n        year=year_var.in_file_as((\"year\",None,None)).remove_when_nan(),\n        latitude=latitude_var.in_file_as((\"lat\",None,None)),\n        longitude=longitude_var.in_file_as((\"lon\",None,None)),\n        phophate=phos_var.not_in_file(),\n        ...                                                            # (1)!\n        nitrate=ntra_var.in_file_as((\"ntra\",None,None)),\n    )\n)\ndata_source2 = bgc_dp.DataSource(\n    provider_name=\"provider2\",\n    data_format=\"csv\",\n    dirin=\"~/provider2/data\",\n    data_category=\"in_situ\",\n    excluded_files=[],\n    files_pattern=bgc_dp.utils.patterns.FileNamePattern(\"data_{years}.csv\"),\n    variables=bgc_dp.SourceVariableSet(\n        year=year_var.in_file_as((\"year\",None,None)).remove_when_nan(),\n        latitude=latitude_var.in_file_as((\"latitude\",None,None)),\n        longitude=longitude_var.in_file_as((\"longitude\",None,None)),\n        ...                                                             # (2)!\n        phosphate=phos_var.in_file_as((\"phosphate\",None,None)),\n        nitrate=ntra_var.not_in_file(),\n    )\n)\n# apply boundaries\nstorers = []\nfor loader in [data_source1, data_source2]:\n    variables = loader.variables\n    constraints = bgc_dp.Constraints()\n    constraints.add_boundary_constraint(\n        field_label=variables.get(variables.latitude_var_name).label,\n        minimal_value=LATITUDE_MIN,\n        maximal_value=LATITUDE_MAX,\n    )\n    constraints.add_boundary_constraint(\n        field_label=variables.get(variables.longitude_var_name).label,\n        minimal_value=LONGITUDE_MIN,\n        maximal_value=LONGITUDE_MAX,\n    )\n    loader.set_longitude_boundaries(\n        longitude_min=longitude_min,\n        longitude_max=longitude_max,\n    )\n    loader.set_latitude_boundaries(\n        latitude_min=latitude_min,\n        latitude_max=latitude_max,\n    )\n    storer = loader.load_all(constraints=constraints)\n    storers.append(storer)\n# Aggregation\naggregated_storer = sum(storers)\n</code></pre> <ol> <li>This is just an example script to show the expected structure of a script file, some mandatory variables are missing to initialize the SourceVariableSet, such as 'provider', 'expocode', 'date', 'month', 'day', 'hour' and 'depth'.</li> <li>This is just an example script to show the expected structure of a script file, some mandatory variables are missing to initialize the SourceVariableSet, such as 'provider', 'expocode', 'date', 'month', 'day', 'hour' and 'depth'.</li> </ol> <p>It is also possible to use <code>Constraints</code> objects as argument when creating a plot. The plot will then follow the constraints defined in the object. Using Constraints object with plotting method allows to load a large dataset once and for all and then only plotting slices of this dataset.</p>"},{"location":"examples/plot/","title":"Density Map","text":"<p>Example script to create a density map of the data. The data has previously been saved in the files <code>\"data1.txt\"</code>, <code>\"data2.txt\"</code>, <code>\"data3.txt\"</code>, <code>\"data4.txt\"</code> and <code>\"data5.txt\"</code> and can be loaded from these files.</p> <p>In order to plot the data density, we use the <code>DensityPlotter</code>.</p> <pre><code>from pathlib import Path\n\nimport bgc_data_processing as bgc_dp\n\nfiles = [\n    Path(\"path/to/data1.txt\"),\n    Path(\"path/to/data2.txt\"),\n    Path(\"path/to/data3.txt\"),\n    Path(\"path/to/data4.txt\"),\n    Path(\"path/to/data5.txt\"),\n]\n# Files Loading\nstorer = bgc_dp.io.read_files(\n    filepath=files,\n    providers_column_label = \"PROVIDER\",\n    expocode_column_label = \"EXPOCODE\",\n    date_column_label = \"DATE\",\n    year_column_label = \"YEAR\",\n    month_column_label = \"MONTH\",\n    day_column_label = \"DAY\",\n    hour_column_label = \"HOUR\",\n    latitude_column_label = \"LATITUDE\",\n    longitude_column_label = \"LONGITUDE\",\n    depth_column_label = \"DEPH\",\n    category=\"in_situ\",\n    unit_row_index=1,\n    delim_whitespace=False,\n)\n# Constraints\nconstraints = bgc_dp.Constraints()            # (1)!\n# Mapping\nmesh = bgc_dp.tracers.DensityPlotter(storer, constraints=constraints)\nmesh.set_bin_size(bins_size=[0.5,1.5])\nmesh.show(\n    variable_name=\"PHOS\",\n    title=\"Phosphate data density\",\n)\n</code></pre> <ol> <li>No constraint defined for this example</li> </ol>"},{"location":"how_it_works/","title":"How does it work ?","text":"<p>This project loads data from different providers, standardizes the data and saves (or maps) the resulting data.</p> <p>In order to do so, one needs to follow 4 major steps :</p>"},{"location":"how_it_works/#defining-variables","title":"Defining variables","text":"<p>Variable objects save the meta data for data variables. It contains informations about a variable's name, unit, storing type in the output dataframe. These objects can also be instanciated to 'fit' a proper source.  For example, one can specify a particular alias under which the variable is stored in the source data, flag columns and values to use to filter the source data, a particular function to correct the data from the source...</p> <p>Defining a variable existing in the source data: </p> <pre><code>import bgc_data_processing as bgc_dp\n\nvariable = bgc_dp.variables.ExistingVar(\n    name=\"LONGITUDE\",                           # (1)\n    unit=\"[deg_E]\",                             # (2)\n    var_type=float,                             # (3)\n    name_format=\"%-12s\",                        # (4)\n    value_format=\"%12.6f\",                      # (5)\n).set_aliases((\"Longitude\", \"longitudef\", [1])) # (6)\n</code></pre> <ol> <li>Name of the variable (can be different from its name in the dataset).</li> <li>Unit of the variable, as one wants it to appear when saving.</li> <li>Data type, used to convert types in the dataframe.</li> <li>Format string to use to format the label and the unit of the variable when saving.</li> <li>Format string to use to format the values of the variable when saving.</li> <li>Sets the Aliases list to the given args where each element is a tuple containing:<ul> <li>alias: variable name in the source data</li> <li>flag alias: variable flag name in the source data</li> <li>flag correct value: list of values to keep from the flag column</li> </ul> </li> </ol> <p>More informations on variables</p>"},{"location":"how_it_works/#loading-the-data","title":"Loading the data","text":"<p>In order to load the data from a provider, one must instanciate a <code>DataSource</code>. This data source instanciate a loader which correspond to the format of the data of the source (CSV, NetCDF or abfile).  This data source object contains all the informations on the provider (name, files location, required variables stored in a variable storing object).</p> <p>Defining a DataSource for GLODAPv2.2022 :</p> <pre><code>import bgc_data_processing as bgc_dp\n\nvariables = bgc_dp.SourceVariableSet(\nlongitude=longitude,                                                        # (1)!\nlatitude=latitude,                                                          # (2)!\n...                                                                         # (3)!\n)\ndsource = bgc_dp.DataSource(\n    provider_name=\"GLODAP_2022\",                                            # (4)!\n    data_format=\"csv\",                                                      # (5)!\n    dirin=\"path/to/file/directory\",                                         # (6)!\n    data_category=\"in_situ\",                                                # (7)!\n    excluded_files=[],                                                      # (8)!\n    files_pattern=bgc_dp.utils.patterns.FileNamepattern(\"glodap_2022.csv\"), # (9)!\n    variable_ensemble=variables,                                            # (10)!\n    read_params={\n        \"low_memory\": False,\n        \"index_col\": False,\n        \"na_values\": -9999,\n    },                                                                      # (11)!\n)\nstorer = dsource.load_all()                                                 # (12)!\n</code></pre> <ol> <li>Variable object of type ExistingVar or NotExistingVar referring to longitude variable.</li> <li>Variable object of type ExistingVar or NotExistingVar referring to latitude variable.</li> <li>his is just an example script to show the expected structure of a script file, some mandatory variables are missing to initialize the SourceVariableSet, such as 'provider', 'expocode', 'date', 'month', 'day', 'hour' and 'depth'.</li> <li>Name of the data provider.</li> <li>Format of the provided data.</li> <li>Path to the directory containing the files to load.</li> <li>The category of the provider, can be 'in_situ' or 'float'.</li> <li>Filenames to exclude when loading the data.</li> <li>Files pattern, only the files matching the pattern will be loaded. If the strings <code>'{years}'</code>, <code>'{months}'</code> or <code>'{days}'</code> are included, they will be replaced by the dates to load. For example:  if the pattern is \"glodap_{years}.csv\" and the years to load are 2007 and 2008, only the files matching the regex \"glodap_(2007|2008).csv\" will be loaded.</li> <li>Variables to load (if the variables are not in the data source, the column will still be created)</li> <li>Additionnal parameter passed to pd.read_csv</li> <li>The load_all method from the loader will then load the data and return a <code>Storer</code> containing the resulting dataframe.</li> </ol> <p>More informations on loading</p>"},{"location":"how_it_works/#aggregating-the-data","title":"Aggregating the data","text":"<p>Once data has been loaded from some providers, the aggregation of the resulting storers can be done using the <code>+</code> operator. However, in order for the aggregation to work, all storer must have similar variables (to concatenates the data) and same category (category-different storers can't be aggregated together).  Then, in order to save a storer, one has to use a <code>StorerSaver</code>.</p> <pre><code>storer_glodap = dsource_glodap.load_all()                   # (1)!\nstorer_imr = dsource_imr.load_all()                         # (2)!\n# Aggregation\naggregated_storer = storer_glodap + storer_imr              # (3)!\n# Saving\nsaver = StorerSaver(aggregated_storer)\nsaver.save_all_storer(Path(\"path/to/save/file.txt\"))        # (4)!\n</code></pre> <ol> <li>Loader for GLODAP 2022.</li> <li>Loader for IMR.</li> <li>Summing both storer returns the aggregation of them.</li> <li>Calling the .save_all_storer method to save the entire storer.</li> </ol>"},{"location":"how_it_works/#plotting-the-data","title":"Plotting the data","text":"<p>To plot the data, one has to create a <code>DensityPlotter</code> (to create 2D Mesh) and then call its <code>.show</code> method. To save the data, one has to use the <code>.save</code> method.</p> <pre><code>import bgc_data_processing.tracers as bgc_dp\n\nmesher = bgc_dp.tracers.DensityPlotter(storer)                # (1)!\nmesher.set_bins_soze(bins_size=[0.1, 0.2])  # (2)!\nmesher.set-geographic_boundaries(\n    latitude_min = 50,\n    latitude_max = 90,\n    longitude_min = -40,\n    longitude_max = 40,\n)\nmesher.plot(\n    variable_name=\"CPHL\",                   # (3)!\n    title=\"some title\",                     # (4)!\n    suptitle=\"some suptitle\",               # (5)!\n)\nmesher.save(\n    save_path=\"path/to/figure\"              # (6)!\n    variable_name=\"CPHL\",                   # (7)!\n    title=\"some title\",                     # (8)!\n    suptitle=\"some suptitle\",               # (9)!\n)\n</code></pre> <ol> <li>Storer object to map the data of.</li> <li>Size of the binning square (latitude, longitude)</li> <li>Name of the variable to plot on the map.</li> <li>Title for the plot.</li> <li>Suptitle for the plot.</li> <li>Path to the saving location.</li> <li>Name of the variable to plot on the map.</li> <li>Title for the plot.</li> <li>Suptitle for the plot.</li> </ol> <p>More informations on plotting</p>"},{"location":"how_it_works/closest_point_find/","title":"Match Simulation Points to Observations","text":"<p>Simulation Data (from HYCOM model) is provided on a 3D grid. The observation datapoints can be anywhere within this grid. Therefore, to compare observations and simulations, one must match observation datapoints to their \"corresponding\" simulation point.</p>"},{"location":"how_it_works/closest_point_find/#defining-closest-point-value","title":"Defining Closest Point Value","text":"<ol> <li> <p>Finding the Closest point on a the 2D grid:</p> <p>Since HYCOM output file are provided with a \"Grid File\", a file containing only latitude and longitude coordinates of all simulated points. Using this values, it is possible to find the closest simulated point to a given observed point (only considering latitude and longitude). This is done using scikit-learn's NearestNeighbor Algorithm with the following parameters:</p> <ul> <li><code>n_neighbors=1</code> -&gt; Only the closest point is required</li> <li><code>metric = \"haversine\"</code> -&gt; Great-circle distance between two points on a sphere</li> </ul> </li> <li> <p>Loading all simulated point at this location for the given date</p> </li> <li> <p>Finding the right value in depth:</p> <p>Instead of finding the closest depth value within the avalaible levels, the simulated point values are interpolated from water profile.</p> </li> </ol> <p>Therefore, in the end, the \"closest point\" is the point which latitude and longitude are the closest to the observation (relative to the Haversine metric) and values have been interpolated to match the observation's depth.</p>"},{"location":"how_it_works/closest_point_find/#illustration","title":"Illustration","text":"<p>Closest points selected from a set of points (grey points on the figure), match with their corresponding 'observation'. Observations are triangles facing the right while simulations are triangles facing the left.</p> <p>Same color observations and simulations are supposed to represent a simulation-observation pair. However, since the amount of color is finite for a better visual interpretation, some colors might be duplicated.</p> <p></p>"},{"location":"how_it_works/loading/","title":"Loading","text":"<p>In order to load the data from its sources (providers csv or netcdf files, already processed files), one can use DataSource objects. Basically, the data sources are initializated with all necessary infomations on providers, files locations and variables and only calling the <code>load_all</code> method is needed to load the data and return a storer. </p>"},{"location":"how_it_works/loading/#loading-from-providers-data","title":"Loading from providers data","text":"<p>When loading from a provider, the following arguments must be given to the DataSource at least:</p> <ul> <li>The name of the provider: <code>provider_name</code></li> <li>The format of the data: <code>data_format</code></li> <li>The directory containing the files to load: <code>dirin</code></li> <li>The category of the data ('float' or 'in_situ'): <code>data_category</code></li> <li>The pattern of the names of the files to read (a <code>FileNamePattern</code>): <code>files_pattern</code></li> <li>The <code>SourceVariableSet</code> object containing all variables: <code>variable_ensemble</code></li> </ul> <p>Behind the hood, theses <code>DataSource</code> object instantiate loaders, all deriving from <code>BaseLoader</code>. All loaders are file-format specific, meaning that at least one specific subclass of <code>Baseloader</code> exists for every supported data format.</p> CSVNetCDFAB files <p>Loader for CSV files uses the <code>read_params</code> additional argument to pass specific argument to pandas.read_csv</p> <pre><code>from bgc_data_processing.core.loaders.csv_loaders import CSVLoader\n\nloader = CSVLoader(\n    provider_name=\"GLODAP_2022\",\n    category=\"in_situ\",\n    exclude=[],\n    variables=variables,                        # (1)\n    read_params={\n        \"low_memory\": False,\n        \"index_col\": False,\n        \"na_values\": -9999,\n    },\n)\n</code></pre> <ol> <li>Pre-set <code>SourceVariableSet</code> object</li> </ol> <p>Loader for NetCDF files.</p> <pre><code>from bgc_data_processing.loaders.netcdf_loaders import NetCDFLoader\n\nloader = NetCDFLoader(\n    provider_name=\"ARGO\",\n    category=\"float\",\n    exclude=[],\n    variables=variables,                        # (1)\n)\n</code></pre> <ol> <li>Pre-set VariablesStorer object</li> </ol> <p>Loader for AB files uses the <code>grid_basename</code> additional argument to get the regional grid for the AB file. The AB files are handled using a library developped by the Nansen Environmental and remote Sensing center. Ths library is included into the src folder and therefore will not be improved / impacted by any change that could occur in the original repository.</p> <pre><code>from bgc_data_processing.loaders.abfile_loaders import ABFileLoader\n\nloader = ABFileLoader(\n    provider_name=\"HYCOM\",\n    category=\"float\",\n    exclude=[],\n    variables=variables,                        # (1)\n    grid_basename=\"regional.grid\"\n)\n</code></pre> <ol> <li>Pre-set VariablesStorer object</li> </ol> <p>Once the data source is set, it is simple to get the corresponding storer :</p> <pre><code>storer = dsource.load_all()\n</code></pre>"},{"location":"how_it_works/loading/#loading-from-already-processed-file","title":"Loading from already processed file","text":"<p>It is also possible to load data from files which have saved using the <code>read_files</code> function:</p> <pre><code>from pathlib import Path\nimport bgc_data_processing as bgc_dp\n\nstorer = bgc_dp.io.read_files(\n    filepath = [Path(\"file1.txt\"), Path(\"file2.txt\")],      # (1)!\n    providers_column_label = \"PROVIDER\",                    # (2)!\n    expocode_column_label = \"EXPOCODE\",                     # (3)!\n    date_column_label = \"DATE\",                             # (4)!\n    year_column_label = \"YEAR\",                             # (5)!\n    month_column_label = \"MONTH\",                           # (6)!\n    day_column_label = \"DAY\",                               # (7)!\n    hour_column_label = \"HOUR\",                             # (8)!\n    latitude_column_label = \"LATITUDE\",                     # (9)!\n    longitude_column_label = \"LONGITUDE\",                   # (10)!\n    depth_column_label = \"DEPH\",                            # (11)!\n    category = \"in_situ\",                                   # (12)!\n    unit_row_index = 1,                                     # (13)!\n    delim_whitespace = True,                                # (14)!\n)\n</code></pre> <ol> <li>List of the filepaths of the files to load</li> <li>Name of the column containing the provider informations in all files</li> <li>Name of the column containing the expocode informations in all files</li> <li>Name of the column containing the date informations in all files</li> <li>Name of the column containing the year informations in all files</li> <li>Name of the column containing the month informations in all files</li> <li>Name of the column containing the day informations in all files</li> <li>Name of the column containing the hour informations in all files</li> <li>Name of the column containing the latitude informations in all files</li> <li>Name of the column containing the longitude informations in all files</li> <li>Name of the column containing the depth informations in all files</li> <li>Category of all files (otherwise they couldn't be aggregated together in a single storer)</li> <li>Index of the unit row.</li> <li>Whether to delimitate values based on whitespaces</li> </ol>"},{"location":"how_it_works/loading/#storers","title":"Storers","text":"<p>Once the data in a Storer, it is easy to save this data to a file using the <code>save_storer</code> function:</p> <pre><code>import bgc_data_processing as bgc_dp\nbgc_dp.io.save_all_storer(Path(\"filepath/to/save/in.txt\"))\n</code></pre> <p>It also possible to slice the Dataframe based on the dates of the rows using the <code>.slice_on_dates</code> method. This will return a Slice object, a child class of Storer but only storing indexes of the dataframe slice and not the dataframe slice itself (to reduce the amount of memory used) :</p> <pre><code>import pandas as pd\nimport datetime as dt\ndrng = pd.Series(\n    {\n        \"start_date\": dt.datetime(2000,1,1),\n        \"end_date\": dt.datetime(2020,1,1),\n    }\n)\nslicer = storer.slice_on_dates(drng)            # (1)\n</code></pre> <ol> <li>Storer is a pre-set Storer object.</li> </ol> <p>Slice objects can be saved in the same way as any Storer:</p> <pre><code>import bgc_data_processing as bgc_dp\nbgc_dp.io.save_all_storer(\"filepath/to/save/in.txt\")\n</code></pre>"},{"location":"how_it_works/plotting/","title":"Plotting","text":"<p>All plotting object derive from the same class: <code>BasePlot</code>. Therefore, these classes all share the same public methods:</p> <ul> <li><code>show</code>: Show the figure in a new Figure.</li> <li><code>save</code>: Save the figure in a file (file suffix must be <code>.png</code>)</li> <li><code>plot_to_axes</code>: Add the figure to an already existing Axes instance.</li> </ul>"},{"location":"how_it_works/plotting/#overview","title":"Overview:","text":"<p>All the following plotting objects are defined in tracers.py.</p>"},{"location":"how_it_works/plotting/#meshplot","title":"Meshplot","text":"<p>Plot Data Density on a map.</p> <p></p>"},{"location":"how_it_works/plotting/#evolutionprofile","title":"EvolutionProfile","text":"<p>Plot the Data Density profile over the time.</p> <p></p>"},{"location":"how_it_works/plotting/#temperaturesalinitydiagram","title":"TemperatureSalinityDiagram","text":"<p>Plot the Temperature-Salinity Diagram of the given data.</p> <p></p>"},{"location":"how_it_works/plotting/#variableboxplot","title":"VariableBoxPlot","text":"<p>Plot the Box plots of a given variable for different water masses.</p> <p></p>"},{"location":"how_it_works/plotting/#variablehistogram","title":"VariableHistogram","text":"<p>Plot the histograms of a given variable for different water masses.</p> <p></p>"},{"location":"how_it_works/plotting/#watermassvariablecomparison","title":"WaterMassVariableComparison","text":"<p>Plot the values of variable vs pressure for all given water_masses.</p> <p></p>"},{"location":"how_it_works/variables/","title":"Variables","text":"<p>Variable objects save the meta data for data variables.  The object's informations are among:</p> <ul> <li>variable name in file: <code>alias</code></li> <li>variable name to use to manipulate the object: <code>name</code></li> <li>name to write in the dataframe: <code>label</code></li> <li>unit name to display in the dataframe: <code>unit</code></li> <li>the data type to use for this particular data: <code>var_type</code></li> </ul> <p>Additionally, the object also contains the informations on the transformations to apply to the data :</p> <ul> <li>corrections functions to apply to the column (for example to change its unit)</li> <li>flags informations to use to only keep data with a 'good' flag</li> </ul> <p>Different types of variables exist :</p> TemplateVarNotExistingVarExistingVarParsedVarFeatureVar <p>Pre-created variable which can then be turned into ExistingVar or NotExistingVar depending on the variables in the dataset.</p> <pre><code>import bgc_data_processing as bgc_dp\n\ntemplate = bgc_dp.variables.TemplateVar(\n    name = \"LATITUDE\",\n    unit = \"[deg_N]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\n</code></pre> <p>Usecase of TemplateVar</p> <p>When loading data from different sources, it is recommended to use TemplateVar to define all variable and then properly instantiate the variable for each source using the <code>.not_in_file</code> and <code>.in_file_as</code> methods.</p> <p>Variable which is known to not exist in the dataset. If needed, the corresponding column in the dataframe can be filled later or it will remain as nan.</p> <p>They can be created from a TemplateVar (recommended):</p> <pre><code>import bgc_data_processing as bgc_dp\n\ntemplate = bgc_dp.variables.TemplateVar(\n    name = \"LATITUDE\",\n    unit = \"[deg_N]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\nnotexisting = template.not_in_file()\n</code></pre> <p>or they can be created from scratch:</p> <pre><code>import bgc_data_processing as bgc_dp\n\nnotexisting = bgc_dp.variables.NotExistingVar(\n    name = \"LATITUDE\",\n    unit = \"[deg_N]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\n</code></pre> <p>Variable which is supposed to be find in the dataset under a certain alias. These objects also come methods to define correction functions and flag filtering options. To use theses variables properly, one must define the aliases (the name of the variable in the dataset) for the variable. It can be given any number of aliases, but the order of the aliases in important since if defines their relative priority (the first the highest priority). When loading the dataset, the first found aliases will be used to load the variable from the dataset.</p> <p>They can be created from a TemplateVar (recommended):</p> <pre><code>import bgc_data_processing as bgc_dp\n\ntemplate = bgc_dp.variables.TemplateVar(\n    name = \"LATITUDE\",\n    unit = \"[deg_N]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\nexisting = template.in_file_as(\n    (\"latitude\",\"latitude_flag\", [1])   # (1)\n    (\"latitude2\",None,None),            # (2)\n)\n</code></pre> <ol> <li>Use column \"latitude\" from source, only keep rows where the flag column (name \"latitude_flag\") value is 1.</li> <li>No flag filtering for the second alias.</li> </ol> <p>or they can be created from scratch:</p> <pre><code>import bgc_data_processing as bgc_dp\n\nexisting = bgc_dp.variables.ExistingVar(\n    name = \"LATITUDE\",\n    unit = \"[deg_N]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n).in_file_as(\n    (\"latitude\",\"latitude_flag\", [1])\n    (\"latitude2\",None,None),\n)\n</code></pre> <p>Variable partially reconstructed from a csv file saved with a <code>StorerSaver</code>.</p> <p>They can be created from scratch but usually it useless to manually use them.</p> <p>Variable which result from a <code>feature</code>. A feature variable is made out of operations over other variables.</p> <p>For example, the <code>CPHL</code> (chlorophyll) variable, made from <code>DIAC</code>(diatoms) and <code>FLAC</code> (flagellates) :</p> <pre><code>import numpy as np\n\nimport bgc_data_processing as bgc_dp\n\nfeature_var = bgc_dp.variables.FeatureVar(\n    feature = bgc_dp.features.ChlorophyllFromDiatomFlagellate(\n        diatom_variable=DIATOM_VAR,                             # (1)!\n        flagellate_variable=FLAGELLATE_VAR,                     # (2)!\n        var_name = \"CPHL\",\n        var_unit = \"[mg/m3]\",\n        var_type = float,\n        var_default = np.nan,\n        var_name_format = \"%-10s\",\n        var_value_format = \"%10.3f\",\n    )\n)\n</code></pre> <ol> <li>Pre-defined <code>Existingvar</code> for diatom concentration</li> <li>Pre-defined <code>Existingvar</code> for flagellate concentration</li> </ol> <p>using the <code>is_loadable</code> from the feature will return True if the input list of variables contains all necessary variable to create the feature.</p> <p>Then, using the <code>insert_in_storer</code> of the FeatureVar.feature property makes it possible to insert the FeatureVar into a storer containing all required variables.</p> <p>Note that no variable is created by the <code>DataSource</code>. For example, if the 'DATE' variable is required in the loader's routine, then the variable must exists in the <code>SourceVariableSet</code> provided when initializating the object.</p>"},{"location":"how_it_works/variables/#corrections","title":"Corrections","text":"<p>It is possible to specify corrections functions to apply to an ExistingVar in order to apply minor correction. This can be done using the <code>.correct_with</code> method. The function given to the method will then be applied to the column once the data loaded.</p> <pre><code>import bgc_data_processing as bgc_dp\n\ntemplate = bgc_dp.variables.TemplateVar(\n    name = \"LATITUDE\",\n    unit = \"[deg_N]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\nexisting = template.in_file_as(\n    (\"latitude\",\"latitude_flag\", [1])\n    (\"latitude2\",None,None),\n).correct_with(\n    lambda x : 2*x                      # (1)\n)\n</code></pre> <ol> <li>Correction function definition to double the value of the variable in all rows.</li> </ol>"},{"location":"how_it_works/variables/#removing-rows-when-variables-are-nan","title":"Removing rows when variables are NaN","text":"<p>It possible to specify settings for ExistingVar and NotExistingVar to remove the rows where the variable is NaN or where specific variable ar all NaN</p> When a particular variable is NaNWhen many variables are Nan <p>It can be done using the .remove_when_nan method. Then, when the values associated to the object returned by this method will be nan, the row will be deleted.</p> <pre><code>import bgc_data_processing as bgc_dp\n\ntemplate = bgc_dp.variables.TemplateVar(\n    name = \"LATITUDE\",\n    unit = \"[deg_N]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\nexisting = template.in_file_as(\n    (\"latitude\",\"latitude_flag\", [1])\n    (\"latitude2\",None,None),\n).remove_when_nan()                     # (1)\n</code></pre> <ol> <li>If latitude value is NaN, the row is dropped.</li> </ol> <p>It can be done using the <code>.remove_when_all_nan</code> method. Then, when the values associated to the object returned by this method will be nan, the row will be deleted.</p> <pre><code>import bgc_data_processing as bgc_dp\n\ntemplate_lat = bgc_dp.variables.TemplateVar(\n    name = \"LATITUDE\",\n    unit = \"[deg_N]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\ntemplate_lon = bgc_dp.variables.TemplateVar(\n    name = \"LONGITUDE\",\n    unit = \"[deg_E]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\nexisting_lat = template_lat.in_file_as(\n    (\"latitude\",\"latitude_flag\", [1])\n).remove_when_all_nan()                     # (1)\nexisting_lon = template_lon.in_file_as(\n    (\"longitude\",\"longitude_flag\", [1])\n).remove_when_all_nan()                     # (2)\n</code></pre> <ol> <li>If both latitude and longitude value are NaN, the row is dropped.</li> <li>If both latitude and longitude value are NaN, the row is dropped.</li> </ol>"},{"location":"how_it_works/variables/#variables-sets","title":"Variables Sets","text":"<p>All variables can then be stored in a <code>VariableSet</code> object so that loaders can easily interact with them.</p> <pre><code>from bgc_data_processing.core.variables.vars import TemplateVar\nfrom bgc_data_processing.core.variables.sets import VariableSet\n\ntemplate_lat = TemplateVar(\n    name = \"LATITUDE\",\n    unit = \"[deg_N]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\ntemplate_lon = TemplateVar(\n    name = \"LONGITUDE\",\n    unit = \"[deg_E]\",\n    var_type = float,\n    name_format = \"%-12s\",\n    value_format = \"%12.6f\",\n)\nexisting_lat = template_lat.in_file_as(\n    (\"latitude\",\"latitude_flag\", [1])\n)\nexisting_lon = template_lon.in_file_as(\n    (\"longitude\",\"longitude_flag\", [1])\n)\nvariables_storer = VariablesStorer(\n    latitude=existing_lat,\n    longitude=existing_lon,\n)\n</code></pre>"},{"location":"how_it_works/variables/#default-variables","title":"Default variables","text":"<p>By default, some variables are alreadey defined in <code>config/variables.toml</code> (in <code>config/default/variables.toml</code>) as TemplateVar. These variables are the most common ones for this project and the templates can be used to instanciate the <code>ExistingVar</code> or <code>NotExistingvar</code> depending on the source dataset.</p> <p>One variable definition example can be found here: <pre><code># Lines starting with '#? ' are used to verify variables' types\n# Type hints lines are structured the following way:\n# Variable keys: possible types: additionnal comment\n\n[provider]\n#? provider.NAME: str: variable name\nNAME = \"PROVIDER\"\n#? provider.UNIT: str: variable unit\nUNIT = \"[]\"\n#? provider.TYPE: str: variable type (among ['int', 'float', 'str', 'datetime64[ns]'])\nTYPE = \"str\"\n#? provider.DEFAULT: str | int | float: default variable value if nan or not existing\nDEFAULT = nan\n</code></pre></p> <p>To add a new variable, one simply has to create and edit a new set of rows, following the pattern of the already defined variables, creating for example the variable <code>var</code>: <pre><code>[var1]\n#? var1.NAME: str: variable name\nNAME=\"VAR1\"\n#? var1.UNIT: str: variable unit\nUNIT=\"[]\"\n#? var1.TYPE: str: variable type (among ['int', 'float', 'str', 'datetime64[ns]'])\nTYPE=\"str\"\n#? var1.DEFAULT: str | int | float: default variable value if nan or not existing\nDEFAULT=nan\n#? var1.NAME_FORMAT: str: format to use to save the name and unit of the variable\nNAME_FORMAT=\"%-15s\"\n#? var1.VALUE_FORMAT: str: format to use to save the values of the variable\nVALUE_FORMAT=\"%15s\"\n</code></pre></p> <p>The lines starting with <code>#?</code> allow type hinting for the variables to ensure that the correct value type is inputed.</p>"},{"location":"reference/","title":"<code>bgc_data_processing</code>","text":""},{"location":"reference/#bgc_data_processing--biogeochemical-data-processing-module-for-python","title":"Biogeochemical Data Processing module for Python","text":"<p>bgc-data-processing is a module to preprocess and analyze Biogeochemical Data with standard formats (CSV, NetCDF...).</p> <p>This module provides tools to:</p> <ul> <li>Preprocess Data from different sources</li> <li>Save preprocessed data under a standardized format</li> <li>Read preprocessed data and perform some analysis on it (visual validation,     water mass comparison...)</li> </ul> <p>All docstrings examples will assume that <code>bgc_data_processing</code> has been imported as <code>bgc_dp</code>:</p> <pre><code>&gt;&gt;&gt; import bgc_data_processing as bgc_dp\n</code></pre> <p>From this namespace are accessible:</p> <ul> <li><code>Constraints</code>             -&gt; Constraints object to slice storers</li> <li><code>DataSource</code>              -&gt; Providers defining object</li> <li><code>SelectiveDataSource</code>     -&gt; Selective loader</li> <li><code>SourceVariableSet</code>       -&gt; Set of variables to pass to datasource</li> <li><code>Storer</code>                  -&gt; Biogeochemical data and metadat storer</li> <li><code>WaterMass</code>               -&gt; Water mass defining object</li> <li><code>comparison</code>              -&gt; Comparison tools to compare observations to simulations</li> <li><code>dateranges</code>              -&gt; Daterange-related objects</li> <li><code>defaults</code>                -&gt; Defaults objects parsed from the configuration files</li> <li><code>exceptions</code>              -&gt; Exceptions fro the project</li> <li><code>features</code>                -&gt; Features to compute new variables</li> <li><code>io</code>                      -&gt; Input/Output tools</li> <li><code>metrics</code>                 -&gt; Comparison metrics</li> <li><code>parsers</code>                 -&gt; Configuartion parsing objects</li> <li><code>providers</code>               -&gt; Providers loaders</li> <li><code>savers</code>                  -&gt; Saving objects</li> <li><code>set_verbose_level</code>       -&gt; Set the verbose level</li> <li><code>tracers</code>                 -&gt; Plotting objects</li> <li><code>units</code>                   -&gt; Unit conversion</li> <li><code>utils</code>                   -&gt; Utilities</li> <li><code>variables</code>               -&gt; Variables-related objects</li> <li><code>water_masses</code>            -&gt; Water mass definition</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>bgc_data_processing<ul> <li>comparison<ul> <li>interpolation</li> <li>matching</li> <li>metrics</li> </ul> </li> <li>core<ul> <li>filtering</li> <li>io<ul> <li>readers</li> <li>savers</li> </ul> </li> <li>loaders<ul> <li>abfile_loaders</li> <li>base</li> <li>csv_loaders</li> <li>netcdf_loaders</li> </ul> </li> <li>sources</li> <li>storers</li> <li>variables<ul> <li>sets</li> <li>vars</li> </ul> </li> </ul> </li> <li>defaults</li> <li>exceptions</li> <li>features</li> <li>parsers</li> <li>providers<ul> <li>argo</li> <li>clivar</li> <li>cmems</li> <li>esacci_oc</li> <li>glodap</li> <li>glodap_2019</li> <li>glodap_2022</li> <li>hycom</li> <li>ices</li> <li>imr</li> <li>nmdc</li> </ul> </li> <li>tracers</li> <li>units</li> <li>utils<ul> <li>convert_polygons</li> <li>dateranges</li> <li>patterns</li> </ul> </li> <li>verbose</li> <li>water_masses</li> </ul> </li> </ul>"},{"location":"reference/exceptions/","title":"<code>bgc_data_processing.exceptions</code>","text":"<p>Specific exceptions.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.VariableInstantiationError","title":"<code>VariableInstantiationError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception class to raise when instantiating variables.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.IncorrectVariableNameError","title":"<code>IncorrectVariableNameError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when trying to access a variable using an incorrect name.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.DuplicatedVariableNameError","title":"<code>DuplicatedVariableNameError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when trying to add a variable with an already used name.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.IncompatibleVariableSetsError","title":"<code>IncompatibleVariableSetsError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when performing operation on incompatible variable sets.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.FeatureConstructionError","title":"<code>FeatureConstructionError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when unsuccessfully trying to build a feature.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.CSVLoadingError","title":"<code>CSVLoadingError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception class to raise when loading CSV files.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.NetCDFLoadingError","title":"<code>NetCDFLoadingError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception class to raise when loading NetCDF files.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.ABFileLoadingError","title":"<code>ABFileLoadingError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception class to raise when loading NetCDF files.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.UnsupportedLoadingFormatError","title":"<code>UnsupportedLoadingFormatError(file_format)</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when trying to load data with unsupported format.</p> Source code in <code>src/bgc_data_processing/exceptions.py</code> <pre><code>def __init__(self, file_format: str) -&gt; None:\n    error_msg = f\"File format: {file_format} can not be loaded.\"\n    super().__init__(error_msg)\n</code></pre>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.IncompatibleCategoriesError","title":"<code>IncompatibleCategoriesError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when performing operation on incompatible data categories.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.DifferentSliceOriginError","title":"<code>DifferentSliceOriginError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when performing operation ons torers with differents storers.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.IncomparableStorersError","title":"<code>IncomparableStorersError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when storers with uncomparable properties are compared.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.ImpossibleSaveError","title":"<code>ImpossibleSaveError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when it is impossible to save a Storer.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.ImpossibleTypeParsingError","title":"<code>ImpossibleTypeParsingError(keys, filepath)</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when a TOML parser can not access to a variable's type.</p> Source code in <code>src/bgc_data_processing/exceptions.py</code> <pre><code>def __init__(self, keys: list[str], filepath: Path | str) -&gt; None:\n    error_msg = f\"Type of {'.'.join(keys)} can't be parsed from {filepath}\"\n    super().__init__(error_msg)\n</code></pre>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.InvalidParameterKeyError","title":"<code>InvalidParameterKeyError(keys, filepath)</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when accessing a parsed parameter using an incorrect name.</p> Source code in <code>src/bgc_data_processing/exceptions.py</code> <pre><code>def __init__(self, keys: list[str], filepath: Path | str) -&gt; None:\n    error_msg = f\"Variable {'.'.join(keys)} does not exist in {filepath}\"\n    super().__init__(error_msg)\n</code></pre>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.IncompatibleMaskShapeError","title":"<code>IncompatibleMaskShapeError(correct_shape, incorrect_shape)</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when a Mask with an incorrect shape is trying to be set.</p> Source code in <code>src/bgc_data_processing/exceptions.py</code> <pre><code>def __init__(self, correct_shape: tuple, incorrect_shape: tuple) -&gt; None:\n    error_msg = (\n        f\"Mask shape should be {correct_shape}. \"\n        f\"Given mask shape is {incorrect_shape}.\"\n    )\n    super().__init__(error_msg)\n</code></pre>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.InvalidDateInputsError","title":"<code>InvalidDateInputsError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when a DateIntervalPattern is set with invalid dates.</p>"},{"location":"reference/exceptions/#bgc_data_processing.exceptions.InvalidPrecisionError","title":"<code>InvalidPrecisionError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when a DateIntervalPattern is set with invalid precision.</p>"},{"location":"reference/features/","title":"<code>bgc_data_processing.features</code>","text":"<p>Transformations to apply to data to create new features.</p>"},{"location":"reference/features/#bgc_data_processing.features.BaseFeature","title":"<code>BaseFeature(var_name, var_unit, var_type, var_default, var_name_format, var_value_format)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for added features.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>Name of the added variable.</p> required <code>var_unit</code> <code>str</code> <p>Unit of the added variable.</p> required <code>var_type</code> <code>str</code> <p>Type of the added variable.</p> required <code>var_default</code> <code>Any</code> <p>Default value for the added variable.</p> required <code>var_name_format</code> <code>str</code> <p>Name format for the added variable.</p> required <code>var_value_format</code> <code>str</code> <p>Value format for the added variable.</p> required Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>def __init__(\n    self,\n    var_name: str,\n    var_unit: str,\n    var_type: str,\n    var_default: Any,\n    var_name_format: str,\n    var_value_format: str,\n) -&gt; None:\n    self._output_var = NotExistingVar(\n        name=var_name,\n        unit=var_unit,\n        var_type=var_type,\n        default=var_default,\n        name_format=var_name_format,\n        value_format=var_value_format,\n    )\n</code></pre>"},{"location":"reference/features/#bgc_data_processing.features.BaseFeature.variable","title":"<code>variable: NotExistingVar</code>  <code>property</code>","text":"<p>Variable which correspond to the feature.</p>"},{"location":"reference/features/#bgc_data_processing.features.BaseFeature.required_variables","title":"<code>required_variables: list[ExistingVar | NotExistingVar | ParsedVar]</code>  <code>property</code>","text":"<p>Required variables for the feature computation.</p>"},{"location":"reference/features/#bgc_data_processing.features.BaseFeature.transform","title":"<code>transform(*args)</code>  <code>abstractmethod</code>","text":"<p>Compute the new variable values using all required series.</p> Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>@abstractmethod\ndef transform(self, *args: pd.Series) -&gt; pd.Series:\n\"\"\"Compute the new variable values using all required series.\"\"\"\n</code></pre>"},{"location":"reference/features/#bgc_data_processing.features.BaseFeature.insert_in_storer","title":"<code>insert_in_storer(storer)</code>","text":"<p>Insert the new feature in a given storer.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to include data into.</p> required Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>def insert_in_storer(self, storer: \"Storer\") -&gt; None:\n\"\"\"Insert the new feature in a given storer.\n\n    Parameters\n    ----------\n    storer : Storer\n        Storer to include data into.\n    \"\"\"\n    data = self.transform(*self._extract_from_storer(storer=storer))\n    data.index = storer.data.index\n    storer.add_feature(\n        variable=self.variable,\n        data=data,\n    )\n</code></pre>"},{"location":"reference/features/#bgc_data_processing.features.BaseFeature.copy_var_infos_from_template","title":"<code>copy_var_infos_from_template(template, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create Feature from a variable template.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>TemplateVar</code> <p>Template to use for the variable definition.</p> required <p>Returns:</p> Type Description <code>BaseFeature</code> <p>Feature.</p> Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>@classmethod\ndef copy_var_infos_from_template(\n    cls,\n    template: \"TemplateVar\",\n    **kwargs: ExistingVar | NotExistingVar | ParsedVar,\n) -&gt; \"BaseFeature\":\n\"\"\"Create Feature from a variable template.\n\n    Parameters\n    ----------\n    template : TemplateVar\n        Template to use for the variable definition.\n\n    Returns\n    -------\n    BaseFeature\n        Feature.\n    \"\"\"\n    return cls(\n        **kwargs,\n        var_name=template.name,\n        var_unit=template.unit,\n        var_type=template.type,\n        var_default=template.default,\n        var_name_format=template.name_format,\n        var_value_format=template.value_format,\n    )\n</code></pre>"},{"location":"reference/features/#bgc_data_processing.features.Pressure","title":"<code>Pressure(depth_variable, latitude_variable, var_name='PRES', var_unit='[dbars]', var_type=float, var_default=np.nan, var_name_format='%-10s', var_value_format='%10.3f')</code>","text":"<p>             Bases: <code>BaseFeature</code></p> <p>Pressure feature.</p> <p>Parameters:</p> Name Type Description Default <code>depth_variable</code> <code>ExistingVar | NotExistingVar | ParsedVar</code> <p>Variable for depth.</p> required <code>latitude_variable</code> <code>ExistingVar | NotExistingVar | ParsedVar</code> <p>Variable for latitude.</p> required <code>var_name</code> <code>str</code> <p>Variable Name., by default \"PRES\"</p> <code>'PRES'</code> <code>var_unit</code> <code>str</code> <p>Variable unit., by default \"[dbars]\"</p> <code>'[dbars]'</code> <code>var_type</code> <code>str</code> <p>Data type., by default float</p> <code>float</code> <code>var_default</code> <code>Any</code> <p>Default value., by default np.nan</p> <code>nan</code> <code>var_name_format</code> <code>str</code> <p>Name format for the added variable., by default \"%-10s\"</p> <code>'%-10s'</code> <code>var_value_format</code> <code>str</code> <p>Value format for the added variable., by default \"%10.3f\"</p> <code>'%10.3f'</code> Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>def __init__(\n    self,\n    depth_variable: ExistingVar | NotExistingVar | ParsedVar,\n    latitude_variable: ExistingVar | NotExistingVar | ParsedVar,\n    var_name: str = \"PRES\",\n    var_unit: str = \"[dbars]\",\n    var_type: str = float,\n    var_default: Any = np.nan,\n    var_name_format: str = \"%-10s\",\n    var_value_format: str = \"%10.3f\",\n) -&gt; None:\n    super().__init__(\n        var_name=var_name,\n        var_unit=var_unit,\n        var_type=var_type,\n        var_default=var_default,\n        var_name_format=var_name_format,\n        var_value_format=var_value_format,\n    )\n    self._source_vars = [depth_variable, latitude_variable]\n</code></pre>"},{"location":"reference/features/#bgc_data_processing.features.Pressure.transform","title":"<code>transform(depth, latitude)</code>","text":"<p>Compute pressure from depth and latitude.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>Series</code> <p>Depth (in meters).</p> required <code>latitude</code> <code>Series</code> <p>Latitude (in degree).</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Pressure (in dbars).</p> Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>def transform(self, depth: pd.Series, latitude: pd.Series) -&gt; pd.Series:\n\"\"\"Compute pressure from depth and latitude.\n\n    Parameters\n    ----------\n    depth : pd.Series\n        Depth (in meters).\n    latitude : pd.Series\n        Latitude (in degree).\n\n    Returns\n    -------\n    pd.Series\n        Pressure (in dbars).\n    \"\"\"\n    pressure = pd.Series(eos80.pres(np.abs(depth), latitude))\n    pressure.name = self.variable.label\n    return pressure\n</code></pre>"},{"location":"reference/features/#bgc_data_processing.features.PotentialTemperature","title":"<code>PotentialTemperature(salinity_variable, temperature_variable, pressure_variable, var_name='PTEMP', var_unit='[deg_C]', var_type=float, var_default=np.nan, var_name_format='%-10s', var_value_format='%10.3f')</code>","text":"<p>             Bases: <code>BaseFeature</code></p> <p>Potential Temperature feature..</p> <p>Parameters:</p> Name Type Description Default <code>salinity_variable</code> <code>ExistingVar | NotExistingVar | ParsedVar</code> <p>Variable for salinity.</p> required <code>temperature_variable</code> <code>ExistingVar | NotExistingVar | ParsedVar</code> <p>Variable for temperature.</p> required <code>pressure_variable</code> <code>ExistingVar | NotExistingVar | ParsedVar</code> <p>Variable for pressure.</p> required <code>var_name</code> <code>str</code> <p>Variable name., by default \"PTEMP\"</p> <code>'PTEMP'</code> <code>var_unit</code> <code>str</code> <p>Variable unit., by default \"[deg_C]\"</p> <code>'[deg_C]'</code> <code>var_type</code> <code>str</code> <p>Data type., by default float</p> <code>float</code> <code>var_default</code> <code>Any</code> <p>Default value, by default np.nan</p> <code>nan</code> <code>var_name_format</code> <code>str</code> <p>Name format for the added variable., by default \"%-10s\"</p> <code>'%-10s'</code> <code>var_value_format</code> <code>str</code> <p>Value format for the added variable., by default \"%10.3f\"</p> <code>'%10.3f'</code> Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>def __init__(\n    self,\n    salinity_variable: ExistingVar | NotExistingVar | ParsedVar,\n    temperature_variable: ExistingVar | NotExistingVar | ParsedVar,\n    pressure_variable: ExistingVar | NotExistingVar | ParsedVar,\n    var_name: str = \"PTEMP\",\n    var_unit: str = \"[deg_C]\",\n    var_type: str = float,\n    var_default: Any = np.nan,\n    var_name_format: str = \"%-10s\",\n    var_value_format: str = \"%10.3f\",\n) -&gt; None:\n    super().__init__(\n        var_name=var_name,\n        var_unit=var_unit,\n        var_type=var_type,\n        var_default=var_default,\n        var_name_format=var_name_format,\n        var_value_format=var_value_format,\n    )\n    self._source_vars = [salinity_variable, temperature_variable, pressure_variable]\n</code></pre>"},{"location":"reference/features/#bgc_data_processing.features.PotentialTemperature.transform","title":"<code>transform(salinity, temperature, pressure)</code>","text":"<p>Compute potential temperature from salinity, temperature and pressure.</p> <p>Parameters:</p> Name Type Description Default <code>salinity</code> <code>Series</code> <p>Salinity (in psu).</p> required <code>temperature</code> <code>Series</code> <p>Temperature (in Celsius degree).</p> required <code>pressure</code> <code>Series</code> <p>Pressure (in dbars).</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Potential Temperature (in Celsisus degree).</p> Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>def transform(\n    self,\n    salinity: pd.Series,\n    temperature: pd.Series,\n    pressure: pd.Series,\n) -&gt; pd.Series:\n\"\"\"Compute potential temperature from salinity, temperature and pressure.\n\n    Parameters\n    ----------\n    salinity : pd.Series\n        Salinity (in psu).\n    temperature : pd.Series\n        Temperature (in Celsius degree).\n    pressure : pd.Series\n        Pressure (in dbars).\n\n    Returns\n    -------\n    pd.Series\n        Potential Temperature (in Celsisus degree).\n    \"\"\"\n    potential_temperature = pd.Series(eos80.ptmp(salinity, temperature, pressure))\n    potential_temperature.name = self.variable.label\n    return potential_temperature\n</code></pre>"},{"location":"reference/features/#bgc_data_processing.features.SigmaT","title":"<code>SigmaT(salinity_variable, temperature_variable, var_name='SIGT', var_unit='[kg/m3]', var_type=float, var_default=np.nan, var_name_format='%-10s', var_value_format='%10.3f')</code>","text":"<p>             Bases: <code>BaseFeature</code></p> <p>Sigma T feature.</p> <p>Parameters:</p> Name Type Description Default <code>salinity_variable</code> <code>ExistingVar | NotExistingVar | ParsedVar</code> <p>Variable for slainity.</p> required <code>temperature_variable</code> <code>ExistingVar | NotExistingVar | ParsedVar</code> <p>Variable for temperature.</p> required <code>var_name</code> <code>str</code> <p>Variable name., by default \"SIGT\"</p> <code>'SIGT'</code> <code>var_unit</code> <code>str</code> <p>Variable unit., by default \"[kg/m3]\"</p> <code>'[kg/m3]'</code> <code>var_type</code> <code>str</code> <p>Data type., by default float</p> <code>float</code> <code>var_default</code> <code>Any</code> <p>Default value., by default np.nan</p> <code>nan</code> <code>var_name_format</code> <code>str</code> <p>Name format for the added variable., by default \"%-10s\"</p> <code>'%-10s'</code> <code>var_value_format</code> <code>str</code> <p>Value format for the added variable., by default \"%10.3f\"</p> <code>'%10.3f'</code> Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>def __init__(\n    self,\n    salinity_variable: ExistingVar | NotExistingVar | ParsedVar,\n    temperature_variable: ExistingVar | NotExistingVar | ParsedVar,\n    var_name: str = \"SIGT\",\n    var_unit: str = \"[kg/m3]\",\n    var_type: str = float,\n    var_default: Any = np.nan,\n    var_name_format: str = \"%-10s\",\n    var_value_format: str = \"%10.3f\",\n) -&gt; None:\n    super().__init__(\n        var_name=var_name,\n        var_unit=var_unit,\n        var_type=var_type,\n        var_default=var_default,\n        var_name_format=var_name_format,\n        var_value_format=var_value_format,\n    )\n    self._source_vars = [salinity_variable, temperature_variable]\n</code></pre>"},{"location":"reference/features/#bgc_data_processing.features.SigmaT.transform","title":"<code>transform(salinity, temperature)</code>","text":"<p>Compute sigma t from salinity and temperature.</p> <p>Parameters:</p> Name Type Description Default <code>salinity</code> <code>Series</code> <p>Salinity (in psu).</p> required <code>temperature</code> <code>Series</code> <p>Temperature (in Celsius degree).</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Sigma T (in kg/m3).</p> Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>def transform(\n    self,\n    salinity: pd.Series,\n    temperature: pd.Series,\n) -&gt; pd.Series:\n\"\"\"Compute sigma t from salinity and temperature.\n\n    Parameters\n    ----------\n    salinity : pd.Series\n        Salinity (in psu).\n    temperature : pd.Series\n        Temperature (in Celsius degree).\n\n    Returns\n    -------\n    pd.Series\n        Sigma T (in kg/m3).\n    \"\"\"\n    sigma_t = pd.Series(eos80.dens0(salinity, temperature) - 1000)\n    sigma_t.name = self.variable.label\n    return sigma_t\n</code></pre>"},{"location":"reference/features/#bgc_data_processing.features.ChlorophyllFromDiatomFlagellate","title":"<code>ChlorophyllFromDiatomFlagellate(diatom_variable, flagellate_variable, var_name='CPHL', var_unit='[mg/m3]', var_type=float, var_default=np.nan, var_name_format='%-10s', var_value_format='%10.3f')</code>","text":"<p>             Bases: <code>BaseFeature</code></p> <p>Chlorophyll-a feature.</p> <p>Parameters:</p> Name Type Description Default <code>diatom_variable</code> <code>ExistingVar | NotExistingVar | ParsedVar</code> <p>Variable for diatom.</p> required <code>flagellate_variable</code> <code>ExistingVar | NotExistingVar | ParsedVar</code> <p>Variable for flagellate.</p> required <code>var_name</code> <code>str</code> <p>Variable name., by default \"CPHL\"</p> <code>'CPHL'</code> <code>var_unit</code> <code>str</code> <p>Variable unit., by default \"[mg/m3]\"</p> <code>'[mg/m3]'</code> <code>var_type</code> <code>str</code> <p>Data type., by default float</p> <code>float</code> <code>var_default</code> <code>Any</code> <p>Default value., by default np.nan</p> <code>nan</code> <code>var_name_format</code> <code>str</code> <p>Name format for the added variable., by default \"%-10s\"</p> <code>'%-10s'</code> <code>var_value_format</code> <code>str</code> <p>Value format for the added variable., by default \"%10.3f\"</p> <code>'%10.3f'</code> Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>def __init__(\n    self,\n    diatom_variable: ExistingVar | NotExistingVar | ParsedVar,\n    flagellate_variable: ExistingVar | NotExistingVar | ParsedVar,\n    var_name: str = \"CPHL\",\n    var_unit: str = \"[mg/m3]\",\n    var_type: str = float,\n    var_default: Any = np.nan,\n    var_name_format: str = \"%-10s\",\n    var_value_format: str = \"%10.3f\",\n) -&gt; None:\n    super().__init__(\n        var_name=var_name,\n        var_unit=var_unit,\n        var_type=var_type,\n        var_default=var_default,\n        var_name_format=var_name_format,\n        var_value_format=var_value_format,\n    )\n    self._source_vars = [diatom_variable, flagellate_variable]\n</code></pre>"},{"location":"reference/features/#bgc_data_processing.features.ChlorophyllFromDiatomFlagellate.transform","title":"<code>transform(diatom, flagellate)</code>","text":"<p>Compute chlorophyll-a from diatom and flagellate.</p> <p>Parameters:</p> Name Type Description Default <code>diatom</code> <code>Series</code> <p>Diatoms (in mg/m3).</p> required <code>flagellate</code> <code>Series</code> <p>Flagellates (in mg/m3).</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Chlorophyll-a (in kg/m3).</p> Source code in <code>src/bgc_data_processing/features.py</code> <pre><code>def transform(\n    self,\n    diatom: pd.Series,\n    flagellate: pd.Series,\n) -&gt; pd.Series:\n\"\"\"Compute chlorophyll-a from diatom and flagellate.\n\n    Parameters\n    ----------\n    diatom : pd.Series\n        Diatoms (in mg/m3).\n    flagellate : pd.Series\n        Flagellates (in mg/m3).\n\n    Returns\n    -------\n    pd.Series\n        Chlorophyll-a (in kg/m3).\n    \"\"\"\n    sigma_t = diatom + flagellate\n    sigma_t.name = self.variable.label\n    return sigma_t\n</code></pre>"},{"location":"reference/parsers/","title":"<code>bgc_data_processing.parsers</code>","text":"<p>Parsing tools to determine date ranges.</p>"},{"location":"reference/parsers/#bgc_data_processing.parsers.TomlParser","title":"<code>TomlParser(filepath, check_types=True)</code>","text":"<p>Parsing class for config.toml.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Path to the config file.</p> required <code>check_types</code> <code>bool</code> <p>Whether to check types or not., by default True</p> <code>True</code> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>def __init__(self, filepath: Path | str, check_types: bool = True) -&gt; None:\n    self.filepath = Path(filepath)\n    self._check = check_types\n    with self.filepath.open(\"rb\") as f:\n        self._elements = tomllib.load(f)\n    if check_types:\n        self._parsed_types = self._parse_types(filepath=self.filepath)\n</code></pre>"},{"location":"reference/parsers/#bgc_data_processing.parsers.TomlParser.filepath","title":"<code>filepath = Path(filepath)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/parsers/#bgc_data_processing.parsers.TomlParser.raise_if_wrong_type_below","title":"<code>raise_if_wrong_type_below(keys)</code>","text":"<p>Verify types for all variables 'below' keys level.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>'Root' level which to start checking types after</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>if self._elements is not a dictionnary.</p> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>def raise_if_wrong_type_below(\n    self,\n    keys: list[str],\n) -&gt; None:\n\"\"\"Verify types for all variables 'below' keys level.\n\n    Parameters\n    ----------\n    keys : list[str]\n        'Root' level which to start checking types after\n\n    Raises\n    ------\n    TypeError\n        if self._elements is not a dictionnary.\n    \"\"\"\n    if not self._check:\n        return\n    if keys:\n        var = self._get(keys)\n        if not isinstance(var, dict):\n            self.raise_if_wrong_type(keys)\n        else:\n            for key in var:\n                self.raise_if_wrong_type_below(keys=[*keys, key])\n    elif not isinstance(self._elements, dict):\n        error_msg = \"Wrong type for toml object, should be a dictionnary\"\n        raise TypeError(error_msg)\n    else:\n        for key in self._elements:\n            self.raise_if_wrong_type_below(keys=[*keys, key])\n</code></pre>"},{"location":"reference/parsers/#bgc_data_processing.parsers.TomlParser.raise_if_wrong_type","title":"<code>raise_if_wrong_type(keys)</code>","text":"<p>Raise a TypeError if the variable type is none of the specified types.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>List path to the variable: [\"VAR1\", \"VAR2\", \"VAR3\"] is the path to the variable VAR1.VAR2.VAR3 in the toml.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the variable doesn't match any of the required types.</p> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>def raise_if_wrong_type(\n    self,\n    keys: list[str],\n) -&gt; None:\n\"\"\"Raise a TypeError if the variable type is none of the specified types.\n\n    Parameters\n    ----------\n    keys : list[str]\n        List path to the variable: [\"VAR1\", \"VAR2\", \"VAR3\"]\n        is the path to the variable VAR1.VAR2.VAR3 in the toml.\n\n    Raises\n    ------\n    TypeError\n        If the variable doesn't match any of the required types.\n    \"\"\"\n    var = self._get(keys)\n    types = self._get_type(keys)\n    # Check type:\n    is_any_type = any(self._check_type(var, var_type) for var_type in types)\n    if not is_any_type:\n        type_msg = f\"Type of {'.'.join(keys)} from {self.filepath} is incorrect.\"\n        crop = lambda x: str(x).split(\"'\")[1]\n        iterables = [t for t in types if isinstance(t, tuple)]\n        str_iter = [crop(t[0]) + \"[\" + crop(t[1]) + \"]\" for t in iterables]\n        str_other = [crop(t) for t in types if not isinstance(t, tuple)]\n        str_types = \", \".join(str_other + str_iter)\n        correct_type_msg = f\"Must be of one of these types: {str_types}.\"\n        error_msg = f\"{type_msg} {correct_type_msg}\"\n        raise TypeError(error_msg)\n</code></pre>"},{"location":"reference/parsers/#bgc_data_processing.parsers.ConfigParser","title":"<code>ConfigParser(filepath, check_types=True, dates_vars_keys=None, dirs_vars_keys=None, existing_directory='raise')</code>","text":"<p>             Bases: <code>TomlParser</code></p> <p>Class to parse toml config scripts.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Path to the file.</p> required <code>check_types</code> <code>bool</code> <p>Whether to check types or not., by default True</p> <code>True</code> <code>dates_vars_keys</code> <code>list[str | list[str]] | None</code> <p>Keys to variable defining dates., by default None</p> <code>None</code> <code>dirs_vars_keys</code> <code>list[str | list[str]] | None</code> <p>Keys to variable defining directories., by default None</p> <code>None</code> <code>existing_directory</code> <code>str</code> <p>Behavior for directory creation, 'raise' raises an error if the directory exists and is not empty, 'merge' will keep the directory as is but might replace its content when savong file and 'clean' will erase the directory if it exists.</p> <code>'raise'</code> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>def __init__(\n    self,\n    filepath: Path | str,\n    check_types: bool = True,\n    dates_vars_keys: list[str | list[str]] | None = None,\n    dirs_vars_keys: list[str | list[str]] | None = None,\n    existing_directory: str = \"raise\",\n) -&gt; None:\n    super().__init__(filepath, check_types)\n    if dates_vars_keys is None:\n        self.dates_vars_keys = []\n    else:\n        self.dates_vars_keys = dates_vars_keys\n    self.dirs_vars_keys: list[list[str]] = []\n    self._parsed = False\n    if dirs_vars_keys is not None:\n        for var in dirs_vars_keys:\n            if isinstance(var, list):\n                self.dirs_vars_keys.append(var)\n            elif isinstance(var, str):\n                self.dirs_vars_keys.append([var])\n            else:\n                error_msg = (\n                    f\"Unsupported type for directory key {var}: {type(var)}.\"\n                )\n                raise TypeError(error_msg)\n    self.existing_dir_behavior = existing_directory\n    self._dir_created = {\n        \"-\".join(directory): False for directory in self.dirs_vars_keys\n    }\n</code></pre>"},{"location":"reference/parsers/#bgc_data_processing.parsers.ConfigParser.dates_vars_keys","title":"<code>dates_vars_keys = []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/parsers/#bgc_data_processing.parsers.ConfigParser.dirs_vars_keys","title":"<code>dirs_vars_keys: list[list[str]] = []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/parsers/#bgc_data_processing.parsers.ConfigParser.existing_dir_behavior","title":"<code>existing_dir_behavior = existing_directory</code>  <code>instance-attribute</code>","text":""},{"location":"reference/parsers/#bgc_data_processing.parsers.ConfigParser.parse","title":"<code>parse()</code>","text":"<p>Parse the elements to verify types, convert dates and create directries.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Transformed dictionnary</p> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>def parse(\n    self,\n) -&gt; dict:\n\"\"\"Parse the elements to verify types, convert dates and create directries.\n\n    Returns\n    -------\n    dict\n        Transformed dictionnary\n    \"\"\"\n    if self._parsed:\n        return\n    self._parsed = True\n    self.raise_if_wrong_type_below([])\n    for keys in self.dates_vars_keys:\n        all_keys = [keys] if isinstance(keys, str) else keys\n        date = dt.datetime.strptime(self._get(all_keys), \"%Y%m%d\")\n        self._set(all_keys, date)\n</code></pre>"},{"location":"reference/parsers/#bgc_data_processing.parsers.ConfigParser.get","title":"<code>get(keys)</code>","text":"<p>Get a variable by giving the list of keys to reach the variable.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>Keys to the variable.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The desired variable.</p> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>@directory_check\ndef get(self, keys: list[str]) -&gt; Any:\n\"\"\"Get a variable by giving the list of keys to reach the variable.\n\n    Parameters\n    ----------\n    keys : list[str]\n        Keys to the variable.\n\n    Returns\n    -------\n    Any\n        The desired variable.\n    \"\"\"\n    return super()._get(keys)\n</code></pre>"},{"location":"reference/parsers/#bgc_data_processing.parsers.ConfigParser.__getitem__","title":"<code>__getitem__(__k)</code>","text":"<p>Return self._elements[__k].</p> <p>Parameters:</p> Name Type Description Default <code>__k</code> <code>str</code> <p>Key</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Value associated to __k.</p> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>@directory_check\ndef __getitem__(self, __k: str) -&gt; Any:\n\"\"\"Return self._elements[__k].\n\n    Parameters\n    ----------\n    __k : str\n        Key\n\n    Returns\n    -------\n    Any\n        Value associated to __k.\n    \"\"\"\n    self.parse()\n    return self._elements[__k]\n</code></pre>"},{"location":"reference/parsers/#bgc_data_processing.parsers.ConfigParser.__repr__","title":"<code>__repr__()</code>","text":"<p>Represent the object as a string.</p> <p>Returns:</p> Type Description <code>str</code> <p>self._elements.repr()</p> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>def __repr__(self) -&gt; str:\n\"\"\"Represent the object as a string.\n\n    Returns\n    -------\n    str\n        self._elements.__repr__()\n    \"\"\"\n    return self._elements.__repr__()\n</code></pre>"},{"location":"reference/parsers/#bgc_data_processing.parsers.DefaultTemplatesParser","title":"<code>DefaultTemplatesParser</code>","text":"<p>             Bases: <code>TomlParser</code></p> <p>Parser for variables.toml to create Template Variables.</p>"},{"location":"reference/parsers/#bgc_data_processing.parsers.DefaultTemplatesParser.variables","title":"<code>variables: dict[str, TemplateVar]</code>  <code>property</code>","text":"<p>Return the dictionnary with all created variables.</p> <p>Returns:</p> Type Description <code>dict[str, TemplateVar]</code> <p>Dictionnary mapping variables names to variables templates.</p>"},{"location":"reference/parsers/#bgc_data_processing.parsers.DefaultTemplatesParser.to_list","title":"<code>to_list()</code>","text":"<p>Return the variable ensemble as a list.</p> <p>Returns:</p> Type Description <code>list[TemplateVar]</code> <p>LIst of all templates.</p> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>def to_list(self) -&gt; list[TemplateVar]:\n\"\"\"Return the variable ensemble as a list.\n\n    Returns\n    -------\n    list[TemplateVar]\n        LIst of all templates.\n    \"\"\"\n    return list(self.variables.values())\n</code></pre>"},{"location":"reference/parsers/#bgc_data_processing.parsers.DefaultTemplatesParser.__getitem__","title":"<code>__getitem__(__k)</code>","text":"<p>Return self.variables[__k].</p> <p>Parameters:</p> Name Type Description Default <code>__k</code> <code>str</code> <p>Variable name as defined in variables.toml.</p> required <p>Returns:</p> Type Description <code>TemplateVar</code> <p>Template Variable associated to __k.</p> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>def __getitem__(self, __k: str) -&gt; TemplateVar:\n\"\"\"Return self.variables[__k].\n\n    Parameters\n    ----------\n    __k : str\n        Variable name as defined in variables.toml.\n\n    Returns\n    -------\n    TemplateVar\n        Template Variable associated to __k.\n    \"\"\"\n    return self.variables[__k]\n</code></pre>"},{"location":"reference/parsers/#bgc_data_processing.parsers.WaterMassesParser","title":"<code>WaterMassesParser</code>","text":"<p>             Bases: <code>TomlParser</code></p> <p>Parser for water_masses.toml to create WaterMass objects.</p>"},{"location":"reference/parsers/#bgc_data_processing.parsers.WaterMassesParser.variables","title":"<code>variables: dict[str, WaterMass]</code>  <code>property</code>","text":"<p>Return the dictionnary with all created WaterMass.</p> <p>Returns:</p> Type Description <code>dict[str, WaterMass]</code> <p>Dictionnary mapping WaterMass acronyms to WaterMass.</p>"},{"location":"reference/parsers/#bgc_data_processing.parsers.WaterMassesParser.__getitem__","title":"<code>__getitem__(__k)</code>","text":"<p>Return self.variables[__k].</p> <p>Parameters:</p> Name Type Description Default <code>__k</code> <code>str</code> <p>WaterMass acronym as defined in water_masses.toml.</p> required <p>Returns:</p> Type Description <code>WaterMass</code> <p>WaterMass associated to __k.</p> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>def __getitem__(self, __k: str) -&gt; WaterMass:\n\"\"\"Return self.variables[__k].\n\n    Parameters\n    ----------\n    __k : str\n        WaterMass acronym as defined in water_masses.toml.\n\n    Returns\n    -------\n    WaterMass\n        WaterMass associated to __k.\n    \"\"\"\n    return self.variables[__k]\n</code></pre>"},{"location":"reference/parsers/#bgc_data_processing.parsers.directory_check","title":"<code>directory_check(get_variable)</code>","text":"<p>Use as decorator to create directories only when needed.</p> <p>Parameters:</p> Name Type Description Default <code>get_variable</code> <code>Callable</code> <p>get of getitem function.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Wrapper function.</p> <p>Raises:</p> Type Description <code>IsADirectoryError</code> <p>If the directory exists</p> Source code in <code>src/bgc_data_processing/parsers.py</code> <pre><code>def directory_check(get_variable: Callable) -&gt; Callable:\n\"\"\"Use as decorator to create directories only when needed.\n\n    Parameters\n    ----------\n    get_variable : Callable\n        get of __getitem__ function.\n\n    Returns\n    -------\n    Callable\n        Wrapper function.\n\n    Raises\n    ------\n    IsADirectoryError\n        If the directory exists\n    \"\"\"\n\n    @wraps(get_variable)\n    def wrapper_func(self: \"ConfigParser\", keys: str | list[str]):\n        keys_dirs = [keys] if isinstance(keys, str) else keys\n        if (\n            keys_dirs in self.dirs_vars_keys\n            and not self._dir_created[\"-\".join(keys_dirs)]\n        ):\n            directory = Path(get_variable(self, keys))\n            if directory.is_dir():\n                if [p for p in directory.glob(\"*.*\") if p.name != \".gitignore\"]:\n                    if self.existing_dir_behavior == \"raise\":\n                        error_msg = (\n                            f\"Directory {directory} already exists and is not empty.\"\n                        )\n                        raise IsADirectoryError(error_msg)\n                    if self.existing_dir_behavior == \"merge\":\n                        pass\n                    elif self.existing_dir_behavior == \"clean\":\n                        shutil.rmtree(directory)\n                        directory.mkdir()\n            else:\n                directory.mkdir()\n                gitignore = directory.joinpath(\".gitignore\")\n                with gitignore.open(\"w\") as file:\n                    file.write(\"*\")\n            self._dir_created[\"-\".join(keys_dirs)] = True\n            return directory\n        return get_variable(self, keys)\n\n    return wrapper_func\n</code></pre>"},{"location":"reference/tracers/","title":"<code>bgc_data_processing.tracers</code>","text":"<p>Plotting objects.</p>"},{"location":"reference/tracers/#bgc_data_processing.tracers.BasePlot","title":"<code>BasePlot(storer, constraints)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class to plot data from a storer.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to plot data of.</p> required <code>constraints</code> <code>Constraints</code> <pre><code>Constraint slicer.\n</code></pre> required <p>Initiate Base class to plot data from a storer.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to plot data of.</p> required <code>constraints</code> <code>Constraints</code> <pre><code>Constraint slicer.\n</code></pre> required Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def __init__(self, storer: \"Storer\", constraints: \"Constraints\") -&gt; None:\n\"\"\"Initiate Base class to plot data from a storer.\n\n    Parameters\n    ----------\n    storer : Storer\n        Storer to plot data of.\n    constraints: Constraints\n            Constraint slicer.\n    \"\"\"\n    self._storer = constraints.apply_constraints_to_storer(storer)\n    self._variables = storer.variables\n    self._constraints = constraints\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.BasePlot.show","title":"<code>show(title=None, suptitle=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Plot method.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str</code> <p>Specify a suptitle to change from default., by default None</p> <code>None</code> <code>*kwargs</code> <p>Additional parameters to pass to self._build_to_new_figure.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>@abstractmethod\n@with_verbose(trigger_threshold=0, message=\"Showing Figure.\")\ndef show(\n    self,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Plot method.\n\n    Parameters\n    ----------\n    title : str, optional\n        Specify a title to change from default., by default None\n    suptitle : str, optional\n        Specify a suptitle to change from default., by default None\n    *kwargs: dict\n        Additional parameters to pass to self._build_to_new_figure.\n    \"\"\"\n    self._build_to_new_figure(\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n    plt.show()\n    plt.close()\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.BasePlot.save","title":"<code>save(save_path, title=None, suptitle=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Figure saving method.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to save the output image.</p> required <code>title</code> <code>str</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str</code> <p>Specify a suptitle to change from default., by default None</p> <code>None</code> <code>*kwargs</code> <p>Additional parameters to pass to self._build_to_new_figure.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>@abstractmethod\n@with_verbose(trigger_threshold=0, message=\"Saving figure in [save_path]\")\ndef save(\n    self,\n    save_path: str,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Figure saving method.\n\n    Parameters\n    ----------\n    save_path : str\n        Path to save the output image.\n    title : str, optional\n        Specify a title to change from default., by default None\n    suptitle : str, optional\n        Specify a suptitle to change from default., by default None\n    *kwargs: dict\n        Additional parameters to pass to self._build_to_new_figure.\n    \"\"\"\n    self._build_to_new_figure(\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n    plt.savefig(save_path)\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.BasePlot.plot_to_axes","title":"<code>plot_to_axes(ax, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Plot data to the given axes.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes | GeoAxes</code> <p>Axes to plot the data on.</p> required <code>*args</code> <p>Additional parameters for the axes plotting method.</p> <code>()</code> <code>*kwargs</code> <p>Additional parameters for the axes plotting method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes | GeoAxes</code> <p>Axes were the data is plotted on.</p> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>@abstractmethod\ndef plot_to_axes(\n    self,\n    ax: \"Axes | GeoAxes\",\n    *args,\n    **kwargs,\n) -&gt; \"Axes | GeoAxes\":\n\"\"\"Plot data to the given axes.\n\n    Parameters\n    ----------\n    ax : Axes | GeoAxes\n        Axes to plot the data on.\n    *args: list\n        Additional parameters for the axes plotting method.\n    *kwargs: dict\n        Additional parameters for the axes plotting method.\n\n    Returns\n    -------\n    Axes | GeoAxes\n        Axes were the data is plotted on.\n    \"\"\"\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.DensityPlotter","title":"<code>DensityPlotter(storer, constraints=None)</code>","text":"<p>             Bases: <code>BasePlot</code></p> <p>Base class for tracing on earthmaps.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Data Storer containing data to plot.</p> required <code>constraints</code> <code>Constraints | None</code> <pre><code>Constraint slicer.\n</code></pre> <code>None</code> <p>Instanciate base class for tracing on earthmaps.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Data Storer containing data to plot.</p> required <code>constraints</code> <code>Constraints | None</code> <pre><code>Constraint slicer.\n</code></pre> <code>None</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def __init__(\n    self,\n    storer: \"Storer\",\n    constraints: Constraints | None = None,\n) -&gt; None:\n\"\"\"Instanciate base class for tracing on earthmaps.\n\n    Parameters\n    ----------\n    storer : Storer\n        Data Storer containing data to plot.\n    constraints: Constraints | None\n            Constraint slicer.\n    \"\"\"\n    if constraints is None:\n        constraints = Constraints()\n    super().__init__(storer=storer, constraints=constraints)\n    self._lat_bin: int | float = self.__default_lat_bin\n    self._lon_bin: int | float = self.__default_lon_bin\n    self._depth_density: bool = self.__default_depth_density\n    self._lat_map_min = np.nan\n    self._lat_map_max = np.nan\n    self._lon_map_min = np.nan\n    self._lon_map_max = np.nan\n    depth_var_name = self._variables.depth_var_name\n    depth_var_label = self._variables.get(depth_var_name).label\n    self._data = self._storer.data.sort_values(depth_var_label, ascending=False)\n    self._grouping_columns = self._get_grouping_columns(self._variables)\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.DensityPlotter.__default_lat_bin","title":"<code>__default_lat_bin: int | float = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.DensityPlotter.__default_lon_bin","title":"<code>__default_lon_bin: int | float = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.DensityPlotter.__default_depth_density","title":"<code>__default_depth_density: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.DensityPlotter.set_bins_size","title":"<code>set_bins_size(bins_size)</code>","text":"<p>Set the bin sizes.</p> <p>Parameters:</p> Name Type Description Default <code>bins_size</code> <code>int | float | Iterable[int | float]</code> <p>Bins size, if tuple, first for latitude, second for longitude. If float or int, size is applied for both latitude and longitude. Unit is supposed to be degree.</p> required Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def set_bins_size(\n    self,\n    bins_size: int | float | Iterable[int | float],\n) -&gt; None:\n\"\"\"Set the bin sizes.\n\n    Parameters\n    ----------\n    bins_size : int | float | Iterable[int  |  float]\n        Bins size, if tuple, first for latitude, second for longitude.\n        If float or int, size is applied for both latitude and longitude.\n        Unit is supposed to be degree.\n    \"\"\"\n    if isinstance(bins_size, Iterable):\n        self._lat_bin = bins_size[0]\n        self._lon_bin = bins_size[1]\n    else:\n        self._lat_bin = bins_size\n        self._lon_bin = bins_size\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.DensityPlotter.set_density_type","title":"<code>set_density_type(consider_depth)</code>","text":"<p>Set the self._depth_density value.</p> <p>Parameters:</p> Name Type Description Default <code>consider_depth</code> <code>bool</code> <p>Whether to consider all value in the water for density mapping.</p> required Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def set_density_type(self, consider_depth: bool) -&gt; None:\n\"\"\"Set the self._depth_density value.\n\n    Parameters\n    ----------\n    consider_depth : bool\n        Whether to consider all value in the water for density mapping.\n    \"\"\"\n    self._depth_density = consider_depth\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.DensityPlotter.set_map_boundaries","title":"<code>set_map_boundaries(latitude_min=np.nan, latitude_max=np.nan, longitude_min=np.nan, longitude_max=np.nan)</code>","text":"<p>Define the boundaries of the map.</p> <p>(different from the boundaries of the plotted data).</p> <p>Parameters:</p> Name Type Description Default <code>latitude_min</code> <code>int | float</code> <p>Minimum latitude, by default np.nan</p> <code>nan</code> <code>latitude_max</code> <code>int | float</code> <p>Maximal latitude, by default np.nan</p> <code>nan</code> <code>longitude_min</code> <code>int | float</code> <p>Mnimal longitude, by default np.nan</p> <code>nan</code> <code>longitude_max</code> <code>int | float</code> <p>Maximal longitude, by default np.nan</p> <code>nan</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def set_map_boundaries(\n    self,\n    latitude_min: int | float = np.nan,\n    latitude_max: int | float = np.nan,\n    longitude_min: int | float = np.nan,\n    longitude_max: int | float = np.nan,\n) -&gt; None:\n\"\"\"Define the boundaries of the map.\n\n    (different from the boundaries of the plotted data).\n\n    Parameters\n    ----------\n    latitude_min : int | float, optional\n        Minimum latitude, by default np.nan\n    latitude_max : int | float, optional\n        Maximal latitude, by default np.nan\n    longitude_min : int | float, optional\n        Mnimal longitude, by default np.nan\n    longitude_max : int | float, optional\n        Maximal longitude, by default np.nan\n    \"\"\"\n    if not np.isnan(latitude_min):\n        self._lat_map_min = latitude_min\n    if not np.isnan(latitude_max):\n        self._lat_map_max = latitude_max\n    if not np.isnan(longitude_min):\n        self._lon_map_min = longitude_min\n    if not np.isnan(longitude_max):\n        self._lon_map_max = longitude_max\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.DensityPlotter.save","title":"<code>save(save_path, variable_name, title=None, suptitle=None, **kwargs)</code>","text":"<p>Plot the colormesh for the given variable.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to save the figure at.</p> required <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>title</code> <code>str | None</code> <p>Title for the figure, if set to None, automatically created. , by default None.</p> <code>None</code> <code>suptitle</code> <code>str | None</code> <p>Suptitle for the figure, if set to None, automatically created. , by default None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to plt.pcolor.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def save(\n    self,\n    save_path: str,\n    variable_name: str,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Plot the colormesh for the given variable.\n\n    Parameters\n    ----------\n    save_path: str\n        Path to save the figure at.\n    variable_name : str\n        Name of the variable to plot.\n    title: str, optional\n        Title for the figure, if set to None, automatically created.\n        , by default None.\n    suptitle: str, optional\n        Suptitle for the figure, if set to None, automatically created.\n        , by default None.\n    **kwargs\n        Additional arguments to pass to plt.pcolor.\n    \"\"\"\n    super().save(\n        save_path=save_path,\n        variable_name=variable_name,\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.DensityPlotter.show","title":"<code>show(variable_name, title=None, suptitle=None, **kwargs)</code>","text":"<p>Plot the colormesh for the given variable.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>title</code> <code>str | None</code> <p>Title for the figure, if set to None, automatically created. , by default None.</p> <code>None</code> <code>suptitle</code> <code>str | None</code> <p>Suptitle for the figure, if set to None, automatically created. , by default None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to plt.pcolor.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def show(\n    self,\n    variable_name: str,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Plot the colormesh for the given variable.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to plot.\n    title: str, optional\n        Title for the figure, if set to None, automatically created.\n        , by default None.\n    suptitle: str, optional\n        Suptitle for the figure, if set to None, automatically created.\n        , by default None.\n    **kwargs\n        Additional arguments to pass to plt.pcolor.\n    \"\"\"\n    super().show(\n        variable_name=variable_name,\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.DensityPlotter.plot_to_axes","title":"<code>plot_to_axes(variable_name, ax, **kwargs)</code>","text":"<p>Build the plot to given axes.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>ax</code> <code>GeoAxes</code> <p>GeoAxes (from cartopy) to plot the data to.</p> required <code>**kwargs</code> <p>Additional arguments to pass to plt.pcolormesh.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[GeoAxes, Collection]</code> <p>Axes, Colorbar.</p> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def plot_to_axes(\n    self,\n    variable_name: str,\n    ax: \"GeoAxes\",\n    **kwargs,\n) -&gt; tuple[\"GeoAxes\", \"Collection\"]:\n\"\"\"Build the plot to given axes.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to plot.\n    ax : GeoAxes\n        GeoAxes (from cartopy) to plot the data to.\n    **kwargs\n        Additional arguments to pass to plt.pcolormesh.\n\n    Returns\n    -------\n    tuple[GeoAxes, Collection]\n        Axes, Colorbar.\n    \"\"\"\n    return self._build_to_geoaxes(\n        variable_name=variable_name,\n        ax=ax,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.DensityPlotter.get_df","title":"<code>get_df(variable_name)</code>","text":"<p>Return the density of the given variable on the bins.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to bin.</p> required <p>Returns:</p> Type Description <code>Dataframe</code> <p>Three columns dataframe : longitude, latitude and variable density.  The column names are the same as in the original DataFrame.</p> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>@with_verbose(trigger_threshold=1, message=\"Meshing [variable_name] data.\")\ndef get_df(\n    self,\n    variable_name: str,\n) -&gt; pd.DataFrame:\n\"\"\"Return the density of the given variable on the bins.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to bin.\n\n    Returns\n    -------\n    pd.Dataframe\n        Three columns dataframe : longitude, latitude and variable density.\n         The column names are the same as in the original DataFrame.\n    \"\"\"\n    if variable_name == \"all\":\n        label = \"all\"\n    else:\n        label = self._variables.get(variable_name).label\n    lon_label = self._variables.get(self._variables.longitude_var_name).label\n    lat_label = self._variables.get(self._variables.latitude_var_name).label\n    df = self._group(\n        var_key=label,\n        lat_key=lat_label,\n        lon_key=lon_label,\n    )\n    if df.empty:\n        return pd.DataFrame(columns=[lon_label, lat_label, label])\n    longis_2d, latis_2d, values_2d = self._mesh(\n        df=df,\n        label=label,\n    )\n    # Ravel the arrays to concatenate them in a single dataframe\n    lons = longis_2d.ravel()\n    lats = latis_2d.ravel()\n    vals = values_2d.ravel()\n    data = {lon_label: lons, lat_label: lats, label: vals}\n    transformed_df = pd.DataFrame.from_dict(data)\n    return transformed_df[~transformed_df[label].isna()]\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile","title":"<code>EvolutionProfile(storer, constraints=None)</code>","text":"<p>             Bases: <code>BasePlot</code></p> <p>Class to plot the evolution of data on a given area.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to map data of.</p> required <code>constraints</code> <code>Constraints | None</code> <p>Constraint slicer.</p> <code>None</code> <p>Class to plot the evolution of data on a given area.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to map data of.</p> required <code>constraints</code> <code>Constraints | None</code> <pre><code>Constraint slicer.\n</code></pre> <code>None</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def __init__(\n    self,\n    storer: \"Storer\",\n    constraints: Constraints | None = None,\n) -&gt; None:\n\"\"\"Class to plot the evolution of data on a given area.\n\n    Parameters\n    ----------\n    storer : Storer\n        Storer to map data of.\n    constraints: Constraints|None\n            Constraint slicer.\n    \"\"\"\n    if constraints is None:\n        constraints = Constraints()\n    super().__init__(storer, constraints)\n    self._interval: str = self.__default_interval\n    self._interval_length: int = self.__default_interval_length\n    self._depth_interval: int | float = self.__default_depth_interval\n    self._depth_col = self._variables.get(self._variables.depth_var_name).label\n    self._date_col = self._variables.get(self._variables.date_var_name).label\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile.__default_interval","title":"<code>__default_interval: str = 'day'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile.__default_interval_length","title":"<code>__default_interval_length: int = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile.__default_depth_interval","title":"<code>__default_depth_interval: int = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile.__default_depth_max","title":"<code>__default_depth_max: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile.reset_intervals","title":"<code>reset_intervals()</code>","text":"<p>Reset interval parameters to the default ones.</p> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def reset_intervals(self) -&gt; None:\n\"\"\"Reset interval parameters to the default ones.\"\"\"\n    self._interval = self.__default_interval\n    self._interval_length = self.__default_interval_length\n    self._depth_interval = self.__default_depth_interval\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Reset all boundaries and intervals to default values.</p> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n\"\"\"Reset all boundaries and intervals to default values.\"\"\"\n    self.reset_intervals()\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile.set_depth_interval","title":"<code>set_depth_interval(depth_interval=np.nan)</code>","text":"<p>Set the depth interval value.</p> <p>This represents the vertical resolution of the final plot.</p> <p>Parameters:</p> Name Type Description Default <code>depth_interval</code> <code>int | float | list[int | float]</code> <p>Depth interval values, if numeric, interval resolution, if instance of list, list of depth bins to use., by default np.nan</p> <code>nan</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def set_depth_interval(\n    self,\n    depth_interval: int | float | list[int | float] = np.nan,\n) -&gt; None:\n\"\"\"Set the depth interval value.\n\n    This represents the vertical resolution of the final plot.\n\n    Parameters\n    ----------\n    depth_interval : int | float | list[int|float], optional\n        Depth interval values, if numeric, interval resolution, if instance of list,\n        list of depth bins to use., by default np.nan\n    \"\"\"\n    if isinstance(depth_interval, list) or (not np.isnan(depth_interval)):\n        self._depth_interval = depth_interval\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile.set_date_intervals","title":"<code>set_date_intervals(interval, interval_length=None)</code>","text":"<p>Set the date interval parameters.</p> <p>This represent the horizontal resolution of the final plot.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>str</code> <p>Interval resolution, can be \"day\", \"week\", \"month\", \"year\" or \"custom\".</p> required <code>interval_length</code> <code>int</code> <p>Only useful if the resolution interval is \"custom\".             Represents the interval length, in days., by default None</p> <code>None</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def set_date_intervals(\n    self,\n    interval: str,\n    interval_length: int | None = None,\n) -&gt; None:\n\"\"\"Set the date interval parameters.\n\n    This represent the horizontal resolution of the final plot.\n\n    Parameters\n    ----------\n    interval : str\n        Interval resolution, can be \"day\", \"week\", \"month\", \"year\" or \"custom\".\n    interval_length : int, optional\n        Only useful if the resolution interval is \"custom\". \\\n        Represents the interval length, in days., by default None\n    \"\"\"\n    self._interval = interval\n    if interval_length is not None:\n        self._interval_length = interval_length\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile.show","title":"<code>show(variable_name, title=None, suptitle=None, **kwargs)</code>","text":"<p>Plot the figure of data density evolution in a givemn area.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>title</code> <code>str</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str</code> <p>Specify a suptitle to change from default., by default None</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to plt.pcolor.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def show(\n    self,\n    variable_name: str,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Plot the figure of data density evolution in a givemn area.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to plot.\n    title : str, optional\n        Specify a title to change from default., by default None\n    suptitle : str, optional\n        Specify a suptitle to change from default., by default None\n    **kwargs\n        Additional arguments to pass to plt.pcolor.\n    \"\"\"\n    super().show(\n        variable_name=variable_name,\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile.save","title":"<code>save(save_path, variable_name, title=None, suptitle=None, **kwargs)</code>","text":"<p>Save the figure of data density evolution in a givemn area.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to save the output image.</p> required <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>title</code> <code>str</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str</code> <p>Specify a suptitle to change from default., by default None</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to plt.pcolor.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def save(\n    self,\n    save_path: str,\n    variable_name: str,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Save the figure of data density evolution in a givemn area.\n\n    Parameters\n    ----------\n    save_path : str\n        Path to save the output image.\n    variable_name : str\n        Name of the variable to plot.\n    title : str, optional\n        Specify a title to change from default., by default None\n    suptitle : str, optional\n        Specify a suptitle to change from default., by default None\n    **kwargs\n        Additional arguments to pass to plt.pcolor.\n    \"\"\"\n    super().save(\n        save_path=save_path,\n        variable_name=variable_name,\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.EvolutionProfile.plot_to_axes","title":"<code>plot_to_axes(variable_name, ax, **kwargs)</code>","text":"<p>Plot the data on a given axes.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot the data of.</p> required <code>ax</code> <code>Axes</code> <p>Axes to plot the data on.</p> required <code>**kwargs</code> <p>Additional arguments to pass to plt.pcolormesh.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Axes, Collection]</code> <p>Axes, colorbar</p> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def plot_to_axes(\n    self,\n    variable_name: str,\n    ax: \"Axes\",\n    **kwargs,\n) -&gt; tuple[\"Axes\", \"Collection\"]:\n\"\"\"Plot the data on a given axes.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to plot the data of.\n    ax : Axes\n        Axes to plot the data on.\n    **kwargs:\n        Additional arguments to pass to plt.pcolormesh.\n\n    Returns\n    -------\n    tuple[Axes, Collection]\n        Axes, colorbar\n    \"\"\"\n    return self._build_to_axes(\n        variable_name=variable_name,\n        ax=ax,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.TemperatureSalinityDiagram","title":"<code>TemperatureSalinityDiagram(storer, constraints, salinity_field, temperature_field, ptemperature_field)</code>","text":"<p>             Bases: <code>BasePlot</code></p> <p>Class to plot a Temperature-Salinity Diagram.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to map data of.</p> required <code>constraints</code> <code>Constraints</code> <p>Constraint slicer.</p> required <code>temperature_field</code> <code>str</code> <p>Name of the temperature field in storer (column name).</p> required <code>salinity_field</code> <code>str</code> <p>Name of the salinity field in storer (column name).</p> required Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def __init__(\n    self,\n    storer: \"Storer\",\n    constraints: \"Constraints\",\n    salinity_field: str,\n    temperature_field: str,\n    ptemperature_field: str,\n) -&gt; None:\n    super().__init__(storer, constraints)\n    self.salinity_field = salinity_field\n    self.temperature_field = temperature_field\n    self.ptemperature_field = ptemperature_field\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.TemperatureSalinityDiagram.salinity_field","title":"<code>salinity_field = salinity_field</code>  <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.TemperatureSalinityDiagram.temperature_field","title":"<code>temperature_field = temperature_field</code>  <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.TemperatureSalinityDiagram.ptemperature_field","title":"<code>ptemperature_field = ptemperature_field</code>  <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.TemperatureSalinityDiagram.show","title":"<code>show(title=None, suptitle=None, **kwargs)</code>","text":"<p>Plot the Temperature-Salinity diagram.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str</code> <p>Specify a suptitle., by default None</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to plt.pcolor.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def show(\n    self,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Plot the Temperature-Salinity diagram.\n\n    Parameters\n    ----------\n    title : str, optional\n        Specify a title to change from default., by default None\n    suptitle : str, optional\n        Specify a suptitle., by default None\n    **kwargs\n        Additional arguments to pass to plt.pcolor.\n    \"\"\"\n    super().show(title=title, suptitle=suptitle, **kwargs)\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.TemperatureSalinityDiagram.save","title":"<code>save(save_path, title=None, suptitle=None, **kwargs)</code>","text":"<p>Save the figure of Temperature-Salinity Diagram.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to save the output image.</p> required <code>title</code> <code>str</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str</code> <p>Specify a suptitle to change from default., by default None</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to plt.scatter.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def save(\n    self,\n    save_path: str,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Save the figure of Temperature-Salinity Diagram.\n\n    Parameters\n    ----------\n    save_path : str\n        Path to save the output image.\n    title : str, optional\n        Specify a title to change from default., by default None\n    suptitle : str, optional\n        Specify a suptitle to change from default., by default None\n    **kwargs\n        Additional arguments to pass to plt.scatter.\n    \"\"\"\n    super().save(save_path=save_path, title=title, suptile=suptitle, **kwargs)\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.TemperatureSalinityDiagram.plot_to_axes","title":"<code>plot_to_axes(ax, **kwargs)</code>","text":"<p>Plot the data on a given axes.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axes to plot the data on.</p> required <code>**kwargs</code> <p>Additional arguments to pass to plt.scatter.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Axes, Collection]</code> <p>Axes, colorbar</p> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def plot_to_axes(self, ax: \"Axes\", **kwargs) -&gt; tuple[\"Axes\", \"PathCollection\"]:\n\"\"\"Plot the data on a given axes.\n\n    Parameters\n    ----------\n    ax : Axes\n        Axes to plot the data on.\n    **kwargs:\n        Additional arguments to pass to plt.scatter.\n\n    Returns\n    -------\n    tuple[Axes, Collection]\n        Axes, colorbar\n    \"\"\"\n    return self._build_to_axes(\n        ax=ax,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.VariableBoxPlot","title":"<code>VariableBoxPlot(storer, constraints)</code>","text":"<p>             Bases: <code>BasePlot</code></p> <p>Class to draw box plots for a given variable.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to map the data of.</p> required <code>constraints</code> <code>Constraints</code> <p>Constraints slicer.</p> required Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def __init__(self, storer: \"Storer\", constraints: \"Constraints\") -&gt; None:\n    super().__init__(storer, constraints)\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.VariableBoxPlot.period_mapping","title":"<code>period_mapping: dict[str, str] = {'year': '%Y', 'month': '%Y-%m', 'week': '%Y-%W', 'day': '%Y-%m-%d'}</code>  <code>class-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.VariableBoxPlot.show","title":"<code>show(variable_name, period, title=None, suptitle=None, **kwargs)</code>","text":"<p>Plot method.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>period</code> <code>str</code> <p>Period on which to plot each boxplot.</p> required <code>title</code> <code>str</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str</code> <p>Specify a suptitle to change from default., by default None</p> <code>None</code> <code>*kwargs</code> <p>Additional parameters to pass to plt.boxplot.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def show(\n    self,\n    variable_name: str,\n    period: str,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Plot method.\n\n    Parameters\n    ----------\n    variable_name: str\n        Name of the variable to plot.\n    period: str\n        Period on which to plot each boxplot.\n    title : str, optional\n        Specify a title to change from default., by default None\n    suptitle : str, optional\n        Specify a suptitle to change from default., by default None\n    *kwargs: dict\n        Additional parameters to pass to plt.boxplot.\n    \"\"\"\n    super().show(\n        variable_name=variable_name,\n        period=period,\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.VariableBoxPlot.save","title":"<code>save(variable_name, period, save_path, title=None, suptitle=None, **kwargs)</code>","text":"<p>Figure saving method.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>period</code> <code>str</code> <p>Period on which to plot each boxplot.</p> required <code>save_path</code> <code>str</code> <p>Path to save the ouput image.</p> required <code>title</code> <code>str | None</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str | None</code> <p>Add a suptitle to the figure., by default None</p> <code>None</code> <code>**kwargs</code> <p>Addictional parameters to pass to plt.boxplot.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def save(\n    self,\n    variable_name: str,\n    period: str,\n    save_path: str,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Figure saving method.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to plot.\n    period : str\n        Period on which to plot each boxplot.\n    save_path : str\n        Path to save the ouput image.\n    title : str | None, optional\n        Specify a title to change from default., by default None\n    suptitle : str | None, optional\n        Add a suptitle to the figure., by default None\n    **kwargs: dict\n        Addictional parameters to pass to plt.boxplot.\n    \"\"\"\n    return super().save(\n        save_path=save_path,\n        variable_name=variable_name,\n        period=period,\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.VariableBoxPlot.plot_to_axes","title":"<code>plot_to_axes(variable_name, period, ax, **kwargs)</code>","text":"<p>Plot the data to the given axes.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>period</code> <code>str</code> <p>Period on which to plot each boxplot.</p> required <code>ax</code> <code>Axes</code> <p>Axes to plot the data on.</p> required <code>**kwargs</code> <p>Additional parameters to pass to plt.boxplot.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Axes where the data is plotted on.</p> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def plot_to_axes(\n    self,\n    variable_name: str,\n    period: str,\n    ax: \"Axes\",\n    **kwargs,\n) -&gt; \"Axes\":\n\"\"\"Plot the data to the given axes.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to plot.\n    period : str\n        Period on which to plot each boxplot.\n    ax : Axes\n        Axes to plot the data on.\n    **kwargs: dict\n        Additional parameters to pass to plt.boxplot.\n\n    Returns\n    -------\n    Axes\n        Axes where the data is plotted on.\n    \"\"\"\n    return self._build_to_axes(\n        variable_name=variable_name,\n        period=period,\n        ax=ax,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.WaterMassVariableComparison","title":"<code>WaterMassVariableComparison(storer, constraints, pressure_var_name, ptemperature_var_name, salinity_var_name, sigma_t_var_name)</code>","text":"<p>             Bases: <code>BasePlot</code></p> <p>Class to draw a pressure vs variable comparison of water masses.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to map the data of.</p> required <code>constraints</code> <code>Constraints</code> <p>Constraints slicer.</p> required <code>pressure_var_name</code> <code>str</code> <p>Name of the pressure variable.</p> required <code>ptemperature_var_name</code> <code>str</code> <p>Potential temperature variable name.</p> required <code>salinity_var_name</code> <code>str</code> <p>Salinity variable name.</p> required <code>sigma_t_var_name</code> <code>str</code> <p>Sigma-t variable name.</p> required Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def __init__(\n    self,\n    storer: \"Storer\",\n    constraints: \"Constraints\",\n    pressure_var_name: str,\n    ptemperature_var_name: str,\n    salinity_var_name: str,\n    sigma_t_var_name: str,\n) -&gt; None:\n    super().__init__(storer, constraints)\n    self.pressure_var = self._variables.get(pressure_var_name)\n    self.ptemp_var = self._variables.get(ptemperature_var_name)\n    self.salty_var = self._variables.get(salinity_var_name)\n    self.sigma_t_var = self._variables.get(sigma_t_var_name)\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.WaterMassVariableComparison.pressure_var","title":"<code>pressure_var = self._variables.get(pressure_var_name)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.WaterMassVariableComparison.ptemp_var","title":"<code>ptemp_var = self._variables.get(ptemperature_var_name)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.WaterMassVariableComparison.salty_var","title":"<code>salty_var = self._variables.get(salinity_var_name)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.WaterMassVariableComparison.sigma_t_var","title":"<code>sigma_t_var = self._variables.get(sigma_t_var_name)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.WaterMassVariableComparison.show","title":"<code>show(variable_name, wmasses, title=None, suptitle=None, **kwargs)</code>","text":"<p>Plot method.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>wmasses</code> <code>list[WaterMass]</code> <p>List all water masses.</p> required <code>title</code> <code>str</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str</code> <p>Specify a suptitle to change from default., by default None</p> <code>None</code> <code>*kwargs</code> <p>Additional parameters to pass to plt.scatter.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def show(\n    self,\n    variable_name: str,\n    wmasses: list[\"WaterMass\"],\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Plot method.\n\n    Parameters\n    ----------\n    variable_name: str\n        Name of the variable to plot.\n    wmasses : list[WaterMass]\n        List all water masses.\n    title : str, optional\n        Specify a title to change from default., by default None\n    suptitle : str, optional\n        Specify a suptitle to change from default., by default None\n    *kwargs: dict\n        Additional parameters to pass to plt.scatter.\n    \"\"\"\n    super().show(\n        variable_name=variable_name,\n        wmasses=wmasses,\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.WaterMassVariableComparison.save","title":"<code>save(variable_name, wmasses, save_path, title=None, suptitle=None, **kwargs)</code>","text":"<p>Figure saving method.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>wmasses</code> <code>list[WaterMass]</code> <p>List all water masses.</p> required <code>save_path</code> <code>str</code> <p>Path to save the ouput image.</p> required <code>title</code> <code>str | None</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str | None</code> <p>Add a suptitle to the figure., by default None</p> <code>None</code> <code>**kwargs</code> <p>Addictional parameters to pass to plt.scatter.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def save(\n    self,\n    variable_name: str,\n    wmasses: list[\"WaterMass\"],\n    save_path: str,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Figure saving method.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to plot.\n    wmasses : list[WaterMass]\n        List all water masses.\n    save_path : str\n        Path to save the ouput image.\n    title : str | None, optional\n        Specify a title to change from default., by default None\n    suptitle : str | None, optional\n        Add a suptitle to the figure., by default None\n    **kwargs: dict\n        Addictional parameters to pass to plt.scatter.\n    \"\"\"\n    super().save(\n        variable_name=variable_name,\n        wmasses=wmasses,\n        save_path=save_path,\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.WaterMassVariableComparison.plot_to_axes","title":"<code>plot_to_axes(variable_name, wmasses, ax, **kwargs)</code>","text":"<p>Plot the data to the given axes.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>wmasses</code> <code>list[WaterMass]</code> <p>List all water masses.</p> required <code>ax</code> <code>Axes</code> <p>Axes to plot the data on.</p> required <code>**kwargs</code> <p>Additional parameters to pass to plt.scatter.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Axes where the data is plotted on.</p> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def plot_to_axes(\n    self,\n    variable_name: str,\n    wmasses: list[\"WaterMass\"],\n    ax: \"Axes\",\n    **kwargs,\n) -&gt; \"Axes\":\n\"\"\"Plot the data to the given axes.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to plot.\n    wmasses : list[WaterMass]\n        List all water masses.\n    ax : Axes\n        Axes to plot the data on.\n    **kwargs: dict\n        Additional parameters to pass to plt.scatter.\n\n    Returns\n    -------\n    Axes\n        Axes where the data is plotted on.\n    \"\"\"\n    return self._build_to_axes(\n        variable_name=variable_name,\n        wmasses=wmasses,\n        ax=ax,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.VariableHistogram","title":"<code>VariableHistogram(storer, constraints)</code>","text":"<p>             Bases: <code>BasePlot</code></p> <p>Class to draw histogram for a given variable.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to map the data of.</p> required <code>constraints</code> <code>Constraints</code> <p>Constraints slicer.</p> required Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def __init__(self, storer: \"Storer\", constraints: \"Constraints\") -&gt; None:\n    super().__init__(storer, constraints)\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.VariableHistogram.bin_number","title":"<code>bin_number: int = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/tracers/#bgc_data_processing.tracers.VariableHistogram.show","title":"<code>show(variable_name, title=None, suptitle=None, **kwargs)</code>","text":"<p>Plot method.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>title</code> <code>str</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str</code> <p>Specify a suptitle to change from default., by default None</p> <code>None</code> <code>*kwargs</code> <p>Additional parameters to pass to plt.hist.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def show(\n    self,\n    variable_name: str,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Plot method.\n\n    Parameters\n    ----------\n    variable_name: str\n        Name of the variable to plot.\n    title : str, optional\n        Specify a title to change from default., by default None\n    suptitle : str, optional\n        Specify a suptitle to change from default., by default None\n    *kwargs: dict\n        Additional parameters to pass to plt.hist.\n    \"\"\"\n    super().show(\n        variable_name=variable_name,\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.VariableHistogram.save","title":"<code>save(variable_name, save_path, title=None, suptitle=None, **kwargs)</code>","text":"<p>Figure saving method.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>save_path</code> <code>str</code> <p>Path to save the ouput image.</p> required <code>title</code> <code>str | None</code> <p>Specify a title to change from default., by default None</p> <code>None</code> <code>suptitle</code> <code>str | None</code> <p>Add a suptitle to the figure., by default None</p> <code>None</code> <code>**kwargs</code> <p>Addictional parameters to pass to plt.hist.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def save(\n    self,\n    variable_name: str,\n    save_path: str,\n    title: str | None = None,\n    suptitle: str | None = None,\n    **kwargs,\n) -&gt; None:\n\"\"\"Figure saving method.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to plot.\n    save_path : str\n        Path to save the ouput image.\n    title : str | None, optional\n        Specify a title to change from default., by default None\n    suptitle : str | None, optional\n        Add a suptitle to the figure., by default None\n    **kwargs: dict\n        Addictional parameters to pass to plt.hist.\n    \"\"\"\n    return super().save(\n        save_path=save_path,\n        variable_name=variable_name,\n        title=title,\n        suptitle=suptitle,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tracers/#bgc_data_processing.tracers.VariableHistogram.plot_to_axes","title":"<code>plot_to_axes(variable_name, ax, **kwargs)</code>","text":"<p>Plot the data to the given axes.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to plot.</p> required <code>ax</code> <code>Axes</code> <p>Axes to plot the data on.</p> required <code>**kwargs</code> <p>Additional parameters to pass to plt.hist.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Axes where the data is plotted on.</p> Source code in <code>src/bgc_data_processing/tracers.py</code> <pre><code>def plot_to_axes(\n    self,\n    variable_name: str,\n    ax: \"Axes\",\n    **kwargs,\n) -&gt; \"Axes\":\n\"\"\"Plot the data to the given axes.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to plot.\n    ax : Axes\n        Axes to plot the data on.\n    **kwargs: dict\n        Additional parameters to pass to plt.hist.\n\n    Returns\n    -------\n    Axes\n        Axes where the data is plotted on.\n    \"\"\"\n    return self._build_to_axes(\n        variable_name=variable_name,\n        ax=ax,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/units/","title":"<code>bgc_data_processing.units</code>","text":"<p>Unit conversion functions.</p>"},{"location":"reference/units/#bgc_data_processing.units.convert_umol_by_kg_to_mmol_by_m3","title":"<code>convert_umol_by_kg_to_mmol_by_m3(data_umol_by_kg)</code>","text":"<p>Convert umol/kg to mmol/m3 using sewater denisty.</p> <p>Parameters:</p> Name Type Description Default <code>data_umol_by_kg</code> <code>Series</code> <p>Original data (in umol/kg)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Converted data (mmol/m3)</p> Source code in <code>src/bgc_data_processing/units.py</code> <pre><code>def convert_umol_by_kg_to_mmol_by_m3(\n    data_umol_by_kg: pd.Series,\n) -&gt; pd.Series:\n\"\"\"Convert umol/kg to mmol/m3 using sewater denisty.\n\n    Parameters\n    ----------\n    data_umol_by_kg : pd.Series\n        Original data (in umol/kg)\n\n    Returns\n    -------\n    pd.Series\n        Converted data (mmol/m3)\n    \"\"\"\n    kg_by_m3 = 1025  # seawater density: 1025 kg &lt;=&gt; 1 m3\n    mmol_by_umol = 10 ** (-3)  # 1000 mmol = 1 mol\n    return data_umol_by_kg * mmol_by_umol * kg_by_m3\n</code></pre>"},{"location":"reference/units/#bgc_data_processing.units.convert_doxy_ml_by_l_to_mmol_by_m3","title":"<code>convert_doxy_ml_by_l_to_mmol_by_m3(data_ml_by_l)</code>","text":"<p>Convert dissolved oxygen from mL/L to mmol/m3.</p> <p>Parameters:</p> Name Type Description Default <code>data_ml_by_l</code> <code>Series</code> <p>Original data (mL/L)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Converted data (mmol/m3)</p> Source code in <code>src/bgc_data_processing/units.py</code> <pre><code>def convert_doxy_ml_by_l_to_mmol_by_m3(\n    data_ml_by_l: pd.Series,\n) -&gt; pd.Series:\n\"\"\"Convert dissolved oxygen from mL/L to mmol/m3.\n\n    Parameters\n    ----------\n    data_ml_by_l : pd.Series\n        Original data (mL/L)\n\n    Returns\n    -------\n    pd.Series\n        Converted data (mmol/m3)\n    \"\"\"\n    return data_ml_by_l * 44.6608009\n</code></pre>"},{"location":"reference/units/#bgc_data_processing.units.convert_nitrate_mgc_by_m3_to_umol_by_l","title":"<code>convert_nitrate_mgc_by_m3_to_umol_by_l(data_mgc_m3)</code>","text":"<p>Convert nitrate concentration from mgC/m3 to \u03bcmol/L.</p> <p>Parameters:</p> Name Type Description Default <code>data_mgc_m3</code> <code>Series</code> <p>Original data (mgC/m3)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Converted data (\u03bcmol/L)</p> Source code in <code>src/bgc_data_processing/units.py</code> <pre><code>def convert_nitrate_mgc_by_m3_to_umol_by_l(\n    data_mgc_m3: pd.Series,\n) -&gt; pd.Series:\n\"\"\"Convert nitrate concentration from mgC/m3 to \u03bcmol/L.\n\n    Parameters\n    ----------\n    data_mgc_m3 : pd.Series\n        Original data (mgC/m3)\n\n    Returns\n    -------\n    pd.Series\n        Converted data (\u03bcmol/L)\n    \"\"\"\n    mgc_by_mgno3 = 6.625 * 12.01  # 6.625*12.01 mg(NO3) &lt;=&gt; 1 mg(C)\n    mgno3_by_mgc = 1 / mgc_by_mgno3\n    g_by_molno3 = 62.009  # Nitrate molar mass: 62.009 g &lt;=&gt; 1 mol\n    mg_by_g = 1_000  # 1000 mg = 1 g\n    mg_by_molno3 = mg_by_g * g_by_molno3\n    molno3_by_mg = 1 / mg_by_molno3\n    l_by_m3 = 1_000  # 1000 L &lt;=&gt; 1 m3\n    umol_by_mol = 1_000_000  # 1 000 000 \u03bcmol = 1 mol\n    return data_mgc_m3 * mgno3_by_mgc * molno3_by_mg * umol_by_mol / l_by_m3\n</code></pre>"},{"location":"reference/units/#bgc_data_processing.units.convert_silicate_mgc_by_m3_to_umol_by_l","title":"<code>convert_silicate_mgc_by_m3_to_umol_by_l(data_mgc_m3)</code>","text":"<p>Convert silicate concentration from mgC/m3 to \u03bcmol/L.</p> <p>Parameters:</p> Name Type Description Default <code>data_mgc_m3</code> <code>Series</code> <p>Original data (mgC/m3)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Converted data (\u03bcmol/L)</p> Source code in <code>src/bgc_data_processing/units.py</code> <pre><code>def convert_silicate_mgc_by_m3_to_umol_by_l(\n    data_mgc_m3: pd.Series,\n) -&gt; pd.Series:\n\"\"\"Convert silicate concentration from mgC/m3 to \u03bcmol/L.\n\n    Parameters\n    ----------\n    data_mgc_m3 : pd.Series\n        Original data (mgC/m3)\n\n    Returns\n    -------\n    pd.Series\n        Converted data (\u03bcmol/L)\n    \"\"\"\n    mgc_by_mgsi02 = 6.625 * 12.01  # 6.625*12.01 mg(SiO2) &lt;=&gt; 1 mg(C)\n    mgsi02_by_mgc = 1 / mgc_by_mgsi02\n    g_by_molsi02 = 76.083  # Silicate molar mass: 76.083 g &lt;=&gt; 1 mol\n    mg_by_g = 1_000  # 1000 mg = 1 g\n    mg_by_molsi02 = mg_by_g * g_by_molsi02\n    molsi02_by_mg = 1 / mg_by_molsi02\n    l_by_m3 = 1_000  # 1000 L &lt;=&gt; 1 m3\n    umol_by_mol = 1_000_000  # 1 000 000 \u03bcmol = 1 mol\n    return data_mgc_m3 * mgsi02_by_mgc * molsi02_by_mg * umol_by_mol / l_by_m3\n</code></pre>"},{"location":"reference/units/#bgc_data_processing.units.convert_phosphate_mgc_by_m3_to_umol_by_l","title":"<code>convert_phosphate_mgc_by_m3_to_umol_by_l(data_mgc_m3)</code>","text":"<p>Convert phosphate concentration from mgC/m3 to \u03bcmol/L.</p> <p>Parameters:</p> Name Type Description Default <code>data_mgc_m3</code> <code>Series</code> <p>Original data (mgC/m3)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Converted data (\u03bcmol/L)</p> Source code in <code>src/bgc_data_processing/units.py</code> <pre><code>def convert_phosphate_mgc_by_m3_to_umol_by_l(\n    data_mgc_m3: pd.Series,\n) -&gt; pd.Series:\n\"\"\"Convert phosphate concentration from mgC/m3 to \u03bcmol/L.\n\n    Parameters\n    ----------\n    data_mgc_m3 : pd.Series\n        Original data (mgC/m3)\n\n    Returns\n    -------\n    pd.Series\n        Converted data (\u03bcmol/L)\n    \"\"\"\n    mgc_by_mgh3po4 = 107 * 12.01  # 107*12.01 mg(H3PO4) &lt;=&gt; 1 mg(C)\n    mgh3po4_by_mgc = 1 / mgc_by_mgh3po4\n    g_by_molh3po4 = 94.9714  # Phosphate molar mass:  94.9714 g &lt;=&gt; 1 mol\n    mg_by_g = 1_000  # 1000 mg = 1 g\n    mg_by_molh3po4 = mg_by_g * g_by_molh3po4\n    molh3po4_by_mg = 1 / mg_by_molh3po4\n    l_by_m3 = 1_000  # 1000 L &lt;=&gt; 1 m3\n    umol_by_mol = 1_000_000  # 1 000 000 \u03bcmol = 1 mol\n    return data_mgc_m3 * mgh3po4_by_mgc * molh3po4_by_mg * umol_by_mol / l_by_m3\n</code></pre>"},{"location":"reference/verbose/","title":"<code>bgc_data_processing.verbose</code>","text":"<p>Verbose decorator.</p>"},{"location":"reference/verbose/#bgc_data_processing.verbose.Verbose","title":"<code>Verbose</code>","text":"<p>Verbose Singleton class.</p>"},{"location":"reference/verbose/#bgc_data_processing.verbose.Verbose.max_allowed","title":"<code>max_allowed: int = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/verbose/#bgc_data_processing.verbose.Verbose.min_allowed","title":"<code>min_allowed: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/verbose/#bgc_data_processing.verbose.Verbose.level","title":"<code>level: int</code>  <code>property</code> <code>writable</code>","text":"<p>Verbose level.</p>"},{"location":"reference/verbose/#bgc_data_processing.verbose.Verbose.__new__","title":"<code>__new__()</code>","text":"<p>Instanciate new verbose singleton.</p> <p>Create an instance if there is no instance existing. Otherwise, return the existing one.</p> <p>Returns:</p> Type Description <code>Verbose</code> <p>Verbose singleton</p> Source code in <code>src/bgc_data_processing/verbose.py</code> <pre><code>def __new__(cls) -&gt; \"Verbose\":\n\"\"\"Instanciate new verbose singleton.\n\n    Create an instance if there is no instance existing.\n    Otherwise, return the existing one.\n\n    Returns\n    -------\n    Verbose\n        Verbose singleton\n    \"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n    return cls._instance\n</code></pre>"},{"location":"reference/verbose/#bgc_data_processing.verbose.set_verbose_level","title":"<code>set_verbose_level(level)</code>","text":"<p>Instanciate the Verbose singleton to a given value.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Verbose level.</p> required Source code in <code>src/bgc_data_processing/verbose.py</code> <pre><code>def set_verbose_level(level: int) -&gt; None:\n\"\"\"Instanciate the Verbose singleton to a given value.\n\n    Parameters\n    ----------\n    level : int\n        Verbose level.\n    \"\"\"\n    verbose = Verbose()\n    verbose.level = level\n</code></pre>"},{"location":"reference/verbose/#bgc_data_processing.verbose.with_verbose","title":"<code>with_verbose(trigger_threshold, message)</code>","text":"<p>Display verbose on the function call.</p> <p>One must keep in mind that the message is displayed only if the verbose level is STRICTLY greater than the trigger_threshold.</p> <p>In order to use a placeholder to insert values in the message use square brackets ('[' and ']'). However, keep in mind that for the placeholder to work, one must the name of the parameter in the function call.</p> <p>For example, with:</p> <p>@with_verbose(trigger_threshold=0, message=\"Input with a=[a]\") def func(a: int) -&gt; None:     print(a)</p> <p>We can have the following results depending on how 'func' is called:</p> <p>func(a=3) ... Input with a=3 ... 3</p> <p>Or</p> <p>func(3) ... Input with a=[a] ... 3</p> <p>Parameters:</p> Name Type Description Default <code>trigger_threshold</code> <code>int</code> <p>Level to use as trigger for verbose display. Example: if trigger_level = 1 -&gt; message is displayed if the global verbose level is striclty greater than 1.</p> required <code>message</code> <code>str</code> <p>Message to display.</p> required Source code in <code>src/bgc_data_processing/verbose.py</code> <pre><code>def with_verbose(trigger_threshold: int, message: str):\n\"\"\"Display verbose on the function call.\n\n    One must keep in mind that the message is displayed only if the verbose\n    level is STRICTLY greater than the trigger_threshold.\n\n    In order to use a placeholder to insert values in the message use square brackets\n    ('[' and ']').\n    However, keep in mind that for the placeholder to work, one must\n    the name of the parameter in the function call.\n\n    For example, with:\n\n    &gt;&gt;&gt; @with_verbose(trigger_threshold=0, message=\"Input with a=[a]\")\n    &gt;&gt;&gt; def func(a: int) -&gt; None:\n    &gt;&gt;&gt;     print(a)\n\n    We can have the following results depending on how 'func' is called:\n\n    &gt;&gt;&gt; func(a=3)\n    ... Input with a=3\n    ... 3\n\n    Or\n\n    &gt;&gt;&gt; func(3)\n    ... Input with a=[a]\n    ... 3\n\n    Parameters\n    ----------\n    trigger_threshold : int\n        Level to use as trigger for verbose display.\n        Example: if trigger_level = 1 -&gt; message is displayed if\n        the global verbose level is striclty greater than 1.\n    message : str\n        Message to display.\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if not isinstance(trigger_threshold, int):\n                error_msg = \"Trigger threshold must be an integer\"\n                raise TypeError(error_msg)\n            verbose = Verbose()\n            threshold_or_max = min(trigger_threshold, verbose.max_allowed)\n            level = max(verbose.min_allowed, threshold_or_max)\n            content = message\n            if verbose.level &gt; level:\n                # Adjust indentation\n                offset = \"\".join([\"\\t\"] * level)\n                # Replace placeholders\n                for key, value in kwargs.items():\n                    content = content.replace(\"[\" + key + \"]\", str(value))\n                    # verbose\n                print(f\"{offset}{content}\")\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"reference/water_masses/","title":"<code>bgc_data_processing.water_masses</code>","text":"<p>Objects related to water masses.</p>"},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass","title":"<code>WaterMass(name, acronym=None, ptemperature_range=(np.nan, np.nan), salinity_range=(np.nan, np.nan), sigma_t_range=(np.nan, np.nan))</code>","text":"<p>Water Mass.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Water mass name.</p> required <code>ptemperature_range</code> <code>Iterable[float, float]</code> <p>Potential temperature range: (minimum, maximum), by default (np.nan, np.nan)</p> <code>(nan, nan)</code> <code>salinity_range</code> <code>Iterable[float, float]</code> <p>Salinity range: (minimum, maximum), by default (np.nan, np.nan)</p> <code>(nan, nan)</code> <code>sigma_t_range</code> <code>Iterable[float, float]</code> <p>Sigma-t range: (minimum, maximum), by default (np.nan, np.nan)</p> <code>(nan, nan)</code> Source code in <code>src/bgc_data_processing/water_masses.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    acronym: str | None = None,\n    ptemperature_range: Iterable[float, float] = (np.nan, np.nan),\n    salinity_range: Iterable[float, float] = (np.nan, np.nan),\n    sigma_t_range: Iterable[float, float] = (np.nan, np.nan),\n) -&gt; None:\n    self.name = name\n    if acronym is None:\n        acronym = \"\".join(word[0].upper() for word in name.split(\" \"))\n    self.acronym = acronym\n    self.ptemperature_min = ptemperature_range[0]\n    self.ptemperature_max = ptemperature_range[1]\n    self.salinity_min = salinity_range[0]\n    self.salinity_max = salinity_range[1]\n    self.sigma_t_min = sigma_t_range[0]\n    self.sigma_t_max = sigma_t_range[1]\n</code></pre>"},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.name","title":"<code>name = name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.acronym","title":"<code>acronym = acronym</code>  <code>instance-attribute</code>","text":""},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.ptemperature_min","title":"<code>ptemperature_min = ptemperature_range[0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.ptemperature_max","title":"<code>ptemperature_max = ptemperature_range[1]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.salinity_min","title":"<code>salinity_min = salinity_range[0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.salinity_max","title":"<code>salinity_max = salinity_range[1]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.sigma_t_min","title":"<code>sigma_t_min = sigma_t_range[0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.sigma_t_max","title":"<code>sigma_t_max = sigma_t_range[1]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.__repr__","title":"<code>__repr__()</code>","text":"<p>Represent self as a string.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name and boundaries.</p> Source code in <code>src/bgc_data_processing/water_masses.py</code> <pre><code>def __repr__(self) -&gt; str:\n\"\"\"Represent self as a string.\n\n    Returns\n    -------\n    str\n        Name and boundaries.\n    \"\"\"\n    name_txt = f\"{self.name} ({self.acronym})\"\n    temp_txt = f\"PTemperature in [{self.ptemperature_min},{self.ptemperature_max}]\"\n    saln_txt = f\"Salinity in [{self.salinity_min},{self.salinity_max}]\"\n    sigt_txt = f\"Sigma-t in [{self.sigma_t_min},{self.sigma_t_max}]\"\n    return f\"{name_txt}\\n{temp_txt}\\n{saln_txt}\\n{sigt_txt}\"\n</code></pre>"},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.make_constraints","title":"<code>make_constraints(ptemperature_label, salinity_label, sigma_t_label)</code>","text":"<p>Create the constraint for the water mass.</p> <p>Parameters:</p> Name Type Description Default <code>ptemperature_label</code> <code>str</code> <p>Potential temperature label.</p> required <code>salinity_label</code> <code>str</code> <p>Salinity label.</p> required <code>sigma_t_label</code> <code>str</code> <p>Sigma-t label.</p> required <p>Returns:</p> Type Description <code>Constraints</code> <p>Constraint corresponding to the water mass.</p> Source code in <code>src/bgc_data_processing/water_masses.py</code> <pre><code>def make_constraints(\n    self,\n    ptemperature_label: str,\n    salinity_label: str,\n    sigma_t_label: str,\n) -&gt; Constraints:\n\"\"\"Create the constraint for the water mass.\n\n    Parameters\n    ----------\n    ptemperature_label : str\n        Potential temperature label.\n    salinity_label : str\n        Salinity label.\n    sigma_t_label : str\n        Sigma-t label.\n\n    Returns\n    -------\n    Constraints\n        Constraint corresponding to the water mass.\n    \"\"\"\n    constraints = Constraints()\n    constraints.add_boundary_constraint(\n        field_label=ptemperature_label,\n        minimal_value=self.ptemperature_min,\n        maximal_value=self.ptemperature_max,\n    )\n    constraints.add_boundary_constraint(\n        field_label=salinity_label,\n        minimal_value=self.salinity_min,\n        maximal_value=self.salinity_max,\n    )\n    constraints.add_boundary_constraint(\n        field_label=sigma_t_label,\n        minimal_value=self.sigma_t_min,\n        maximal_value=self.sigma_t_max,\n    )\n    return constraints\n</code></pre>"},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.extract_from_storer","title":"<code>extract_from_storer(storer, ptemperature_name, salinity_name, sigma_t_name)</code>","text":"<p>Extract a the storer whose values are in the water mass.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Original Storer.</p> required <code>ptemperature_name</code> <code>str</code> <p>Potenital temperature variable name.</p> required <code>salinity_name</code> <code>str</code> <p>Salinity Variable name.</p> required <code>sigma_t_name</code> <code>str</code> <p>Sigma-t variable name.</p> required <p>Returns:</p> Type Description <code>Storer</code> <p>Storer whose values are in the water mass.</p> Source code in <code>src/bgc_data_processing/water_masses.py</code> <pre><code>def extract_from_storer(\n    self,\n    storer: \"Storer\",\n    ptemperature_name: str,\n    salinity_name: str,\n    sigma_t_name: str,\n) -&gt; \"Storer\":\n\"\"\"Extract a the storer whose values are in the water mass.\n\n    Parameters\n    ----------\n    storer : Storer\n        Original Storer.\n    ptemperature_name : str\n        Potenital temperature variable name.\n    salinity_name : str\n        Salinity Variable name.\n    sigma_t_name : str\n        Sigma-t variable name.\n\n    Returns\n    -------\n    Storer\n        Storer whose values are in the water mass.\n    \"\"\"\n    constraints = self.make_constraints(\n        ptemperature_label=storer.variables.get(ptemperature_name).label,\n        salinity_label=storer.variables.get(salinity_name).label,\n        sigma_t_label=storer.variables.get(sigma_t_name).label,\n    )\n    return constraints.apply_constraints_to_storer(storer)\n</code></pre>"},{"location":"reference/water_masses/#bgc_data_processing.water_masses.WaterMass.flag_in_storer","title":"<code>flag_in_storer(original_storer, water_mass_variable_name, ptemperature_name, salinity_name, sigma_t_name, create_var_if_missing=True)</code>","text":"<p>Flag the.</p> <p>Parameters:</p> Name Type Description Default <code>original_storer</code> <code>Storer</code> <p>original storer.</p> required <code>water_mass_variable_name</code> <code>str</code> <p>Name of the water mass variable.</p> required <code>ptemperature_name</code> <code>str</code> <p>Potential temperature variable name.</p> required <code>salinity_name</code> <code>str</code> <p>Salinity Variable name.</p> required <code>sigma_t_name</code> <code>str</code> <p>Sigma-t at pressure 0 variable name.</p> required <code>create_var_if_missing</code> <code>bool</code> <p>Whether to create the water mass variable in the storer., by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>Storer</code> <p>Copy of original storer with an updated 'water mass' field.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the water mass variable doens't exists and can't be created.</p> Source code in <code>src/bgc_data_processing/water_masses.py</code> <pre><code>def flag_in_storer(\n    self,\n    original_storer: \"Storer\",\n    water_mass_variable_name: str,\n    ptemperature_name: str,\n    salinity_name: str,\n    sigma_t_name: str,\n    create_var_if_missing: bool = True,\n) -&gt; \"Storer\":\n\"\"\"Flag the.\n\n    Parameters\n    ----------\n    original_storer : Storer\n        original storer.\n    water_mass_variable_name : str\n        Name of the water mass variable.\n    ptemperature_name : str\n        Potential temperature variable name.\n    salinity_name : str\n        Salinity Variable name.\n    sigma_t_name : str\n        Sigma-t at pressure 0 variable name.\n    create_var_if_missing : bool, optional\n        Whether to create the water mass variable in the storer., by default True\n\n    Returns\n    -------\n    Storer\n        Copy of original storer with an updated 'water mass' field.\n\n    Raises\n    ------\n    ValueError\n        If the water mass variable doens't exists and can't be created.\n    \"\"\"\n    constraints = self.make_constraints(\n        ptemperature_label=original_storer.variables.get(ptemperature_name).label,\n        salinity_label=original_storer.variables.get(salinity_name).label,\n        sigma_t_label=original_storer.variables.get(sigma_t_name).label,\n    )\n    full_data = original_storer.data\n    compliant = constraints.apply_constraints_to_dataframe(full_data).index\n    if original_storer.variables.has_name(water_mass_variable_name):\n        water_mass_var = original_storer.variables.get(water_mass_variable_name)\n        water_mass_label = water_mass_var.label\n        data = full_data[water_mass_label]\n        data[compliant] = self.name\n        full_data[water_mass_label] = data\n        return Storer(\n            data=full_data,\n            category=original_storer.category,\n            providers=original_storer.providers[:],\n            variables=original_storer.variables,\n        )\n    if not create_var_if_missing:\n        error_msg = f\"{water_mass_variable_name} invalid for the given storer.\"\n        raise ValueError(error_msg)\n\n    data = pd.Series(np.nan, index=full_data.index)\n    data[compliant] = self.name\n    new_var = NotExistingVar(\n        water_mass_variable_name,\n        \"[]\",\n        str,\n        np.nan,\n        \"%-15s\",\n        \"%15s\",\n    )\n    new_storer = Storer(\n        data=full_data,\n        category=original_storer.category,\n        providers=original_storer.providers[:],\n        variables=original_storer.variables,\n    )\n    new_storer.add_feature(new_var, data)\n    return new_storer\n</code></pre>"},{"location":"reference/comparison/","title":"<code>bgc_data_processing.comparison</code>","text":"<p>Tools to perform Comparison between observations and simulations.</p> <p>From this namespace are accessible: - <code>Interpolator</code>            -&gt; Tool to interpolate data to match data's depth values - <code>NearestNeighborStrategy</code> -&gt; Closest point finding strategy - <code>SelectiveDataSource</code>     -&gt; Selective loader to retrieve data on given location only - <code>metrics</code>                 -&gt; Metrics to compare observations and simulations</p>"},{"location":"reference/comparison/interpolation/","title":"<code>bgc_data_processing.comparison.interpolation</code>","text":"<p>Interpolation objects.</p>"},{"location":"reference/comparison/interpolation/#bgc_data_processing.comparison.interpolation.Interpolator","title":"<code>Interpolator(base, x_column_name, y_columns_name, kind='linear')</code>","text":"<p>Interpolate slices with common index from a reference dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>DataFrame</code> <p>DataFrame to use a base data for the interpolation.</p> required <code>x_column_name</code> <code>str</code> <p>name of the column to use as x for the interpolation.</p> required <code>kind</code> <code>str</code> <p>Type of interpolation, to pass to scipy.interpolate.interp1d. , by default \"linear\"</p> <code>'linear'</code> Source code in <code>src/bgc_data_processing/comparison/interpolation.py</code> <pre><code>def __init__(\n    self,\n    base: Storer,\n    x_column_name: str,\n    y_columns_name: list[str],\n    kind: str = \"linear\",\n) -&gt; None:\n    self._storer = base\n    self._columns_order = base.data.columns\n    self._x = base.variables.get(x_column_name).label\n    self._ys = [base.variables.get(name).label for name in y_columns_name]\n    self.kind = kind\n</code></pre>"},{"location":"reference/comparison/interpolation/#bgc_data_processing.comparison.interpolation.Interpolator.kind","title":"<code>kind = kind</code>  <code>instance-attribute</code>","text":""},{"location":"reference/comparison/interpolation/#bgc_data_processing.comparison.interpolation.Interpolator.interpolate","title":"<code>interpolate(row)</code>","text":"<p>Interpolate a self.reference slice with same index as row.</p> <p>This method is mostly meant to be applied using pd.DataFrame.apply method.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Row to use for interpolation.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Interpolated series with same depth as row.</p> Source code in <code>src/bgc_data_processing/comparison/interpolation.py</code> <pre><code>@with_verbose(trigger_threshold=1, message=\"Interpolating row.\")\ndef interpolate(\n    self,\n    row: pd.Series,\n) -&gt; pd.Series:\n\"\"\"Interpolate a self.reference slice with same index as row.\n\n    This method is mostly meant to be applied using pd.DataFrame.apply method.\n\n    Parameters\n    ----------\n    row : pd.Series\n        Row to use for interpolation.\n\n    Returns\n    -------\n    pd.Series\n        Interpolated series with same depth as row.\n    \"\"\"\n    ref_slice: pd.DataFrame = self._storer.data.loc[row.name, :]\n    obs_depth = row[self._x]\n    ref_depths = ref_slice[self._x]\n    if np.isnan(obs_depth):\n        return self._handle_nan_depth(ref_slice, obs_depth, row.name)\n    if obs_depth &gt; ref_depths.max():\n        return self._handle_outbound_max(ref_slice, obs_depth, row.name)\n    if obs_depth &lt; ref_depths.min():\n        return self._handle_outbound_min(ref_slice, obs_depth, row.name)\n    return self._interpolate(ref_slice, obs_depth, row.name)\n</code></pre>"},{"location":"reference/comparison/interpolation/#bgc_data_processing.comparison.interpolation.Interpolator.interpolate_storer","title":"<code>interpolate_storer(observations_storer)</code>","text":"<p>Interpolate over all rows a Store's dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>observations_storer</code> <code>Storer</code> <p>Storer to interpolate on each row.</p> required <p>Returns:</p> Type Description <code>Storer</code> <p>Storer with interpolated rows to match observations_storer's x values.</p> Source code in <code>src/bgc_data_processing/comparison/interpolation.py</code> <pre><code>@with_verbose(\n    trigger_threshold=0,\n    message=\"Interpolating Data to match observations' depth values.\",\n)\ndef interpolate_storer(\n    self,\n    observations_storer: Storer,\n) -&gt; Storer:\n\"\"\"Interpolate over all rows a Store's dataframe.\n\n    Parameters\n    ----------\n    observations_storer : Storer\n        Storer to interpolate on each row.\n\n    Returns\n    -------\n    Storer\n        Storer with interpolated rows to match observations_storer's x values.\n    \"\"\"\n    obs_data = observations_storer.data\n    matching_index = obs_data.loc[self._storer.data.index.unique(), :]\n    interpolated_df = matching_index.apply(self.interpolate, axis=1)\n    return Storer(\n        data=interpolated_df,\n        category=self._storer.category,\n        providers=self._storer.providers,\n        variables=self._storer.variables,\n    )\n</code></pre>"},{"location":"reference/comparison/matching/","title":"<code>bgc_data_processing.comparison.matching</code>","text":"<p>Data selectors objects.</p>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveABFileLoader","title":"<code>SelectiveABFileLoader(provider_name, category, exclude, variables, grid_basename)</code>","text":"<p>             Bases: <code>ABFileLoader</code></p> <p>Load ABFile only on given points.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Data provider name.</p> required <code>category</code> <code>str</code> <p>Category provider belongs to.</p> required <code>exclude</code> <code>list[str]</code> <p>Filenames to exclude from loading.</p> required <code>variables</code> <code>LoadingVariablesSet</code> <p>Storer object containing all variables to consider for this data, both the one in the data file but and the one not represented in the file.</p> required <code>grid_basename</code> <code>str</code> <p>Basename of the ab grid grid file for the loader. =&gt; files are considered to be loaded over the same grid.</p> required Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def __init__(\n    self,\n    provider_name: str,\n    category: str,\n    exclude: list[str],\n    variables: \"LoadingVariablesSet\",\n    grid_basename: str,\n) -&gt; None:\n    super().__init__(\n        provider_name=provider_name,\n        category=category,\n        exclude=exclude,\n        variables=variables,\n        grid_basename=grid_basename,\n    )\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveABFileLoader.load","title":"<code>load(filepath, constraints, mask)</code>","text":"<p>Load a abfiles from basename.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Path to the basename of the file to load.</p> required <code>constraints</code> <code>Constraints</code> <p>Constraints slicer.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame corresponding to the file.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def load(\n    self,\n    filepath: Path | str,\n    constraints: Constraints,\n    mask: \"Mask\",\n) -&gt; pd.DataFrame:\n\"\"\"Load a abfiles from basename.\n\n    Parameters\n    ----------\n    filepath: Path | str\n        Path to the basename of the file to load.\n    constraints : Constraints, optional\n        Constraints slicer.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame corresponding to the file.\n    \"\"\"\n    self._index = mask.index\n    basename = ABFileLoader.convert_filepath_to_basename(filepath)\n    raw_data = self._read(basename=str(basename), mask=mask)\n    # transform thickness in depth\n    with_depth = self._create_depth_column(raw_data)\n    # create date columns\n    with_dates = self._set_date_related_columns(with_depth, basename)\n    # converts types\n    typed = self._convert_types(with_dates)\n    # apply corrections\n    corrected = self._correct(typed)\n    # apply constraints\n    constrained = constraints.apply_constraints_to_dataframe(corrected)\n    return self.remove_nan_rows(constrained)\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveABFileLoader.from_abloader","title":"<code>from_abloader(loader)</code>  <code>classmethod</code>","text":"<p>Create a Selective loader based on an existing loader.</p> <p>Parameters:</p> Name Type Description Default <code>loader</code> <code>ABFileLoader</code> <p>Loader to use as reference.</p> required <p>Returns:</p> Type Description <code>SelectiveABFileLoader</code> <p>Selective Loader.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>@classmethod\ndef from_abloader(\n    cls,\n    loader: ABFileLoader,\n) -&gt; \"SelectiveABFileLoader\":\n\"\"\"Create a Selective loader based on an existing loader.\n\n    Parameters\n    ----------\n    loader : ABFileLoader\n        Loader to use as reference.\n\n    Returns\n    -------\n    SelectiveABFileLoader\n        Selective Loader.\n    \"\"\"\n    return SelectiveABFileLoader(\n        provider_name=loader.provider,\n        category=loader.category,\n        exclude=loader.excluded_filenames,\n        variables=loader.variables,\n        grid_basename=loader.grid_basename,\n    )\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.NearestNeighborStrategy","title":"<code>NearestNeighborStrategy(**model_kwargs)</code>","text":"<p>Implement a closest point search using NearestNeighbor algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>**model_kwargs</code> <p>Additional arguments to pass to sklearn.neighbors.NearestNeighbors. The value of 'n_neighbors' while be overridden by 1.</p> <code>{}</code> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def __init__(self, **model_kwargs) -&gt; None:\n    model_kwargs[\"n_neighbors\"] = 1\n    self.model_kwargs = model_kwargs\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.NearestNeighborStrategy.model_kwargs","title":"<code>model_kwargs = model_kwargs</code>  <code>instance-attribute</code>","text":""},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.NearestNeighborStrategy.name","title":"<code>name: str</code>  <code>property</code>","text":"<p>Strategy name.</p>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.NearestNeighborStrategy.get_closest_indexes","title":"<code>get_closest_indexes(simulations_lat_lon, observations_lat_lon)</code>","text":"<p>Find closest simulation point for each observation point.</p> <p>Parameters:</p> Name Type Description Default <code>simulations_lat_lon</code> <code>DataFrame</code> <p>DataFrame with longitude and latitude for each simulations point.</p> required <code>observations_lat_lon</code> <code>DataFrame</code> <p>DataFrame with longitude and latitude for each observation point.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Index of closest point for every observation point.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>@with_verbose(\n    trigger_threshold=2,\n    message=f\"Closest index selection using {_strategy_name} strategy.\",\n)\ndef get_closest_indexes(\n    self,\n    simulations_lat_lon: pd.DataFrame,\n    observations_lat_lon: pd.DataFrame,\n) -&gt; pd.Series:\n\"\"\"Find closest simulation point for each observation point.\n\n    Parameters\n    ----------\n    simulations_lat_lon : pd.DataFrame\n        DataFrame with longitude and latitude for each simulations point.\n    observations_lat_lon : pd.DataFrame\n        DataFrame with longitude and latitude for each observation point.\n\n    Returns\n    -------\n    pd.Series\n        Index of closest point for every observation point.\n    \"\"\"\n    model = NearestNeighbors(**self.model_kwargs)\n    # Transforming to radian for haversine metric compatibility\n    sim_radians = simulations_lat_lon * np.pi / 180\n    obs_radians = observations_lat_lon * np.pi / 180\n    model.fit(X=sim_radians)\n    closest = model.kneighbors(\n        obs_radians,\n        return_distance=False,\n    )\n    return pd.Series(closest.flatten(), index=observations_lat_lon.index)\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Mask","title":"<code>Mask(mask_2d, index_2d)</code>","text":"<p>Mask to apply to ABFiles to filter data while loading.</p> <p>Parameters:</p> Name Type Description Default <code>mask_2d</code> <code>ndarray</code> <p>2D array to mask layers when loading them.</p> required <code>index_2d</code> <code>ndarray</code> <p>2D array of indexes to use to reindex the filtered array.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the mask and the index have a different shape.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def __init__(self, mask_2d: np.ndarray, index_2d: np.ndarray) -&gt; None:\n    self._index_2d = index_2d\n    self.mask = mask_2d\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Mask.mask","title":"<code>mask: np.ndarray</code>  <code>property</code> <code>writable</code>","text":"<p>2D boolean mask.</p>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Mask.index","title":"<code>index: pd.Index</code>  <code>property</code>","text":"<p>Index for masked data reindexing.</p> <p>Returns:</p> Type Description <code>Index</code> <p>Data Index.</p>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Mask.__call__","title":"<code>__call__(data_2d, **kwargs)</code>","text":"<p>Apply mask to 2D data.</p> <p>Parameters:</p> Name Type Description Default <code>data_2d</code> <code>ndarray</code> <p>Data to apply the mask to.</p> required <code>**kwargs</code> <p>Additional parameters to pass to pd.Series. The value of 'index' while be overridden by self._index.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Series</code> <p>Masked data as a pd.Series with self._index as index.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def __call__(self, data_2d: np.ndarray, **kwargs) -&gt; pd.Series:\n\"\"\"Apply mask to 2D data.\n\n    Parameters\n    ----------\n    data_2d : np.ndarray\n        Data to apply the mask to.\n    **kwargs:\n        Additional parameters to pass to pd.Series.\n        The value of 'index' while be overridden by self._index.\n\n    Returns\n    -------\n    pd.Series\n        Masked data as a pd.Series with self._index as index.\n    \"\"\"\n    kwargs[\"index\"] = self._index\n    return pd.Series(data_2d[self._mask].flatten(), **kwargs)\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Mask.intersect","title":"<code>intersect(mask_array)</code>","text":"<p>Intersect the mask with another (same-shaped) boolean array.</p> <p>Parameters:</p> Name Type Description Default <code>mask_array</code> <code>ndarray</code> <p>Array to intersect with.</p> required <p>Returns:</p> Type Description <code>Mask</code> <p>New mask whith self._mask &amp; mask_array as mask array.</p> <p>Raises:</p> Type Description <code>IncompatibleMaskShapeError</code> <p>If mask_array has the wrong shape.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def intersect(self, mask_array: np.ndarray) -&gt; \"Mask\":\n\"\"\"Intersect the mask with another (same-shaped) boolean array.\n\n    Parameters\n    ----------\n    mask_array : np.ndarray\n        Array to intersect with.\n\n    Returns\n    -------\n    Mask\n        New mask whith self._mask &amp; mask_array as mask array.\n\n    Raises\n    ------\n    IncompatibleMaskShapeError\n        If mask_array has the wrong shape.\n    \"\"\"\n    if mask_array.shape != self.mask.shape:\n        raise IncompatibleMaskShapeError(self.mask.shape, mask_array.shape)\n    return Mask(\n        mask_2d=self._mask &amp; mask_array,\n        index_2d=self._index_2d,\n    )\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Mask.make_empty","title":"<code>make_empty(grid)</code>  <code>classmethod</code>","text":"<p>Create a Mask with all values True with grid size.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>ABFileGrid</code> <p>ABFileGrid to use to have the grid size.</p> required <p>Returns:</p> Type Description <code>Mask</code> <p>Mask with only True values.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>@classmethod\ndef make_empty(cls, grid: ABFileGrid) -&gt; \"Mask\":\n\"\"\"Create a Mask with all values True with grid size.\n\n    Parameters\n    ----------\n    grid : ABFileGrid\n        ABFileGrid to use to have the grid size.\n\n    Returns\n    -------\n    Mask\n        Mask with only True values.\n    \"\"\"\n    return Mask(\n        mask_2d=np.full((grid.jdm, grid.idm), True),\n        index_2d=np.array(range(grid.jdm * grid.idm)),\n    )\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Match","title":"<code>Match(obs_closests_indexes)</code>","text":"<p>Match between observation indexes and simulations indexes.</p> <p>Parameters:</p> Name Type Description Default <code>obs_closests_indexes</code> <code>Series</code> <p>Closest simulated point index Series. The index is supposed to correspond to observations' index.</p> required Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def __init__(self, obs_closests_indexes: pd.Series) -&gt; None:\n    index_link = obs_closests_indexes.to_frame(name=self.index_simulated)\n    index_link.index.name = self.index_observed\n    index_link.reset_index(inplace=True)\n    self.index_link = index_link\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Match.index_simulated","title":"<code>index_simulated: str = 'sim_index'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Match.index_observed","title":"<code>index_observed: str = 'obs_index'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Match.index_loaded","title":"<code>index_loaded: str = 'load_index'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Match.index_link","title":"<code>index_link = index_link</code>  <code>instance-attribute</code>","text":""},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.Match.match","title":"<code>match(loaded_df)</code>","text":"<p>Transform the DataFrame index to link it to observations' index.</p> <p>Parameters:</p> Name Type Description Default <code>loaded_df</code> <code>DataFrame</code> <p>DataFrame to change the index of.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Copy of loaded_df with a modified index, which correspond to observations' index values.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>@with_verbose(trigger_threshold=1, message=\"Matching indexes.\")\ndef match(self, loaded_df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Transform the DataFrame index to link it to observations' index.\n\n    Parameters\n    ----------\n    loaded_df : pd.DataFrame\n        DataFrame to change the index of.\n\n    Returns\n    -------\n    pd.DataFrame\n        Copy of loaded_df with a modified index, which correspond to\n        observations' index values.\n    \"\"\"\n    loaded_index = pd.Series(loaded_df.index, name=self.index_simulated).to_frame()\n    loaded_index.index.name = self.index_loaded\n    loaded_index.reset_index(inplace=True)\n    loaded_copy = loaded_df.copy()\n    loaded_copy.index = loaded_index.index\n    merge = pd.merge(\n        left=loaded_index,\n        right=self.index_link,\n        left_on=self.index_simulated,\n        right_on=self.index_simulated,\n        how=\"left\",\n    )\n    reshaped = loaded_copy.loc[merge[self.index_loaded], :]\n    reshaped.index = merge[self.index_observed].values\n    return reshaped\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource","title":"<code>SelectiveDataSource(reference, strategy, provider_name, data_format, dirin, data_category, excluded_files, files_pattern, variable_ensemble, **kwargs)</code>","text":"<p>             Bases: <code>DataSource</code></p> <p>Selective Data Source.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>DataFrame</code> <p>Reference Dataframe (observations).</p> required <code>strategy</code> <code>NearestNeighborStrategy</code> <p>Closer point finding strategy.</p> required <code>provider_name</code> <code>str</code> <p>Name of the data provider.</p> required <code>data_format</code> <code>str</code> <p>Data format.</p> required <code>dirin</code> <code>Path | str</code> <p>Input data directory.</p> required <code>data_category</code> <code>str</code> <p>Category of the data.</p> required <code>excluded_files</code> <code>list[str]</code> <p>Files not to load.</p> required <code>files_pattern</code> <code>FileNamePattern</code> <p>Pattern to match to load files.</p> required <code>variable_ensemble</code> <code>SourceVariableSet</code> <p>Ensembles of variables to consider.</p> required Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def __init__(\n    self,\n    reference: \"Storer\",\n    strategy: NearestNeighborStrategy,\n    provider_name: str,\n    data_format: str,\n    dirin: Path | str,\n    data_category: str,\n    excluded_files: list[str],\n    files_pattern: \"FileNamePattern\",\n    variable_ensemble: \"SourceVariableSet\",\n    **kwargs,\n) -&gt; None:\n    super().__init__(\n        provider_name,\n        data_format,\n        dirin,\n        data_category,\n        excluded_files,\n        files_pattern,\n        variable_ensemble,\n        **kwargs,\n    )\n    self.reference = reference.data\n    self.strategy = strategy\n    self.grid = self.loader.grid_file\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource.reference","title":"<code>reference = reference.data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource.strategy","title":"<code>strategy = strategy</code>  <code>instance-attribute</code>","text":""},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource.grid","title":"<code>grid = self.loader.grid_file</code>  <code>instance-attribute</code>","text":""},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource.get_coord","title":"<code>get_coord(var_name)</code>","text":"<p>Get a coordinate field from loader.grid_file.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>Name of the variable to retrieve.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Loaded variable as pd.Series.</p> <p>Raises:</p> Type Description <code>ABFileLoadingError</code> <p>If the variable dosn't exist in the grid file.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def get_coord(self, var_name: str) -&gt; pd.Series:\n\"\"\"Get a coordinate field from loader.grid_file.\n\n    Parameters\n    ----------\n    var_name : str\n        Name of the variable to retrieve.\n\n    Returns\n    -------\n    pd.Series\n        Loaded variable as pd.Series.\n\n    Raises\n    ------\n    ABFileLoadingError\n        If the variable dosn't exist in the grid file.\n    \"\"\"\n    var = self.loader.variables.get(var_name)\n    found = False\n    for alias, _, _ in var.aliases:\n        if alias in self.grid.fieldnames:\n            mask_2d: np.ma.masked_array = self.grid.read_field(alias)\n            found = True\n            break\n    if not found:\n        error_msg = (\n            f\"Grid File has no data for the variable {var.name}.\"\n            f\"Possible fieldnames are {self.grid.fieldnames}.\"\n        )\n        raise ABFileLoadingError(error_msg)\n    value = mask_2d.filled(np.nan)\n    return pd.Series(value.flatten(), name=var.label)\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource.get_x_y_indexes","title":"<code>get_x_y_indexes()</code>","text":"<p>Get x and y indexes.</p> <p>Returns:</p> Type Description <code>tuple[Series, Series]</code> <p>X indexes series, Y indexes series.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>@with_verbose(trigger_threshold=2, message=\"Collecting grid file's indexes.\")\ndef get_x_y_indexes(self) -&gt; tuple[pd.Series, pd.Series]:\n\"\"\"Get x and y indexes.\n\n    Returns\n    -------\n    tuple[pd.Series, pd.Series]\n        X indexes series, Y indexes series.\n    \"\"\"\n    y_coords, x_coords = np.meshgrid(range(self.grid.idm), range(self.grid.jdm))\n    x_coords_series = pd.Series(x_coords.flatten())\n    y_coords_series = pd.Series(y_coords.flatten())\n    return x_coords_series, y_coords_series\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource.select","title":"<code>select(data_slice)</code>","text":"<p>Select closest points in an abfile using self.strategy.</p> <p>Parameters:</p> Name Type Description Default <code>data_slice</code> <code>DataFrame</code> <p>Sice of data to select from.</p> required <p>Returns:</p> Type Description <code>tuple[Mask, Match]</code> <p>Mask to use for loader, Match to link observations to simulations.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>@with_verbose(trigger_threshold=1, message=\"Selecting Data.\")\ndef select(\n    self,\n    data_slice: pd.DataFrame,\n) -&gt; tuple[\"Mask\", \"Match\"]:\n\"\"\"Select closest points in an abfile using self.strategy.\n\n    Parameters\n    ----------\n    data_slice: pd.DataFrame\n        Sice of data to select from.\n\n    Returns\n    -------\n    tuple[Mask, Match]\n        Mask to use for loader, Match to link observations to simulations.\n    \"\"\"\n    lat_series = self.get_coord(self.loader.variables.latitude_var_name)\n    lon_series = self.get_coord(self.loader.variables.longitude_var_name)\n    sims = pd.concat([lat_series, lon_series], axis=1)\n    x_coords_series, y_coords_series = self.get_x_y_indexes()\n    index = self.strategy.get_closest_indexes(\n        simulations_lat_lon=sims,\n        observations_lat_lon=data_slice[sims.columns],\n    )\n    indexes = np.array(range(self.grid.jdm * self.grid.idm))\n    indexes_2d = indexes.reshape((self.grid.jdm, self.grid.idm))\n    selected_xs = x_coords_series.loc[index.values]\n    selected_ys = y_coords_series.loc[index.values]\n    to_keep = np.full(shape=(self.grid.jdm, self.grid.idm), fill_value=False)\n    to_keep[selected_xs, selected_ys] = True\n    return Mask(to_keep, indexes_2d), Match(index)\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource.parse_date_from_filepath","title":"<code>parse_date_from_filepath(filepath)</code>  <code>staticmethod</code>","text":"<p>Parse date from abfile basename.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>File path.</p> required <p>Returns:</p> Type Description <code>date</code> <p>Corresponding date.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>@staticmethod\n@with_verbose(trigger_threshold=0, message=\"Loading data from [filepath].\")\ndef parse_date_from_filepath(filepath: Path | str) -&gt; dt.date:\n\"\"\"Parse date from abfile basename.\n\n    Parameters\n    ----------\n    filepath : Path | str\n        File path.\n\n    Returns\n    -------\n    dt.date\n        Corresponding date.\n    \"\"\"\n    basename = ABFileLoader.convert_filepath_to_basename(filepath)\n    date_part_basename = Path(basename).name.split(\".\")[-1]\n    date = dt.datetime.strptime(date_part_basename, \"%Y_%j_%H\")\n    return date.date()\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource.get_basenames","title":"<code>get_basenames(constraints)</code>","text":"<p>Return basenames of files matching constraints.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>Constraints</code> <p>Data constraints, only year constraint is used.</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of basenames matching constraints.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def get_basenames(\n    self,\n    constraints: \"Constraints\",\n) -&gt; list[Path]:\n\"\"\"Return basenames of files matching constraints.\n\n    Parameters\n    ----------\n    constraints : Constraints\n        Data constraints, only year constraint is used.\n\n    Returns\n    -------\n    list[Path]\n        List of basenames matching constraints.\n    \"\"\"\n    date_label = self.loader.variables.get(\n        self.loader.variables.date_var_name,\n    ).label\n    date_constraint = constraints.get_constraint_parameters(date_label)\n    pattern_matcher = self._files_pattern.build_from_constraint(date_constraint)\n    pattern_matcher.validate = self.loader.is_file_valid\n    return pattern_matcher.select_matching_filepath(\n        research_directory=self.dirin,\n    )\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource.load_all","title":"<code>load_all(constraints)</code>","text":"<p>Load all files for the loader.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>Constraints</code> <p>Constraints slicer., by default Constraints()</p> required <p>Returns:</p> Type Description <code>Storer</code> <p>Storer for the loaded data.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def load_all(self, constraints: \"Constraints\") -&gt; \"Storer\":\n\"\"\"Load all files for the loader.\n\n    Parameters\n    ----------\n    constraints : Constraints, optional\n        Constraints slicer., by default Constraints()\n\n    Returns\n    -------\n    Storer\n        Storer for the loaded data.\n    \"\"\"\n    date_var_name = self.loader.variables.date_var_name\n    date_var_label = self.loader.variables.get(date_var_name).label\n    filepaths = self.get_basenames(\n        constraints,\n    )\n    datas: list[pd.DataFrame] = []\n    for filepath in filepaths:\n        date = self.parse_date_from_filepath(filepath=filepath)\n        data_slice = self.reference[self.reference[date_var_label].dt.date == date]\n        if data_slice.empty:\n            continue\n        mask, match = self.select(data_slice)\n        sim_data = self.loader.load(\n            filepath,\n            constraints=constraints,\n            mask=mask,\n        )\n        datas.append(match.match(sim_data))\n    concatenated = pd.concat(datas, axis=0)\n    storer = Storer(\n        data=concatenated,\n        category=self.loader.category,\n        providers=[self.loader.provider],\n        variables=self._store_vars,\n    )\n    self._insert_all_features(storer)\n    self._remove_temporary_variables(storer)\n    return storer\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource.load_and_save","title":"<code>load_and_save(saving_directory, dateranges_gen, constraints)</code>","text":"<p>Save all the data before saving it all in the saving directory.</p> <p>Parameters:</p> Name Type Description Default <code>saving_directory</code> <code>Path | str</code> <p>Path to the directory to save in.</p> required <code>dateranges_gen</code> <code>DateRangeGenerator</code> <p>Generator to use to retrieve dateranges.</p> required <code>constraints</code> <code>Constraints</code> <p>Contraints ot apply on data.</p> required Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>def load_and_save(\n    self,\n    saving_directory: Path | str,\n    dateranges_gen: \"DateRangeGenerator\",\n    constraints: \"Constraints\",\n) -&gt; None:\n\"\"\"Save all the data before saving it all in the saving directory.\n\n    Parameters\n    ----------\n    saving_directory : Path | str\n        Path to the directory to save in.\n    dateranges_gen : DateRangeGenerator\n        Generator to use to retrieve dateranges.\n    constraints : Constraints\n        Contraints ot apply on data.\n    \"\"\"\n    storer = self.load_all(constraints=constraints)\n    saver = StorerSaver(storer)\n    saver.save_from_daterange(\n        dateranges_gen=dateranges_gen,\n        saving_directory=Path(saving_directory),\n    )\n</code></pre>"},{"location":"reference/comparison/matching/#bgc_data_processing.comparison.matching.SelectiveDataSource.from_data_source","title":"<code>from_data_source(reference, strategy, dsource)</code>  <code>classmethod</code>","text":"<p>Create the sleective data source from an existing data source.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>Storer</code> <p>Reference Dataframe (observations).</p> required <code>strategy</code> <code>NearestNeighborStrategy</code> <p>Closer point finding strategy.</p> required <code>dsource</code> <code>DataSource</code> <p>Template DataSource</p> required <p>Returns:</p> Type Description <code>SelectiveDataSource</code> <p>Selective datasource from Template.</p> Source code in <code>src/bgc_data_processing/comparison/matching.py</code> <pre><code>@classmethod\ndef from_data_source(\n    cls,\n    reference: Storer,\n    strategy: NearestNeighborStrategy,\n    dsource: DataSource,\n) -&gt; \"SelectiveDataSource\":\n\"\"\"Create the sleective data source from an existing data source.\n\n    Parameters\n    ----------\n    reference : Storer\n        Reference Dataframe (observations).\n    strategy : NearestNeighborStrategy\n        Closer point finding strategy.\n    dsource : DataSource\n        Template DataSource\n\n    Returns\n    -------\n    SelectiveDataSource\n        Selective datasource from Template.\n    \"\"\"\n    return cls(\n        reference=reference,\n        strategy=strategy,\n        **dsource.as_template,\n    )\n</code></pre>"},{"location":"reference/comparison/metrics/","title":"<code>bgc_data_processing.comparison.metrics</code>","text":"<p>Metrics to evaluate Simulations against observations.</p>"},{"location":"reference/comparison/metrics/#bgc_data_processing.comparison.metrics.BaseMetric","title":"<code>BaseMetric(variables_to_evaluate)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class to implement metrics.</p> <p>Parameters:</p> Name Type Description Default <code>variables_to_evaluate</code> <code>list[str]</code> <p>List of the names of the variables to use to evaluate.</p> required Source code in <code>src/bgc_data_processing/comparison/metrics.py</code> <pre><code>def __init__(self, variables_to_evaluate: list[str]) -&gt; None:\n    self._eval_vars = variables_to_evaluate\n</code></pre>"},{"location":"reference/comparison/metrics/#bgc_data_processing.comparison.metrics.BaseMetric.metric_name","title":"<code>metric_name: str</code>  <code>instance-attribute</code>","text":""},{"location":"reference/comparison/metrics/#bgc_data_processing.comparison.metrics.BaseMetric.evaluate","title":"<code>evaluate(observations, simulations)</code>","text":"<p>Evaluate observations dataframe against simulations.</p> <p>Parameters:</p> Name Type Description Default <code>observations</code> <code>DataFrame</code> <p>Observations dataframe.</p> required <code>simulations</code> <code>DataFrame</code> <p>Simulations dataframe.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Evaluation result, for every column.</p> Source code in <code>src/bgc_data_processing/comparison/metrics.py</code> <pre><code>def evaluate(\n    self,\n    observations: pd.DataFrame,\n    simulations: pd.DataFrame,\n) -&gt; pd.Series:\n\"\"\"Evaluate observations dataframe against simulations.\n\n    Parameters\n    ----------\n    observations : pd.DataFrame\n        Observations dataframe.\n    simulations : pd.DataFrame\n        Simulations dataframe.\n\n    Returns\n    -------\n    pd.Series\n        Evaluation result, for every column.\n    \"\"\"\n    result = self._eval(observations=observations, simulations=simulations)\n    result.name = self.metric_name\n    return result\n</code></pre>"},{"location":"reference/comparison/metrics/#bgc_data_processing.comparison.metrics.BaseMetric.evaluate_storers","title":"<code>evaluate_storers(observations_storer, simulations_storer)</code>","text":"<p>Evaluate two storers against each other.</p> <p>Parameters:</p> Name Type Description Default <code>observations_storer</code> <code>Storer</code> <p>Observations storer.</p> required <code>simulations_storer</code> <code>Storer</code> <p>Simulations storer.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Result for every column.</p> <p>Raises:</p> Type Description <code>IncomparableStorersError</code> <p>If the storers have different shapes.</p> Source code in <code>src/bgc_data_processing/comparison/metrics.py</code> <pre><code>def evaluate_storers(\n    self,\n    observations_storer: Storer,\n    simulations_storer: Storer,\n) -&gt; pd.Series:\n\"\"\"Evaluate two storers against each other.\n\n    Parameters\n    ----------\n    observations_storer : Storer\n        Observations storer.\n    simulations_storer : Storer\n        Simulations storer.\n\n    Returns\n    -------\n    pd.Series\n        Result for every column.\n\n    Raises\n    ------\n    IncomparableStorersError\n        If the storers have different shapes.\n    \"\"\"\n    obs_vars = observations_storer.variables\n    obs_eval_labels = [obs_vars.get(name).label for name in self._eval_vars]\n    sim_vars = simulations_storer.variables\n    sim_eval_labels = [sim_vars.get(name).label for name in self._eval_vars]\n    obs_df = observations_storer.data.filter(obs_eval_labels, axis=1)\n    sim_df = simulations_storer.data.filter(sim_eval_labels, axis=1)\n\n    if obs_df.shape != sim_df.shape:\n        error_msg = (\n            f\"DataFrame shapes don't match(observations: {obs_df.shape} \"\n            \"- simulations: {sim_df.shape}) -&gt; make sure both storer have \"\n            \"the variables to evaluate on\"\n            f\" (variables: {self._eval_vars})\"\n        )\n        raise IncomparableStorersError(error_msg)\n\n    nans = obs_df.isna().all(axis=1) | sim_df.isna().all(axis=1)\n    return self.evaluate(observations=obs_df[~nans], simulations=sim_df[~nans])\n</code></pre>"},{"location":"reference/comparison/metrics/#bgc_data_processing.comparison.metrics.RMSE","title":"<code>RMSE(variables_to_evaluate)</code>","text":"<p>             Bases: <code>BaseMetric</code></p> <p>Root-Mean Square Error (RMSE).</p>"},{"location":"reference/comparison/metrics/#bgc_data_processing.comparison.metrics.RMSE--see-also","title":"See Also","text":"<p>https://en.wikipedia.org/wiki/Root-mean-square_deviation</p> <p>Parameters:</p> Name Type Description Default <code>variables_to_evaluate</code> <code>list[str]</code> <p>List of the names of the variables to use to evaluate.</p> required Source code in <code>src/bgc_data_processing/comparison/metrics.py</code> <pre><code>def __init__(self, variables_to_evaluate: list[str]) -&gt; None:\n    super().__init__(variables_to_evaluate)\n</code></pre>"},{"location":"reference/comparison/metrics/#bgc_data_processing.comparison.metrics.RMSE.metric_name","title":"<code>metric_name = 'RMSE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/comparison/metrics/#bgc_data_processing.comparison.metrics.Bias","title":"<code>Bias(variables_to_evaluate)</code>","text":"<p>             Bases: <code>BaseMetric</code></p> <p>Bias.</p>"},{"location":"reference/comparison/metrics/#bgc_data_processing.comparison.metrics.Bias--see-also","title":"See Also","text":"<p>https://en.wikipedia.org/wiki/Bias_of_an_estimator</p> <p>Parameters:</p> Name Type Description Default <code>variables_to_evaluate</code> <code>list[str]</code> <p>List of the names of the variables to use to evaluate.</p> required Source code in <code>src/bgc_data_processing/comparison/metrics.py</code> <pre><code>def __init__(self, variables_to_evaluate: list[str]) -&gt; None:\n    super().__init__(variables_to_evaluate)\n</code></pre>"},{"location":"reference/comparison/metrics/#bgc_data_processing.comparison.metrics.Bias.metric_name","title":"<code>metric_name = 'Bias'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/","title":"<code>bgc_data_processing.core</code>","text":"<p>Core module of <code>bgc_data_processing</code> to manipulate data.</p> <p>Please note that this module's main functionalities are accessible from the main <code>bgc_data_processing</code> namespace.</p> <p>From this namespace are accessible:</p> <ul> <li><code>filtering</code>   -&gt; filtering and slicing related objects for bgc data</li> <li><code>io</code>          -&gt; input/output related objects</li> <li><code>loaders</code>     -&gt; data loading-related objects</li> <li><code>sources</code>     -&gt; provider-definition related objects</li> <li><code>storers</code>     -&gt; biogeochemical data-realted storing objects</li> <li><code>variables</code>   -&gt; variables-related storing objects</li> </ul>"},{"location":"reference/core/filtering/","title":"<code>bgc_data_processing.core.filtering</code>","text":"<p>Extract data from storers with given conditions.</p>"},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints","title":"<code>Constraints()</code>","text":"<p>Slicer object to slice dataframes.</p> <p>Initiate slicer object to slice dataframes.</p> Source code in <code>src/bgc_data_processing/core/filtering.py</code> <pre><code>def __init__(self) -&gt; None:\n\"\"\"Initiate slicer object to slice dataframes.\"\"\"\n    self.boundaries: dict[str, dict[str, int | float | datetime]] = {}\n    self.supersets: dict[str, list] = {}\n    self.constraints: dict[str, Callable] = {}\n    self.polygons: list[dict[str, str | Polygon]] = []\n</code></pre>"},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.boundaries","title":"<code>boundaries: dict[str, dict[str, int | float | datetime]] = {}</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.supersets","title":"<code>supersets: dict[str, list] = {}</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.constraints","title":"<code>constraints: dict[str, Callable] = {}</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.polygons","title":"<code>polygons: list[dict[str, str | Polygon]] = []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.reset","title":"<code>reset()</code>","text":"<p>Reset all defined constraints.</p> Source code in <code>src/bgc_data_processing/core/filtering.py</code> <pre><code>def reset(self) -&gt; None:\n\"\"\"Reset all defined constraints.\"\"\"\n    self.boundaries = {}\n    self.supersets = {}\n    self.constraints = {}\n</code></pre>"},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.add_boundary_constraint","title":"<code>add_boundary_constraint(field_label, minimal_value=np.nan, maximal_value=np.nan)</code>","text":"<p>Add a constraint of type 'boundary'.</p> <p>Parameters:</p> Name Type Description Default <code>field_label</code> <code>str</code> <p>Name of the column to apply the constraint to.</p> required <code>minimal_value</code> <code>int | float | datetime</code> <p>Minimum value for the column., by default np.nan</p> <code>nan</code> <code>maximal_value</code> <code>int | float | datetime</code> <p>Maximum value for the column., by default np.nan</p> <code>nan</code> Source code in <code>src/bgc_data_processing/core/filtering.py</code> <pre><code>def add_boundary_constraint(\n    self,\n    field_label: str,\n    minimal_value: int | float | datetime = np.nan,\n    maximal_value: int | float | datetime = np.nan,\n) -&gt; None:\n\"\"\"Add a constraint of type 'boundary'.\n\n    Parameters\n    ----------\n    field_label : str\n        Name of the column to apply the constraint to.\n    minimal_value : int | float | datetime, optional\n        Minimum value for the column., by default np.nan\n    maximal_value : int | float | datetime, optional\n        Maximum value for the column., by default np.nan\n    \"\"\"\n    is_min_nan = isinstance(minimal_value, float) and np.isnan(minimal_value)\n    is_max_nan = isinstance(maximal_value, float) and np.isnan(maximal_value)\n    if not (is_min_nan and is_max_nan):\n        self.boundaries[field_label] = {\n            \"min\": minimal_value,\n            \"max\": maximal_value,\n        }\n</code></pre>"},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.add_superset_constraint","title":"<code>add_superset_constraint(field_label, values_superset=None)</code>","text":"<p>Add a constrainte of type 'superset'.</p> <p>Parameters:</p> Name Type Description Default <code>field_label</code> <code>str</code> <p>Name of the column to apply the constraint to.</p> required <code>values_superset</code> <code>list[Any] | None</code> <p>All the values that the column can take. If empty, no constraint will be applied., by default None</p> <code>None</code> Source code in <code>src/bgc_data_processing/core/filtering.py</code> <pre><code>def add_superset_constraint(\n    self,\n    field_label: str,\n    values_superset: list[Any] | None = None,\n) -&gt; None:\n\"\"\"Add a constrainte of type 'superset'.\n\n    Parameters\n    ----------\n    field_label : str\n        Name of the column to apply the constraint to.\n    values_superset : list[Any] | None\n        All the values that the column can take.\n        If empty, no constraint will be applied., by default None\n    \"\"\"\n    if values_superset is None:\n        values_superset = []\n    if values_superset:\n        self.supersets[field_label] = values_superset\n</code></pre>"},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.add_polygon_constraint","title":"<code>add_polygon_constraint(latitude_field, longitude_field, polygon)</code>","text":"<p>Add a polygon constraint.</p> <p>Parameters:</p> Name Type Description Default <code>latitude_field</code> <code>str</code> <p>Name of the latitude-related field.</p> required <code>longitude_field</code> <code>str</code> <p>Name of the longitude-related field.</p> required <code>polygon</code> <code>Polygon</code> <p>Polygon to use as boundary.</p> required Source code in <code>src/bgc_data_processing/core/filtering.py</code> <pre><code>def add_polygon_constraint(\n    self,\n    latitude_field: str,\n    longitude_field: str,\n    polygon: Polygon,\n) -&gt; None:\n\"\"\"Add a polygon constraint.\n\n    Parameters\n    ----------\n    latitude_field : str\n        Name of the latitude-related field.\n    longitude_field : str\n        Name of the longitude-related field.\n    polygon : Polygon\n        Polygon to use as boundary.\n    \"\"\"\n    constraint_dict = {\n        \"latitude_field\": latitude_field,\n        \"longitude_field\": longitude_field,\n        \"polygon\": polygon,\n    }\n    self.polygons.append(constraint_dict)\n</code></pre>"},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.apply_constraints_to_storer","title":"<code>apply_constraints_to_storer(storer)</code>","text":"<p>Apply all constraints to a DataFrame.</p> <p>The index of the previous Storer's dataframe are conserved.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>DataFrame</code> <p>Storer to apply the constraints to.</p> required <p>Returns:</p> Type Description <code>Storer</code> <p>New storer with equivalent paramters and updated data.</p> Source code in <code>src/bgc_data_processing/core/filtering.py</code> <pre><code>def apply_constraints_to_storer(self, storer: Storer) -&gt; Storer:\n\"\"\"Apply all constraints to a DataFrame.\n\n    The index of the previous Storer's dataframe are conserved.\n\n    Parameters\n    ----------\n    storer : pd.DataFrame\n        Storer to apply the constraints to.\n\n    Returns\n    -------\n    Storer\n        New storer with equivalent paramters and updated data.\n    \"\"\"\n    return Storer(\n        data=self.apply_constraints_to_dataframe(storer.data),\n        category=storer.category,\n        providers=storer.providers,\n        variables=storer.variables,\n    )\n</code></pre>"},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.apply_constraints_to_dataframe","title":"<code>apply_constraints_to_dataframe(dataframe)</code>","text":"<p>Apply all constraints to a DataFrame.</p> <p>This slice conserves indexes values.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>DataFrame to apply the constraints to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame whose rows verify all constraints or None if inplace=True.</p> Source code in <code>src/bgc_data_processing/core/filtering.py</code> <pre><code>def apply_constraints_to_dataframe(\n    self,\n    dataframe: pd.DataFrame,\n) -&gt; pd.DataFrame | None:\n\"\"\"Apply all constraints to a DataFrame.\n\n    This slice conserves indexes values.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        DataFrame to apply the constraints to.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame whose rows verify all constraints or None if inplace=True.\n    \"\"\"\n    bool_boundaries = self._apply_boundary_constraints(dataframe)\n    bool_supersets = self._apply_superset_constraints(dataframe)\n    bool_polygons = self._apply_polygon_constraints(dataframe)\n    verify_all = bool_boundaries &amp; bool_supersets &amp; bool_polygons\n    return dataframe.loc[verify_all, :]\n</code></pre>"},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.apply_specific_constraint","title":"<code>apply_specific_constraint(field_label, df)</code>","text":"<p>Only apply a single constraint.</p> <p>Parameters:</p> Name Type Description Default <code>field_label</code> <code>str</code> <p>Label of the field to apply the constraint to.</p> required <code>df</code> <code>DataFrame</code> <p>DataFrame to apply the constraints to.</p> required <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>DataFrame whose rows verify all constraints or None if inplace=True.</p> Source code in <code>src/bgc_data_processing/core/filtering.py</code> <pre><code>def apply_specific_constraint(\n    self,\n    field_label: str,\n    df: pd.DataFrame,\n) -&gt; pd.DataFrame | None:\n\"\"\"Only apply a single constraint.\n\n    Parameters\n    ----------\n    field_label : str\n        Label of the field to apply the constraint to.\n    df : pd.DataFrame\n        DataFrame to apply the constraints to.\n\n    Returns\n    -------\n    pd.DataFrame | None\n        DataFrame whose rows verify all constraints or None if inplace=True.\n    \"\"\"\n    constraint = Constraints()\n    if field_label in self.boundaries:\n        constraint.add_boundary_constraint(\n            field_label=field_label,\n            minimal_value=self.boundaries[field_label][\"min\"],\n            maximal_value=self.boundaries[field_label][\"max\"],\n        )\n    if field_label in self.supersets:\n        constraint.add_superset_constraint(\n            field_label=field_label,\n            value_superset=self.supersets[field_label],\n        )\n    return constraint.apply_constraints_to_dataframe(dataframe=df)\n</code></pre>"},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.is_constrained","title":"<code>is_constrained(field_name)</code>","text":"<p>Return True if 'field_name' is constrained.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Field to name to test the constraint.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the field has a constraint.</p> Source code in <code>src/bgc_data_processing/core/filtering.py</code> <pre><code>def is_constrained(self, field_name: str) -&gt; bool:\n\"\"\"Return True if 'field_name' is constrained.\n\n    Parameters\n    ----------\n    field_name : str\n        Field to name to test the constraint.\n\n    Returns\n    -------\n    bool\n        True if the field has a constraint.\n    \"\"\"\n    in_boundaries = field_name in self.boundaries\n    in_supersets = field_name in self.supersets\n    return in_boundaries or in_supersets\n</code></pre>"},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.get_constraint_parameters","title":"<code>get_constraint_parameters(field_name)</code>","text":"<p>Return the constraints on 'field_name'.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Field to get the constraint of.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionnary with keys 'boundary' and/or 'superset' if constraints exist.</p> Source code in <code>src/bgc_data_processing/core/filtering.py</code> <pre><code>def get_constraint_parameters(self, field_name: str) -&gt; dict:\n\"\"\"Return the constraints on 'field_name'.\n\n    Parameters\n    ----------\n    field_name : str\n        Field to get the constraint of.\n\n    Returns\n    -------\n    dict\n        Dictionnary with keys 'boundary' and/or 'superset' if constraints exist.\n    \"\"\"\n    constraint_params = {}\n    if field_name in self.boundaries:\n        constraint_params[\"boundary\"] = self.boundaries[field_name]\n    if field_name in self.supersets:\n        constraint_params[\"superset\"] = self.supersets[field_name]\n    return constraint_params\n</code></pre>"},{"location":"reference/core/filtering/#bgc_data_processing.core.filtering.Constraints.get_extremes","title":"<code>get_extremes(field_name, default_min=None, default_max=None)</code>","text":"<p>Return extreme values as they appear in the constraints.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field to get the extreme of.</p> required <code>default_min</code> <code>int | float | datetime</code> <p>Default value for the minimum if not constraint exists., by default None</p> <code>None</code> <code>default_max</code> <code>int | float | datetime</code> <p>Default value for the maximum if not constraint exists., by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[int | float | datetime, int | float | datetime]</code> <p>Minimum value, maximum value</p> Source code in <code>src/bgc_data_processing/core/filtering.py</code> <pre><code>def get_extremes(\n    self,\n    field_name: str,\n    default_min: int | float | datetime | None = None,\n    default_max: int | float | datetime | None = None,\n) -&gt; tuple[int | float | datetime, int | float | datetime]:\n\"\"\"Return extreme values as they appear in the constraints.\n\n    Parameters\n    ----------\n    field_name : str\n        Name of the field to get the extreme of.\n    default_min : int | float | datetime, optional\n        Default value for the minimum if not constraint exists., by default None\n    default_max : int | float | datetime, optional\n        Default value for the maximum if not constraint exists., by default None\n\n    Returns\n    -------\n    tuple[int | float | datetime, int | float | datetime]\n        Minimum value, maximum value\n    \"\"\"\n    if not self.is_constrained(field_name=field_name):\n        return default_min, default_max\n    constraints = self.get_constraint_parameters(field_name=field_name)\n    boundary_in = \"boundary\" in constraints\n    superset_in = \"superset\" in constraints\n    if boundary_in and superset_in and constraints[\"superset\"]:\n        b_min = constraints[\"boundary\"][\"min\"]\n        b_max = constraints[\"boundary\"][\"max\"]\n        s_min = min(constraints[\"superset\"])\n        s_max = max(constraints[\"superset\"])\n        all_min = min(b_min, s_min)\n        all_max = max(b_max, s_max)\n    elif not boundary_in:\n        all_min = min(constraints[\"superset\"])\n        all_max = max(constraints[\"superset\"])\n    elif not superset_in:\n        all_min = constraints[\"boundary\"][\"min\"]\n        all_max = constraints[\"boundary\"][\"max\"]\n    return all_min, all_max\n</code></pre>"},{"location":"reference/core/sources/","title":"<code>bgc_data_processing.core.sources</code>","text":"<p>Data Source objects.</p>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource","title":"<code>DataSource(provider_name, data_format, dirin, data_category, excluded_files, files_pattern, variable_ensemble, **kwargs)</code>","text":"<p>Data Source.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the data provider.</p> required <code>data_format</code> <code>str</code> <p>Data format.</p> required <code>dirin</code> <code>Path | str</code> <p>Input data directory.</p> required <code>data_category</code> <code>str</code> <p>Category of the data.</p> required <code>excluded_files</code> <code>list[str]</code> <p>Files not to load.</p> required <code>files_pattern</code> <code>FileNamePattern</code> <p>Pattern to match to load files.</p> required <code>variable_ensemble</code> <code>SourceVariableSet</code> <p>Ensembles of variables to consider.</p> required Source code in <code>src/bgc_data_processing/core/sources.py</code> <pre><code>def __init__(\n    self,\n    provider_name: str,\n    data_format: str,\n    dirin: Path | str,\n    data_category: str,\n    excluded_files: list[str],\n    files_pattern: \"FileNamePattern\",\n    variable_ensemble: \"SourceVariableSet\",\n    **kwargs,\n) -&gt; None:\n    self._format = data_format\n    self._category = data_category\n    self._vars_ensemble = variable_ensemble\n    self._store_vars = variable_ensemble.storing_variables\n    self._files_pattern = files_pattern\n    self._dirin = Path(dirin)\n    self._provider = provider_name\n    self._read_kwargs = kwargs\n    self._prov_name = provider_name\n    self._excl = excluded_files\n    self._loader = None\n</code></pre>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource.as_template","title":"<code>as_template: dict[str, Any]</code>  <code>property</code>","text":"<p>Create template to easily re-create a similar data source.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Arguements to create a similar data source.</p>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource.dirin","title":"<code>dirin: Path</code>  <code>property</code>","text":"<p>Directory with data to load.</p>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource.files_pattern","title":"<code>files_pattern: FileNamePattern</code>  <code>property</code>","text":"<p>Pattern to match for files in input directory.</p>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource.provider","title":"<code>provider: str</code>  <code>property</code>","text":"<p>Name of the data provider.</p>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource.data_format","title":"<code>data_format: str</code>  <code>property</code>","text":"<p>Name of the data format.</p>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource.data_category","title":"<code>data_category: str</code>  <code>property</code>","text":"<p>Name of the data category.</p>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource.variables","title":"<code>variables: SourceVariableSet</code>  <code>property</code>","text":"<p>Ensemble of all variables.</p>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource.loader","title":"<code>loader: BaseLoader</code>  <code>property</code>","text":"<p>Data loader.</p>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource.saving_order","title":"<code>saving_order: list[str]</code>  <code>property</code> <code>writable</code>","text":"<p>Saving Order for variables.</p>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource.load_and_save","title":"<code>load_and_save(saving_directory, dateranges_gen, constraints)</code>","text":"<p>Save data in files as soon as the data is loaded to relieve memory.</p> <p>Parameters:</p> Name Type Description Default <code>saving_directory</code> <code>Path | str</code> <p>Path to the directory to save in.</p> required <code>dateranges_gen</code> <code>DateRangeGenerator</code> <p>Generator to use to retrieve dateranges.</p> required <code>constraints</code> <code>Constraints</code> <p>Contraints ot apply on data.</p> required Source code in <code>src/bgc_data_processing/core/sources.py</code> <pre><code>def load_and_save(\n    self,\n    saving_directory: Path | str,\n    dateranges_gen: \"DateRangeGenerator\",\n    constraints: \"Constraints\",\n) -&gt; None:\n\"\"\"Save data in files as soon as the data is loaded to relieve memory.\n\n    Parameters\n    ----------\n    saving_directory : Path | str\n        Path to the directory to save in.\n    dateranges_gen : DateRangeGenerator\n        Generator to use to retrieve dateranges.\n    constraints : Constraints\n        Contraints ot apply on data.\n    \"\"\"\n    date_label = self._vars_ensemble.get(self._vars_ensemble.date_var_name).label\n    date_constraint = constraints.get_constraint_parameters(date_label)\n    pattern_matcher = self._files_pattern.build_from_constraint(date_constraint)\n    pattern_matcher.validate = self.loader.is_file_valid\n    filepaths = pattern_matcher.select_matching_filepath(\n        research_directory=self._dirin,\n    )\n    for filepath in filepaths:\n        storer = self._create_storer(filepath=filepath, constraints=constraints)\n        saver = StorerSaver(storer)\n        saver.save_from_daterange(\n            dateranges_gen=dateranges_gen,\n            saving_directory=Path(saving_directory),\n        )\n</code></pre>"},{"location":"reference/core/sources/#bgc_data_processing.core.sources.DataSource.load_all","title":"<code>load_all(constraints)</code>","text":"<p>Load all files for the loader.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>Constraints</code> <p>Constraints slicer., by default Constraints()</p> required <p>Returns:</p> Type Description <code>Storer</code> <p>Storer for the loaded data.</p> Source code in <code>src/bgc_data_processing/core/sources.py</code> <pre><code>def load_all(self, constraints: \"Constraints\") -&gt; \"Storer\":\n\"\"\"Load all files for the loader.\n\n    Parameters\n    ----------\n    constraints : Constraints, optional\n        Constraints slicer., by default Constraints()\n\n    Returns\n    -------\n    Storer\n        Storer for the loaded data.\n    \"\"\"\n    date_label = self._vars_ensemble.get(self._vars_ensemble.date_var_name).label\n    date_constraint = constraints.get_constraint_parameters(date_label)\n    pattern_matcher = self._files_pattern.build_from_constraint(date_constraint)\n    pattern_matcher.validate = self.loader.is_file_valid\n    filepaths = pattern_matcher.select_matching_filepath(\n        research_directory=self._dirin,\n    )\n    storers = []\n    for filepath in filepaths:\n        storer = self._create_storer(filepath=filepath, constraints=constraints)\n        storers.append(storer)\n    return sum(storers)\n</code></pre>"},{"location":"reference/core/storers/","title":"<code>bgc_data_processing.core.storers</code>","text":"<p>Data storing objects.</p>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer","title":"<code>Storer(data, category, providers, variables)</code>","text":"<p>Storing data class, to keep track of metadata.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to store.</p> required <code>category</code> <code>str</code> <p>Data category.</p> required <code>providers</code> <code>list</code> <p>Names of the data providers.</p> required <code>variables</code> <code>StoringVariablesSet</code> <p>Variables storer of object to keep track of the variables in the Dataframe.</p> required Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    category: str,\n    providers: list,\n    variables: \"StoringVariablesSet\",\n) -&gt; None:\n    self._data = data\n    self._category = category\n    self._providers = providers\n    self._variables = deepcopy(variables)\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.data","title":"<code>data: pd.DataFrame</code>  <code>property</code>","text":"<p>Getter for self._data.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe.</p>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.category","title":"<code>category: str</code>  <code>property</code>","text":"<p>Returns the category of the provider.</p> <p>Returns:</p> Type Description <code>str</code> <p>Category provider belongs to.</p>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.providers","title":"<code>providers: list</code>  <code>property</code>","text":"<p>Getter for self._providers.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of providers.</p>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.variables","title":"<code>variables: StoringVariablesSet</code>  <code>property</code>","text":"<p>Getter for self._variables.</p> <p>Returns:</p> Type Description <code>StoringVariablesSet</code> <p>Variables storer.</p>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of self.</p> <p>Returns:</p> Type Description <code>str</code> <p>Representation of self.data.</p> Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def __repr__(self) -&gt; str:\n\"\"\"Representation of self.\n\n    Returns\n    -------\n    str\n        Representation of self.data.\n    \"\"\"\n    return repr(self.data)\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.__eq__","title":"<code>__eq__(__o)</code>","text":"<p>Test equality with other object.</p> <p>Parameters:</p> Name Type Description Default <code>__o</code> <code>object</code> <p>Object to test equality with.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if is same object only.</p> Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def __eq__(self, __o: object) -&gt; bool:\n\"\"\"Test equality with other object.\n\n    Parameters\n    ----------\n    __o : object\n        Object to test equality with.\n\n    Returns\n    -------\n    bool\n        True if is same object only.\n    \"\"\"\n    return self is __o\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.__radd__","title":"<code>__radd__(other)</code>","text":"<p>Perform right addition.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Any</code> <p>Object to add.</p> required <p>Returns:</p> Type Description <code>Storer</code> <p>Concatenation of both storer's dataframes.</p> Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def __radd__(self, other: Any) -&gt; \"Storer\":\n\"\"\"Perform right addition.\n\n    Parameters\n    ----------\n    other : Any\n        Object to add.\n\n    Returns\n    -------\n    Storer\n        Concatenation of both storer's dataframes.\n    \"\"\"\n    if other == 0:\n        return self\n    return self.__add__(other)\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.__add__","title":"<code>__add__(other)</code>","text":"<p>Perform left addition.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Any</code> <p>Object to add.</p> required <p>Returns:</p> Type Description <code>Storer</code> <p>Concatenation of both storer's dataframes.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If other is not a storer.</p> <code>IncompatibleVariableSetsError</code> <p>If both storers have a different variable set.</p> <code>IncompatibleCategoriesError</code> <p>If both storers have different categories.</p> Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def __add__(self, other: object) -&gt; \"Storer\":\n\"\"\"Perform left addition.\n\n    Parameters\n    ----------\n    other : Any\n        Object to add.\n\n    Returns\n    -------\n    Storer\n        Concatenation of both storer's dataframes.\n\n    Raises\n    ------\n    TypeError\n        If other is not a storer.\n    IncompatibleVariableSetsError\n        If both storers have a different variable set.\n    IncompatibleCategoriesError\n        If both storers have different categories.\n    \"\"\"\n    if not isinstance(other, Storer):\n        error_msg = f\"Can't add CSVStorer object to {type(other)}\"\n        raise TypeError(error_msg)\n    # Assert variables are the same\n    if self.variables != other.variables:\n        error_msg = \"Variables or categories are not compatible\"\n        raise IncompatibleVariableSetsError(error_msg)\n    # Assert categories are the same\n    if self.category != other.category:\n        error_msg = \"Categories are not compatible\"\n        raise IncompatibleCategoriesError(error_msg)\n\n    concat_data = pd.concat([self._data, other.data], ignore_index=True)\n    concat_providers = list(set(self.providers + other.providers))\n    # Return Storer with similar variables\n    return Storer(\n        data=concat_data,\n        category=self.category,\n        providers=concat_providers,\n        variables=self.variables,\n    )\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.remove_duplicates","title":"<code>remove_duplicates(priority_list=None)</code>","text":"<p>Update self._data to remove duplicates in data.</p> <p>Parameters:</p> Name Type Description Default <code>priority_list</code> <code>list</code> <p>Providers priority order, first has priority over others and so on. , by default None</p> <code>None</code> Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def remove_duplicates(self, priority_list: list | None = None) -&gt; None:\n\"\"\"Update self._data to remove duplicates in data.\n\n    Parameters\n    ----------\n    priority_list : list, optional\n        Providers priority order, first has priority over others and so on.\n        , by default None\n    \"\"\"\n    df = self._data\n    df = self._remove_duplicates_among_providers(df)\n    df = self._remove_duplicates_between_providers(df, priority_list=priority_list)\n    self._data = df\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.slice_verbose","title":"<code>slice_verbose(_start_date, _end_date)</code>  <code>staticmethod</code>","text":"<p>Invoke Verbose for slicing.</p> <p>Parameters:</p> Name Type Description Default <code>_start_date</code> <code>date</code> <p>Start date.</p> required <code>_end_date</code> <code>date</code> <p>End date.</p> required Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>@staticmethod\n@with_verbose(\n    trigger_threshold=1,\n    message=\"Slicing data for date range: [_start_date]-[_end_date].\",\n)\ndef slice_verbose(_start_date: dt.date, _end_date: dt.date) -&gt; None:\n\"\"\"Invoke Verbose for slicing.\n\n    Parameters\n    ----------\n    _start_date : dt.date\n        Start date.\n    _end_date : dt.date\n        End date.\n    \"\"\"\n    return\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.slice_on_dates","title":"<code>slice_on_dates(drng)</code>","text":"<p>Slice the Dataframe using the date column.</p> <p>Only returns indexes to use for slicing.</p> <p>Parameters:</p> Name Type Description Default <code>drng</code> <code>Series</code> <p>Two values Series, \"start_date\" and \"end_date\".</p> required <p>Returns:</p> Type Description <code>list</code> <p>Indexes to use for slicing.</p> Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def slice_on_dates(\n    self,\n    drng: pd.Series,\n) -&gt; \"Slice\":\n\"\"\"Slice the Dataframe using the date column.\n\n    Only returns indexes to use for slicing.\n\n    Parameters\n    ----------\n    drng : pd.Series\n        Two values Series, \"start_date\" and \"end_date\".\n\n    Returns\n    -------\n    list\n        Indexes to use for slicing.\n    \"\"\"\n    # Params\n    start_date: dt.datetime = drng[\"start_date\"]\n    end_date: dt.datetime = drng[\"end_date\"]\n    self.slice_verbose(_start_date=start_date.date(), _end_date=end_date.date())\n    dates_col = self._data[self._variables.get(self._variables.date_var_name).label]\n    # slice\n    after_start = dates_col &gt;= start_date\n    before_end = dates_col &lt;= end_date\n    slice_index = dates_col.loc[after_start &amp; before_end].index.values.tolist()\n    return Slice(\n        storer=self,\n        slice_index=slice_index,\n    )\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.add_feature","title":"<code>add_feature(variable, data)</code>","text":"<p>Add a new feature to the storer.</p> <p>Parameters:</p> Name Type Description Default <code>variable</code> <code>NotExistingVar</code> <p>Variable corresponding to the feature.</p> required <code>data</code> <code>Series</code> <p>Feature data.</p> required Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def add_feature(\n    self,\n    variable: \"NotExistingVar\",\n    data: pd.Series,\n) -&gt; None:\n\"\"\"Add a new feature to the storer.\n\n    Parameters\n    ----------\n    variable : NotExistingVar\n        Variable corresponding to the feature.\n    data : pd.Series\n        Feature data.\n    \"\"\"\n    self.variables.add_var(variable)\n    self._data[variable.name] = data\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.pop","title":"<code>pop(variable_name)</code>","text":"<p>Remove and return the data for a given variable.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>Name of the variable to remove.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Data of the corresponding variable.</p> Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def pop(self, variable_name: str) -&gt; pd.Series:\n\"\"\"Remove and return the data for a given variable.\n\n    Parameters\n    ----------\n    variable_name : str\n        Name of the variable to remove.\n\n    Returns\n    -------\n    pd.Series\n        Data of the corresponding variable.\n    \"\"\"\n    var = self.variables.pop(variable_name)\n    return self._data.pop(var.name)\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.from_constraints","title":"<code>from_constraints(storer, constraints)</code>  <code>classmethod</code>","text":"<p>Create a new storer object from an existing storer and constraints.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to modify with constraints.</p> required <code>constraints</code> <code>Constraints</code> <p>Constraints to use to modify the storer.</p> required <p>Returns:</p> Type Description <code>Storer</code> <p>New storer respecting the constraints.</p> Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>@classmethod\ndef from_constraints(\n    cls,\n    storer: \"Storer\",\n    constraints: \"Constraints\",\n) -&gt; \"Storer\":\n\"\"\"Create a new storer object from an existing storer and constraints.\n\n    Parameters\n    ----------\n    storer : Storer\n        Storer to modify with constraints.\n    constraints : Constraints\n        Constraints to use to modify the storer.\n\n    Returns\n    -------\n    Storer\n        New storer respecting the constraints.\n    \"\"\"\n    data = constraints.apply_constraints_to_dataframe(dataframe=storer.data)\n    return Storer(\n        data=data,\n        category=storer.category,\n        providers=storer.providers,\n        variables=storer.variables,\n    )\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Storer.slice_using_index","title":"<code>slice_using_index(index)</code>","text":"<p>Slice Storer using.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>Index values to keep.</p> required <p>Returns:</p> Type Description <code>Storer</code> <p>Corresponding storer.</p> Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def slice_using_index(self, index: pd.Index) -&gt; \"Storer\":\n\"\"\"Slice Storer using.\n\n    Parameters\n    ----------\n    index : pd.Index\n        Index values to keep.\n\n    Returns\n    -------\n    Storer\n        Corresponding storer.\n    \"\"\"\n    return Storer(\n        data=self._data.loc[index, :],\n        category=self._category,\n        providers=self._providers,\n        variables=self._variables,\n    )\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Slice","title":"<code>Slice(storer, slice_index)</code>","text":"<p>             Bases: <code>Storer</code></p> <p>Slice storing object, instance of Storer to inherit of the saving method.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to slice.</p> required <code>slice_index</code> <code>list</code> <p>Indexes to keep from the Storer dataframe.</p> required <p>Slice storing object, instance of Storer to inherit of the saving method.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to slice.</p> required <code>slice_index</code> <code>list</code> <p>Indexes to keep from the Storer dataframe.</p> required Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def __init__(\n    self,\n    storer: Storer,\n    slice_index: list,\n) -&gt; None:\n\"\"\"Slice storing object, instance of Storer to inherit of the saving method.\n\n    Parameters\n    ----------\n    storer : Storer\n        Storer to slice.\n    slice_index : list\n        Indexes to keep from the Storer dataframe.\n    \"\"\"\n    self.slice_index = slice_index\n    self.storer = storer\n    super().__init__(\n        data=storer.data,\n        category=storer.category,\n        providers=storer.providers,\n        variables=storer.variables,\n    )\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Slice.slice_index","title":"<code>slice_index = slice_index</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Slice.storer","title":"<code>storer = storer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Slice.providers","title":"<code>providers: list</code>  <code>property</code>","text":"<p>Getter for self.storer._providers.</p> <p>Returns:</p> Type Description <code>list</code> <p>Providers of the dataframe which the slice comes from.</p>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Slice.variables","title":"<code>variables: list</code>  <code>property</code>","text":"<p>Getter for self.storer._variables.</p> <p>Returns:</p> Type Description <code>list</code> <p>Variables of the dataframe which the slice comes from.</p>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Slice.data","title":"<code>data: pd.DataFrame</code>  <code>property</code>","text":"<p>Getter for self.storer._data.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataframe which the slice comes from.</p>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Slice.__repr__","title":"<code>__repr__()</code>","text":"<p>Represent self as a string.</p> <p>Returns:</p> Type Description <code>str</code> <p>str(slice.index)</p> Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def __repr__(self) -&gt; str:\n\"\"\"Represent self as a string.\n\n    Returns\n    -------\n    str\n        str(slice.index)\n    \"\"\"\n    return str(self.slice_index)\n</code></pre>"},{"location":"reference/core/storers/#bgc_data_processing.core.storers.Slice.__add__","title":"<code>__add__(__o)</code>","text":"<p>Perform left addition.</p> <p>Parameters:</p> Name Type Description Default <code>__o</code> <code>object</code> <p>Object to add.</p> required <p>Returns:</p> Type Description <code>Slice</code> <p>Concatenation of both slices.</p> <p>Raises:</p> Type Description <code>DifferentSliceOriginError</code> <p>If the slices don't originate from same storer.</p> Source code in <code>src/bgc_data_processing/core/storers.py</code> <pre><code>def __add__(self, __o: object) -&gt; \"Slice\":\n\"\"\"Perform left addition.\n\n    Parameters\n    ----------\n    __o : object\n        Object to add.\n\n    Returns\n    -------\n    Slice\n        Concatenation of both slices.\n\n    Raises\n    ------\n    DifferentSliceOriginError\n        If the slices don't originate from same storer.\n    \"\"\"\n    if self.storer != __o.storer:\n        error_msg = \"Addition can only be performed with slice from same CSVStorer\"\n        raise DifferentSliceOriginError(error_msg)\n    new_index = list(set(self.slice_index).union(set(__o.slice_index)))\n    return Slice(self.storer, new_index)\n</code></pre>"},{"location":"reference/core/io/","title":"<code>bgc_data_processing.core.io</code>","text":"<p>Input-Output methods.</p> <p>This module regroup all objects which aim at saving storers or reading data from already saved storers.</p> <p>From this namespace are accessible:</p> <ul> <li><code>read_files</code>  -&gt; File reading function</li> <li><code>save_storer</code> -&gt; Storer saviing function</li> </ul>"},{"location":"reference/core/io/readers/","title":"<code>bgc_data_processing.core.io.readers</code>","text":"<p>Read generated files.</p>"},{"location":"reference/core/io/readers/#bgc_data_processing.core.io.readers.Reader","title":"<code>Reader(filepath, providers_column_label='PROVIDER', expocode_column_label='EXPOCODE', date_column_label='DATE', year_column_label='YEAR', month_column_label='MONTH', day_column_label='DAY', hour_column_label='HOUR', latitude_column_label='LATITUDE', longitude_column_label='LONGITUDE', depth_column_label='DEPH', variables_reference=None, category='in_situ', unit_row_index=1, delim_whitespace=True)</code>","text":"<p>Reading routine to parse csv files.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Path to the file to read.</p> required <code>providers_column_label</code> <code>str</code> <p>Provider column in the dataframe., by default \"PROVIDER\"</p> <code>'PROVIDER'</code> <code>expocode_column_label</code> <code>str</code> <p>Expocode column in the dataframe., by default \"EXPOCODE\"</p> <code>'EXPOCODE'</code> <code>date_column_label</code> <code>str</code> <p>Date column in the dataframe., by default \"DATE\"</p> <code>'DATE'</code> <code>year_column_label</code> <code>str</code> <p>Year column in the dataframe., by default \"YEAR\"</p> <code>'YEAR'</code> <code>month_column_label</code> <code>str</code> <p>Month column in the dataframe., by default \"MONTH\"</p> <code>'MONTH'</code> <code>day_column_label</code> <code>str</code> <p>Day column in the dataframe., by default \"DAY\"</p> <code>'DAY'</code> <code>hour_column_label</code> <code>str</code> <p>Hour column in the dataframe., by default \"HOUR\"</p> <code>'HOUR'</code> <code>latitude_column_label</code> <code>str</code> <p>Latitude column in the dataframe., by default \"LATITUDE\"</p> <code>'LATITUDE'</code> <code>longitude_column_label</code> <code>str</code> <p>Longitude column in the dataframe., by default \"LONGITUDE\"</p> <code>'LONGITUDE'</code> <code>depth_column_label</code> <code>str</code> <p>Depth column in the dataframe., by default \"DEPH\"</p> <code>'DEPH'</code> <code>variables_reference</code> <code>list[BaseVar] | None</code> <p>List of variable to use as reference. If a variable label is a column name,  this variable will be used for the output storer., by default None</p> <code>None</code> <code>category</code> <code>str</code> <p>Category of the loaded file., by default \"in_situ\"</p> <code>'in_situ'</code> <code>unit_row_index</code> <code>int</code> <p>Index of the row with the units, None if there's no unit row., by default 1</p> <code>1</code> <code>delim_whitespace</code> <code>bool</code> <p>Whether to use whitespace as delimiters., by default True</p> <code>True</code> <p>Examples:</p> <p>Loading from a file:</p> <pre><code>&gt;&gt;&gt; filepath = \"path/to/file\"\n&gt;&gt;&gt; reader = Reader(filepath, providers=\"providers_column_name\")\n</code></pre> <p>Getting the storer:</p> <pre><code>&gt;&gt;&gt; storer = reader.get_storer()\n</code></pre> Source code in <code>src/bgc_data_processing/core/io/readers.py</code> <pre><code>def __init__(\n    self,\n    filepath: Path | str,\n    providers_column_label: str = \"PROVIDER\",\n    expocode_column_label: str = \"EXPOCODE\",\n    date_column_label: str = \"DATE\",\n    year_column_label: str = \"YEAR\",\n    month_column_label: str = \"MONTH\",\n    day_column_label: str = \"DAY\",\n    hour_column_label: str = \"HOUR\",\n    latitude_column_label: str = \"LATITUDE\",\n    longitude_column_label: str = \"LONGITUDE\",\n    depth_column_label: str = \"DEPH\",\n    variables_reference: list[BaseVar] | None = None,\n    category: str = \"in_situ\",\n    unit_row_index: int = 1,\n    delim_whitespace: bool = True,\n):\n    if variables_reference is None:\n        variables_reference: dict[str, BaseVar] = {}\n    else:\n        self._reference_vars = {var.label: var for var in variables_reference}\n\n    raw_df, unit_row = self._read(\n        filepath=Path(filepath),\n        unit_row_index=unit_row_index,\n        delim_whitespace=delim_whitespace,\n    )\n    mandatory_vars = {\n        providers_column_label: \"provider\",\n        expocode_column_label: \"expocode\",\n        date_column_label: \"date\",\n        year_column_label: \"year\",\n        month_column_label: \"month\",\n        day_column_label: \"day\",\n        hour_column_label: \"hour\",\n        latitude_column_label: \"latitude\",\n        longitude_column_label: \"longitude\",\n        depth_column_label: \"depth\",\n    }\n    self._category = category\n    if providers_column_label is not None:\n        self._providers = raw_df[providers_column_label].unique().tolist()\n    else:\n        self._providers = [\"????\"]\n    self._data = self._add_date_columns(\n        raw_df,\n        year_column_label,\n        month_column_label,\n        day_column_label,\n        date_column_label,\n    )\n    self._variables = self._get_variables(raw_df, unit_row, mandatory_vars)\n</code></pre>"},{"location":"reference/core/io/readers/#bgc_data_processing.core.io.readers.Reader.get_storer","title":"<code>get_storer()</code>","text":"<p>Return the Storer storing the data loaded.</p> <p>Returns:</p> Type Description <code>Storer</code> <p>Contains the data from the csv.</p> Source code in <code>src/bgc_data_processing/core/io/readers.py</code> <pre><code>def get_storer(self) -&gt; \"Storer\":\n\"\"\"Return the Storer storing the data loaded.\n\n    Returns\n    -------\n    Storer\n        Contains the data from the csv.\n    \"\"\"\n    return Storer(\n        data=self._data,\n        category=self._category,\n        providers=self._providers,\n        variables=self._variables.storing_variables,\n    )\n</code></pre>"},{"location":"reference/core/io/readers/#bgc_data_processing.core.io.readers.read_files","title":"<code>read_files(filepath, providers_column_label='PROVIDER', expocode_column_label='EXPOCODE', date_column_label='DATE', year_column_label='YEAR', month_column_label='MONTH', day_column_label='DAY', hour_column_label='HOUR', latitude_column_label='LATITUDE', longitude_column_label='LONGITUDE', depth_column_label='DEPH', variables_reference=None, category='in_situ', unit_row_index=1, delim_whitespace=True)</code>","text":"<p>Build Storer reading data from csv or txt files.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str | list[Path] | list[str]</code> <p>Path to the file to read.</p> required <code>providers_column_label</code> <code>str</code> <p>Provider column in the dataframe., by default \"PROVIDER\"</p> <code>'PROVIDER'</code> <code>expocode_column_label</code> <code>str</code> <p>Expocode column in the dataframe., by default \"EXPOCODE\"</p> <code>'EXPOCODE'</code> <code>date_column_label</code> <code>str</code> <p>Date column in the dataframe., by default \"DATE\"</p> <code>'DATE'</code> <code>year_column_label</code> <code>str</code> <p>Year column in the dataframe., by default \"YEAR\"</p> <code>'YEAR'</code> <code>month_column_label</code> <code>str</code> <p>Month column in the dataframe., by default \"MONTH\"</p> <code>'MONTH'</code> <code>day_column_label</code> <code>str</code> <p>Day column in the dataframe., by default \"DAY\"</p> <code>'DAY'</code> <code>hour_column_label</code> <code>str</code> <p>Hour column in the dataframe., by default \"HOUR\"</p> <code>'HOUR'</code> <code>latitude_column_label</code> <code>str</code> <p>Latitude column in the dataframe., by default \"LATITUDE\"</p> <code>'LATITUDE'</code> <code>longitude_column_label</code> <code>str</code> <p>Longitude column in the dataframe., by default \"LONGITUDE\"</p> <code>'LONGITUDE'</code> <code>depth_column_label</code> <code>str</code> <p>Depth column in the dataframe., by default \"DEPH\"</p> <code>'DEPH'</code> <code>variables_reference</code> <code>list[BaseVar] | None</code> <p>List of variable to use as reference. If a variable label is a column name,  this variable will be used for the output storer., by default None</p> <code>None</code> <code>category</code> <code>str</code> <p>Category of the loaded file., by default \"in_situ\"</p> <code>'in_situ'</code> <code>unit_row_index</code> <code>int</code> <p>Index of the row with the units, None if there's no unit row., by default 1</p> <code>1</code> <code>delim_whitespace</code> <code>bool</code> <p>Whether to use whitespace as delimiters., by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>Storer</code> <p>Storer aggregating the data from all the files</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If filepath argument is not an instance of string or list.</p> <p>Examples:</p> <p>Loading from a single file:</p> <pre><code>&gt;&gt;&gt; filepath = \"path/to/file\"\n&gt;&gt;&gt; storer = read_files(filepath, providers=\"providers_column_name\")\n</code></pre> <p>Loading from multiple files:</p> <pre><code>&gt;&gt;&gt; filepaths = [\n...     \"path/to/file1\",\n...     \"path/to/file2\",\n... ]\n&gt;&gt;&gt; storer = read_files(\n...     filepaths,\n... )\n</code></pre> Source code in <code>src/bgc_data_processing/core/io/readers.py</code> <pre><code>def read_files(\n    filepath: Path | str | list[Path] | list[str],\n    providers_column_label: str = \"PROVIDER\",\n    expocode_column_label: str = \"EXPOCODE\",\n    date_column_label: str = \"DATE\",\n    year_column_label: str = \"YEAR\",\n    month_column_label: str = \"MONTH\",\n    day_column_label: str = \"DAY\",\n    hour_column_label: str = \"HOUR\",\n    latitude_column_label: str = \"LATITUDE\",\n    longitude_column_label: str = \"LONGITUDE\",\n    depth_column_label: str = \"DEPH\",\n    variables_reference: list[BaseVar] | None = None,\n    category: str = \"in_situ\",\n    unit_row_index: int = 1,\n    delim_whitespace: bool = True,\n) -&gt; \"Storer\":\n\"\"\"Build Storer reading data from csv or txt files.\n\n    Parameters\n    ----------\n    filepath : Path | str | list[Path] | list[str]\n        Path to the file to read.\n    providers_column_label : str, optional\n        Provider column in the dataframe., by default \"PROVIDER\"\n    expocode_column_label : str, optional\n        Expocode column in the dataframe., by default \"EXPOCODE\"\n    date_column_label : str, optional\n        Date column in the dataframe., by default \"DATE\"\n    year_column_label : str, optional\n        Year column in the dataframe., by default \"YEAR\"\n    month_column_label : str, optional\n        Month column in the dataframe., by default \"MONTH\"\n    day_column_label : str, optional\n        Day column in the dataframe., by default \"DAY\"\n    hour_column_label : str, optional\n        Hour column in the dataframe., by default \"HOUR\"\n    latitude_column_label : str, optional\n        Latitude column in the dataframe., by default \"LATITUDE\"\n    longitude_column_label : str, optional\n        Longitude column in the dataframe., by default \"LONGITUDE\"\n    depth_column_label : str, optional\n        Depth column in the dataframe., by default \"DEPH\"\n    variables_reference: list[BaseVar] | None\n        List of variable to use as reference. If a variable label is a column name,\n         this variable will be used for the output storer., by default None\n    category : str, optional\n        Category of the loaded file., by default \"in_situ\"\n    unit_row_index : int, optional\n        Index of the row with the units, None if there's no unit row., by default 1\n    delim_whitespace : bool, optional\n        Whether to use whitespace as delimiters., by default True\n\n    Returns\n    -------\n    Storer\n        Storer aggregating the data from all the files\n\n    Raises\n    ------\n    TypeError\n        If filepath argument is not an instance of string or list.\n\n    Examples\n    --------\n    Loading from a single file:\n    &gt;&gt;&gt; filepath = \"path/to/file\"\n    &gt;&gt;&gt; storer = read_files(filepath, providers=\"providers_column_name\")\n\n    Loading from multiple files:\n    &gt;&gt;&gt; filepaths = [\n    ...     \"path/to/file1\",\n    ...     \"path/to/file2\",\n    ... ]\n    &gt;&gt;&gt; storer = read_files(\n    ...     filepaths,\n    ... )\n    \"\"\"\n    if isinstance(filepath, list):\n        storers = []\n        for path in filepath:\n            storer = read_files(\n                filepath=path,\n                providers_column_label=providers_column_label,\n                expocode_column_label=expocode_column_label,\n                date_column_label=date_column_label,\n                year_column_label=year_column_label,\n                month_column_label=month_column_label,\n                day_column_label=day_column_label,\n                hour_column_label=hour_column_label,\n                latitude_column_label=latitude_column_label,\n                longitude_column_label=longitude_column_label,\n                depth_column_label=depth_column_label,\n                variables_reference=variables_reference,\n                category=category,\n                unit_row_index=unit_row_index,\n                delim_whitespace=delim_whitespace,\n            )\n\n            storers.append(storer)\n        return sum(storers)\n    if isinstance(filepath, Path):\n        path = filepath\n    elif isinstance(filepath, str):\n        path = Path(filepath)\n    else:\n        error_msg = (\n            f\"Can't read filepaths from {filepath}. Accepted types are Path or str.\"\n        )\n        raise TypeError(error_msg)\n    reader = Reader(\n        filepath=path,\n        providers_column_label=providers_column_label,\n        expocode_column_label=expocode_column_label,\n        date_column_label=date_column_label,\n        year_column_label=year_column_label,\n        month_column_label=month_column_label,\n        day_column_label=day_column_label,\n        hour_column_label=hour_column_label,\n        latitude_column_label=latitude_column_label,\n        longitude_column_label=longitude_column_label,\n        depth_column_label=depth_column_label,\n        variables_reference=variables_reference,\n        category=category,\n        unit_row_index=unit_row_index,\n        delim_whitespace=delim_whitespace,\n    )\n    return reader.get_storer()\n</code></pre>"},{"location":"reference/core/io/savers/","title":"<code>bgc_data_processing.core.io.savers</code>","text":"<p>Save Storers to a file.</p>"},{"location":"reference/core/io/savers/#bgc_data_processing.core.io.savers.StorerSaver","title":"<code>StorerSaver(storer, save_aggregated_data_only=False)</code>","text":"<p>Saver for Storer objects.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to save.</p> required <code>save_aggregated_data_only</code> <code>bool</code> <p>Whether to only save the aggregated data or not. If False, for every provider, a folder with the providers' data will be created.</p> <code>False</code> Source code in <code>src/bgc_data_processing/core/io/savers.py</code> <pre><code>def __init__(\n    self,\n    storer: \"Storer\",\n    save_aggregated_data_only: bool = False,\n) -&gt; None:\n    self._storer = storer\n    self._variables = copy(storer.variables.saving_variables)\n    self.save_aggregated_data_only = save_aggregated_data_only\n</code></pre>"},{"location":"reference/core/io/savers/#bgc_data_processing.core.io.savers.StorerSaver.single_filename_format","title":"<code>single_filename_format: str = 'nutrients_{provider}_{dates}.txt'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/io/savers/#bgc_data_processing.core.io.savers.StorerSaver.aggr_filename_format","title":"<code>aggr_filename_format: str = 'bgc_{category}_{dates}.txt'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/io/savers/#bgc_data_processing.core.io.savers.StorerSaver.save_aggregated_data_only","title":"<code>save_aggregated_data_only = save_aggregated_data_only</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/io/savers/#bgc_data_processing.core.io.savers.StorerSaver.saving_order","title":"<code>saving_order: list[str]</code>  <code>property</code> <code>writable</code>","text":"<p>Saving Order for variables.</p>"},{"location":"reference/core/io/savers/#bgc_data_processing.core.io.savers.StorerSaver.save_from_daterange","title":"<code>save_from_daterange(dateranges_gen, saving_directory)</code>","text":"<p>Save the storer's data according to the given dateranges.</p> <p>Parameters:</p> Name Type Description Default <code>dateranges_gen</code> <code>DateRangeGenerator</code> <p>Generator to use to retrieve dateranges.</p> required <code>saving_directory</code> <code>Path | str</code> <p>Path to the idrectory to save in.</p> required Source code in <code>src/bgc_data_processing/core/io/savers.py</code> <pre><code>def save_from_daterange(\n    self,\n    dateranges_gen: \"DateRangeGenerator\",\n    saving_directory: Path | str,\n) -&gt; None:\n\"\"\"Save the storer's data according to the given dateranges.\n\n    Parameters\n    ----------\n    dateranges_gen : DateRangeGenerator\n        Generator to use to retrieve dateranges.\n    saving_directory: Path | str\n        Path to the idrectory to save in.\n    \"\"\"\n    dateranges = dateranges_gen()\n    dates_slices = self._slice_using_drng(dateranges)\n    dates_slices.apply(\n        self._save_slice,\n        axis=1,\n        saving_directory=Path(saving_directory),\n    )\n</code></pre>"},{"location":"reference/core/io/savers/#bgc_data_processing.core.io.savers.StorerSaver.save_all_storer","title":"<code>save_all_storer(filepath)</code>","text":"<p>Save all the storer to the given file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>File in which to save the storer data.</p> required <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If filepath points to an existing file.</p> Source code in <code>src/bgc_data_processing/core/io/savers.py</code> <pre><code>def save_all_storer(self, filepath: Path | str) -&gt; None:\n\"\"\"Save all the storer to the given file.\n\n    Parameters\n    ----------\n    filepath : Path | str\n        File in which to save the storer data.\n\n    Raises\n    ------\n    FileExistsError\n        If filepath points to an existing file.\n    \"\"\"\n    if filepath.is_file():\n        error_msg = f\"A file already exist at {filepath} and can not be erased.\"\n        raise FileExistsError(error_msg)\n    self._save_data(filepath=Path(filepath), data_slice=self._storer)\n</code></pre>"},{"location":"reference/core/io/savers/#bgc_data_processing.core.io.savers.StorerSaver.save","title":"<code>save(storer, filepath, save_aggregated_data_only=False)</code>  <code>classmethod</code>","text":"<p>Save all the storer to the given file.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to save.</p> required <code>filepath</code> <code>Path | str</code> <p>File in which to save the storer data.</p> required <code>save_aggregated_data_only</code> <code>bool</code> <p>Whether to only save the aggregated data or not. If False, for every provider, a folder with the provider's data will be created.</p> <code>False</code> Source code in <code>src/bgc_data_processing/core/io/savers.py</code> <pre><code>@classmethod\ndef save(\n    cls,\n    storer: \"Storer\",\n    filepath: Path | str,\n    save_aggregated_data_only: bool = False,\n) -&gt; None:\n\"\"\"Save all the storer to the given file.\n\n    Parameters\n    ----------\n    storer : Storer\n        Storer to save.\n    filepath : Path | str\n        File in which to save the storer data.\n    save_aggregated_data_only: bool\n        Whether to only save the aggregated data or not.\n        If False, for every provider, a folder with the provider's\n        data will be created.\n    \"\"\"\n    saver = cls(storer=storer, save_aggregated_data_only=save_aggregated_data_only)\n    saver.save_all_storer(filepath=Path(filepath))\n</code></pre>"},{"location":"reference/core/io/savers/#bgc_data_processing.core.io.savers.save_storer","title":"<code>save_storer(storer, filepath, saving_order=None, save_aggregated_data_only=True)</code>","text":"<p>Save en entire Storer to a given filepath.</p> <p>Parameters:</p> Name Type Description Default <code>storer</code> <code>Storer</code> <p>Storer to save.</p> required <code>filepath</code> <code>Path | str</code> <p>File in which to save the storer data.</p> required <code>saving_order</code> <code>list[str] | None</code> <p>Variable order to respect when saving. If the list is empty , all variables are saved., by default None</p> <code>None</code> <code>save_aggregated_data_only</code> <code>bool</code> <p>Whether to only save the aggregated data or not. If False, for every provider, a folder with the provider's data will be created., by default True</p> <code>True</code> Source code in <code>src/bgc_data_processing/core/io/savers.py</code> <pre><code>def save_storer(\n    storer: \"Storer\",\n    filepath: Path | str,\n    saving_order: list[str] | None = None,\n    save_aggregated_data_only: bool = True,\n) -&gt; None:\n\"\"\"Save en entire Storer to a given filepath.\n\n    Parameters\n    ----------\n    storer : Storer\n        Storer to save.\n    filepath : Path | str\n        File in which to save the storer data.\n    saving_order : list[str] | None, optional\n        Variable order to respect when saving. If the list is empty\n        , all variables are saved., by default None\n    save_aggregated_data_only : bool, optional\n        Whether to only save the aggregated data or not.\n        If False, for every provider, a folder with the provider's\n        data will be created., by default True\n    \"\"\"\n    saver = StorerSaver(\n        storer=storer,\n        save_aggregated_data_only=save_aggregated_data_only,\n    )\n    if saving_order is not None:\n        saver.saving_order = saving_order\n    saver.save_all_storer(filepath=Path(filepath))\n</code></pre>"},{"location":"reference/core/loaders/","title":"<code>bgc_data_processing.core.loaders</code>","text":"<p>Contains data loaders.</p> <p>From this namespace are accessible:</p> <ul> <li><code>ABFileLoader</code>            -&gt; ABfiles loader</li> <li><code>CSVLoader</code>               -&gt; CSV files loader</li> <li><code>NetCDFLoader</code>            -&gt; NetCDF files loader</li> <li><code>SatelliteNetCDFLoader</code>   -&gt; NetCDF files loader for satellite data</li> </ul>"},{"location":"reference/core/loaders/abfile_loaders/","title":"<code>bgc_data_processing.core.loaders.abfile_loaders</code>","text":"<p>ABfiles Loaders.</p>"},{"location":"reference/core/loaders/abfile_loaders/#bgc_data_processing.core.loaders.abfile_loaders.ABFileLoader","title":"<code>ABFileLoader(provider_name, category, exclude, variables, grid_basename)</code>","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader class to use with ABFiles.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Data provider name.</p> required <code>category</code> <code>str</code> <p>Category provider belongs to.</p> required <code>exclude</code> <code>list[str]</code> <p>Filenames to exclude from loading.</p> required <code>variables</code> <code>SourceVariableSet</code> <p>Storer object containing all variables to consider for this data, both the one in the data file but and the one not represented in the file.</p> required <code>grid_basename</code> <code>str</code> <p>Basename of the ab grid grid file for the loader. =&gt; files are considered to be loaded over the same grid.</p> required Source code in <code>src/bgc_data_processing/core/loaders/abfile_loaders.py</code> <pre><code>def __init__(\n    self,\n    provider_name: str,\n    category: str,\n    exclude: list[str],\n    variables: \"SourceVariableSet\",\n    grid_basename: str,\n) -&gt; None:\n    super().__init__(\n        provider_name=provider_name,\n        category=category,\n        exclude=exclude,\n        variables=variables,\n    )\n    self.grid_basename = grid_basename\n    self.grid_file = ABFileGrid(basename=grid_basename, action=\"r\")\n    self._index = None\n</code></pre>"},{"location":"reference/core/loaders/abfile_loaders/#bgc_data_processing.core.loaders.abfile_loaders.ABFileLoader.level_column","title":"<code>level_column: str = 'LEVEL'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/loaders/abfile_loaders/#bgc_data_processing.core.loaders.abfile_loaders.ABFileLoader.level_key_bfile","title":"<code>level_key_bfile: str = 'k'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/loaders/abfile_loaders/#bgc_data_processing.core.loaders.abfile_loaders.ABFileLoader.field_key_bfile","title":"<code>field_key_bfile: str = 'field'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/loaders/abfile_loaders/#bgc_data_processing.core.loaders.abfile_loaders.ABFileLoader.pascal_by_seawater_meter","title":"<code>pascal_by_seawater_meter: int = 9806</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/loaders/abfile_loaders/#bgc_data_processing.core.loaders.abfile_loaders.ABFileLoader.grid_basename","title":"<code>grid_basename = grid_basename</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/loaders/abfile_loaders/#bgc_data_processing.core.loaders.abfile_loaders.ABFileLoader.grid_file","title":"<code>grid_file = ABFileGrid(basename=grid_basename, action='r')</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/loaders/abfile_loaders/#bgc_data_processing.core.loaders.abfile_loaders.ABFileLoader.convert_filepath_to_basename","title":"<code>convert_filepath_to_basename(filepath)</code>  <code>staticmethod</code>","text":"<p>Convert a filepath to the basename.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Filepath ot convert.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Basename</p> Source code in <code>src/bgc_data_processing/core/loaders/abfile_loaders.py</code> <pre><code>@staticmethod\ndef convert_filepath_to_basename(filepath: Path | str) -&gt; Path:\n\"\"\"Convert a filepath to the basename.\n\n    Parameters\n    ----------\n    filepath : Path | str\n        Filepath ot convert.\n\n    Returns\n    -------\n    Path\n        Basename\n    \"\"\"\n    path = Path(filepath)\n    return path.parent.joinpath(path.stem)\n</code></pre>"},{"location":"reference/core/loaders/abfile_loaders/#bgc_data_processing.core.loaders.abfile_loaders.ABFileLoader.load","title":"<code>load(filepath, constraints=None)</code>","text":"<p>Load a abfiles from basename.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Path to the basename of the file to load.</p> required <code>constraints</code> <code>Constraints | None</code> <p>Constraints slicer., by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame corresponding to the file.</p> Source code in <code>src/bgc_data_processing/core/loaders/abfile_loaders.py</code> <pre><code>def load(\n    self,\n    filepath: Path | str,\n    constraints: Constraints | None = None,\n) -&gt; pd.DataFrame:\n\"\"\"Load a abfiles from basename.\n\n    Parameters\n    ----------\n    filepath: Path | str\n        Path to the basename of the file to load.\n    constraints : Constraints| None, optional\n        Constraints slicer., by default None\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame corresponding to the file.\n    \"\"\"\n    if constraints is None:\n        constraints = Constraints()\n    basename = self.convert_filepath_to_basename(filepath)\n    raw_data = self._read(basename=str(basename))\n    # transform thickness in depth\n    with_depth = self._create_depth_column(raw_data)\n    # create date columns\n    with_dates = self._set_date_related_columns(with_depth, Path(basename))\n    # converts types\n    typed = self._convert_types(with_dates)\n    # apply corrections\n    corrected = self._correct(typed)\n    # apply constraints\n    constrained = constraints.apply_constraints_to_dataframe(corrected)\n    return self.remove_nan_rows(constrained)\n</code></pre>"},{"location":"reference/core/loaders/abfile_loaders/#bgc_data_processing.core.loaders.abfile_loaders.ABFileLoader.is_file_valid","title":"<code>is_file_valid(filepath)</code>","text":"<p>Check whether a file is valid or not.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>File filepath.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file can be loaded.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the afile doesn't not exist.</p> <code>FileNotFoundError</code> <p>If the bfile doesn't not exist.</p> Source code in <code>src/bgc_data_processing/core/loaders/abfile_loaders.py</code> <pre><code>def is_file_valid(self, filepath: Path | str) -&gt; bool:\n\"\"\"Check whether a file is valid or not.\n\n    Parameters\n    ----------\n    filepath : Path | str\n        File filepath.\n\n    Returns\n    -------\n    bool\n        True if the file can be loaded.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the afile doesn't not exist.\n    FileNotFoundError\n        If the bfile doesn't not exist.\n    \"\"\"\n    path = Path(filepath)\n    basepath = path.parent / path.name[:-2]\n    keep_filepath = str(path) not in self.excluded_filenames\n    keep_filename = path.name not in self.excluded_filenames\n    keep_file = keep_filename and keep_filepath\n    keep_basepath = str(basepath) not in self.excluded_filenames\n    keep_basename = basepath.name not in self.excluded_filenames\n    keep_base = keep_basename and keep_basepath\n    afile_path = Path(f\"{basepath}.a\")\n    bfile_path = Path(f\"{basepath}.b\")\n    if not afile_path.is_file():\n        error_msg = f\"{afile_path} does not exist.\"\n        raise FileNotFoundError(error_msg)\n    if not bfile_path.is_file():\n        error_msg = f\"{bfile_path} does not exist.\"\n        raise FileNotFoundError(error_msg)\n    return keep_base and keep_file\n</code></pre>"},{"location":"reference/core/loaders/base/","title":"<code>bgc_data_processing.core.loaders.base</code>","text":"<p>Base Loaders.</p>"},{"location":"reference/core/loaders/base/#bgc_data_processing.core.loaders.base.BaseLoader","title":"<code>BaseLoader(provider_name, category, exclude, variables)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class to load data.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Data provider name.</p> required <code>category</code> <code>str</code> <p>Category provider belongs to.</p> required <code>exclude</code> <code>list[str]</code> <p>Filenames to exclude from loading.</p> required <code>variables</code> <code>SourceVariableSet</code> <p>Storer object containing all variables to consider for this data, both the one in the data file but and the one not represented in the file.</p> required Source code in <code>src/bgc_data_processing/core/loaders/base.py</code> <pre><code>def __init__(\n    self,\n    provider_name: str,\n    category: str,\n    exclude: list[str],\n    variables: \"LoadingVariablesSet\",\n) -&gt; None:\n    self._provider = provider_name\n    self._category = category\n    self._exclude = exclude\n    self._variables = variables\n</code></pre>"},{"location":"reference/core/loaders/base/#bgc_data_processing.core.loaders.base.BaseLoader.provider","title":"<code>provider: str</code>  <code>property</code>","text":"<p>_provider attribute getter.</p> <p>Returns:</p> Type Description <code>str</code> <p>data provider name.</p>"},{"location":"reference/core/loaders/base/#bgc_data_processing.core.loaders.base.BaseLoader.category","title":"<code>category: str</code>  <code>property</code>","text":"<p>Returns the category of the provider.</p> <p>Returns:</p> Type Description <code>str</code> <p>Category provider belongs to.</p>"},{"location":"reference/core/loaders/base/#bgc_data_processing.core.loaders.base.BaseLoader.variables","title":"<code>variables: LoadingVariablesSet</code>  <code>property</code>","text":"<p>_variables attribute getter.</p> <p>Returns:</p> Type Description <code>LoadingVariablesSet</code> <p>Loading variables storer.</p>"},{"location":"reference/core/loaders/base/#bgc_data_processing.core.loaders.base.BaseLoader.excluded_filenames","title":"<code>excluded_filenames: list[str]</code>  <code>property</code>","text":"<p>Filenames to exclude from loading.</p>"},{"location":"reference/core/loaders/base/#bgc_data_processing.core.loaders.base.BaseLoader.is_file_valid","title":"<code>is_file_valid(filepath)</code>","text":"<p>Indicate whether a file is valid to be kept or not.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Name of the file</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the name is not to be excluded.</p> Source code in <code>src/bgc_data_processing/core/loaders/base.py</code> <pre><code>def is_file_valid(self, filepath: Path | str) -&gt; bool:\n\"\"\"Indicate whether a file is valid to be kept or not.\n\n    Parameters\n    ----------\n    filepath : Path | str\n        Name of the file\n\n    Returns\n    -------\n    bool\n        True if the name is not to be excluded.\n    \"\"\"\n    keep_path = str(filepath) not in self.excluded_filenames\n    keep_name = Path(filepath).name not in self.excluded_filenames\n\n    return keep_name and keep_path\n</code></pre>"},{"location":"reference/core/loaders/base/#bgc_data_processing.core.loaders.base.BaseLoader.load","title":"<code>load(filepath)</code>  <code>abstractmethod</code>","text":"<p>Load data.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Data object.</p> Source code in <code>src/bgc_data_processing/core/loaders/base.py</code> <pre><code>@abstractmethod\ndef load(self, filepath: str) -&gt; pd.DataFrame:\n\"\"\"Load data.\n\n    Returns\n    -------\n    Any\n        Data object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/core/loaders/base/#bgc_data_processing.core.loaders.base.BaseLoader.remove_nan_rows","title":"<code>remove_nan_rows(df)</code>","text":"<p>Remove rows.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DatafRame on which to remove rows.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with rows removed</p> Source code in <code>src/bgc_data_processing/core/loaders/base.py</code> <pre><code>def remove_nan_rows(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Remove rows.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DatafRame on which to remove rows.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with rows removed\n    \"\"\"\n    # Load keys\n    vars_to_remove_when_any_nan = self._variables.to_remove_if_any_nan\n    vars_to_remove_when_all_nan = self._variables.to_remove_if_all_nan\n    # Check for nans\n    if vars_to_remove_when_any_nan:\n        any_nans = df[vars_to_remove_when_any_nan].isna().any(axis=1)\n    else:\n        any_nans = pd.Series(False, index=df.index)\n    if vars_to_remove_when_all_nan:\n        all_nans = df[vars_to_remove_when_all_nan].isna().all(axis=1)\n    else:\n        all_nans = pd.Series(False, index=df.index)\n    # Get indexes to drop\n    indexes_to_drop = df[any_nans | all_nans].index\n    return df.drop(index=indexes_to_drop)\n</code></pre>"},{"location":"reference/core/loaders/csv_loaders/","title":"<code>bgc_data_processing.core.loaders.csv_loaders</code>","text":"<p>CSV Loaders.</p>"},{"location":"reference/core/loaders/csv_loaders/#bgc_data_processing.core.loaders.csv_loaders.CSVLoader","title":"<code>CSVLoader(provider_name, category, exclude, variables, read_params=None)</code>","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader class to use with csv files.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Data provider name.</p> required <code>category</code> <code>str</code> <p>Category provider belongs to.</p> required <code>exclude</code> <code>list[str]</code> <p>Filenames to exclude from loading.</p> required <code>variables</code> <code>SourceVariableSet</code> <p>Storer object containing all variables to consider for this data, both the one in the data file but and the one not represented in the file.</p> required <code>read_params</code> <code>dict | None</code> <p>Additional parameter to pass to pandas.read_csv., by default None</p> <code>None</code> Source code in <code>src/bgc_data_processing/core/loaders/csv_loaders.py</code> <pre><code>def __init__(\n    self,\n    provider_name: str,\n    category: str,\n    exclude: list[str],\n    variables: \"SourceVariableSet\",\n    read_params: dict | None = None,\n) -&gt; None:\n    if read_params is None:\n        self._read_params = {}\n    else:\n        self._read_params = read_params\n    super().__init__(\n        provider_name=provider_name,\n        category=category,\n        exclude=exclude,\n        variables=variables,\n    )\n</code></pre>"},{"location":"reference/core/loaders/csv_loaders/#bgc_data_processing.core.loaders.csv_loaders.CSVLoader.load","title":"<code>load(filepath, constraints=None)</code>","text":"<p>Load a csv file from filepath.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Path to the file to load.</p> required <code>constraints</code> <code>Constraints | None</code> <p>Constraints slicer., by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame corresponding to the file.</p> Source code in <code>src/bgc_data_processing/core/loaders/csv_loaders.py</code> <pre><code>def load(\n    self,\n    filepath: Path | str,\n    constraints: Constraints | None = None,\n) -&gt; pd.DataFrame:\n\"\"\"Load a csv file from filepath.\n\n    Parameters\n    ----------\n    filepath: Path | str\n        Path to the file to load.\n    constraints : Constraints| None, optional\n        Constraints slicer., by default None\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame corresponding to the file.\n    \"\"\"\n    if constraints is None:\n        constraints = Constraints()\n    df_raw = self._read(Path(filepath))\n    df_form = self._format(df_raw)\n    df_type = self._convert_types(df_form)\n    df_corr = self._correct(df_type)\n    df_sliced = constraints.apply_constraints_to_dataframe(df_corr)\n    return self.remove_nan_rows(df_sliced)\n</code></pre>"},{"location":"reference/core/loaders/netcdf_loaders/","title":"<code>bgc_data_processing.core.loaders.netcdf_loaders</code>","text":"<p>NetCDF Loaders.</p>"},{"location":"reference/core/loaders/netcdf_loaders/#bgc_data_processing.core.loaders.netcdf_loaders.NetCDFLoader","title":"<code>NetCDFLoader(provider_name, category, exclude, variables)</code>","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader class to use with NetCDF files.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Data provider name.</p> required <code>category</code> <code>str</code> <p>Category provider belongs to.</p> required <code>exclude</code> <code>list[str]</code> <p>Filenames to exclude from loading.</p> required <code>variables</code> <code>SourceVariableSet</code> <p>Storer object containing all variables to consider for this data, both the one in the data file but and the one not represented in the file.</p> required Source code in <code>src/bgc_data_processing/core/loaders/netcdf_loaders.py</code> <pre><code>def __init__(\n    self,\n    provider_name: str,\n    category: str,\n    exclude: list[str],\n    variables: \"SourceVariableSet\",\n) -&gt; None:\n    super().__init__(\n        provider_name=provider_name,\n        category=category,\n        exclude=exclude,\n        variables=variables,\n    )\n</code></pre>"},{"location":"reference/core/loaders/netcdf_loaders/#bgc_data_processing.core.loaders.netcdf_loaders.NetCDFLoader.load","title":"<code>load(filepath, constraints=None)</code>","text":"<p>Load a netCDF file from filepath.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Path to the file to load.</p> required <code>constraints</code> <code>Constraints | None</code> <p>Constraints slicer., by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame corresponding to the file.</p> Source code in <code>src/bgc_data_processing/core/loaders/netcdf_loaders.py</code> <pre><code>def load(\n    self,\n    filepath: Path | str,\n    constraints: Constraints | None = None,\n) -&gt; pd.DataFrame:\n\"\"\"Load a netCDF file from filepath.\n\n    Parameters\n    ----------\n    filepath: Path | str\n        Path to the file to load.\n    constraints : Constraints | None, optional\n        Constraints slicer., by default None\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame corresponding to the file.\n    \"\"\"\n    if constraints is None:\n        constraints = Constraints()\n    file_id = self._get_id(Path(filepath).name)\n    nc_data = self._read(filepath=Path(filepath))\n    df_format = self._format(nc_data)\n    df_dates = self._set_dates(df_format)\n    df_dates_sliced = constraints.apply_specific_constraint(\n        field_label=self._variables.get(self._variables.date_var_name).label,\n        df=df_dates,\n    )\n    df_prov = self._set_provider(df_dates_sliced)\n    df_expo = self._set_expocode(df_prov, file_id)\n    df_ecols = self._add_empty_cols(df_expo)\n    df_types = self._convert_type(df_ecols)\n    df_corr = self._correct(df_types)\n    df_sliced = constraints.apply_constraints_to_dataframe(df_corr)\n    return self.remove_nan_rows(df_sliced)\n</code></pre>"},{"location":"reference/core/loaders/netcdf_loaders/#bgc_data_processing.core.loaders.netcdf_loaders.SatelliteNetCDFLoader","title":"<code>SatelliteNetCDFLoader(provider_name, category, exclude, variables)</code>","text":"<p>             Bases: <code>NetCDFLoader</code></p> <p>Loader class to use with NetCDF files related to Satellite data.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Data provider name.</p> required <code>category</code> <code>str</code> <p>Category provider belongs to.</p> required <code>exclude</code> <code>list[str]</code> <p>Filenames to exclude from loading.</p> required <code>variables</code> <code>SourceVariableSet</code> <p>Storer object containing all variables to consider for this data, both the one in the data file but and the one not represented in the file.</p> required Source code in <code>src/bgc_data_processing/core/loaders/netcdf_loaders.py</code> <pre><code>def __init__(\n    self,\n    provider_name: str,\n    category: str,\n    exclude: list[str],\n    variables: \"SourceVariableSet\",\n) -&gt; None:\n    super().__init__(\n        provider_name=provider_name,\n        category=category,\n        exclude=exclude,\n        variables=variables,\n    )\n</code></pre>"},{"location":"reference/core/variables/","title":"<code>bgc_data_processing.core.variables</code>","text":"<p>Contain all variables-related objects.</p> <p>From this namespace are accessibel:</p> <ul> <li><code>ExistingVar</code>     -&gt; Existing variable object</li> <li><code>FeatureVar</code>      -&gt; Feature variable (variable depending on operations)</li> <li><code>NotExistingVar</code>  -&gt; Non existing variables object</li> <li><code>TemplateVar</code>     -&gt; Template defining object</li> </ul>"},{"location":"reference/core/variables/sets/","title":"<code>bgc_data_processing.core.variables.sets</code>","text":"<p>Variable Ensembles.</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.AllVariablesTypes","title":"<code>AllVariablesTypes: TypeAlias = ExistingVar | NotExistingVar | ParsedVar | FeatureVar</code>  <code>module-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.NotParsedvar","title":"<code>NotParsedvar: TypeAlias = ExistingVar | NotExistingVar | FeatureVar</code>  <code>module-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.FromFileVariables","title":"<code>FromFileVariables: TypeAlias = ExistingVar | NotExistingVar</code>  <code>module-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet","title":"<code>VariableSet(*args, **kwargs)</code>","text":"<p>Variable ensemble behavior implementation.</p> <p>This class represents the set of both variables present     in the file and variables to take in consideration     (therefore to add even if empty) when loading the data.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods.</p> <code>()</code> <code>*kwargs</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods. The parameter name has no importance.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError:</code> <p>If multiple var object have the same name.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __init__(\n    self,\n    *args: FromFileVariables,\n    **kwargs: FromFileVariables,\n) -&gt; None:\n    if len(args) != len({var.name for var in args}):\n        error_msg = (\n            \"To set multiple alias for the same variable, \"\n            \"use Var.in_file_as([alias1, alias2])\"\n        )\n        raise ValueError(error_msg)\n    self._instantiate_from_elements([*args, *kwargs.values()])\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.elements","title":"<code>elements: list[AllVariablesTypes]</code>  <code>property</code>","text":"<p>All variables in the ensemble.</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.labels","title":"<code>labels: dict[str, str]</code>  <code>property</code>","text":"<p>Returns a dicitonnary mapping variable names to variables labels.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>name : label</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.mapper_by_name","title":"<code>mapper_by_name: dict[str, FromFileVariables]</code>  <code>property</code>","text":"<p>Mapper between variables names and variables Var objects (for getitem).</p> <p>Returns:</p> Type Description <code>dict[str, Var]</code> <p>Mapping between names (str) and variables (Var)</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.__getitem__","title":"<code>__getitem__(__k)</code>","text":"<p>Get variable by its name.</p> <p>Parameters:</p> Name Type Description Default <code>__k</code> <code>str</code> <p>Variable name</p> required <p>Returns:</p> Type Description <code>FromFileVariables</code> <p>Corresponding variable with name __k</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __getitem__(self, __k: str) -&gt; FromFileVariables:\n\"\"\"Get variable by its name.\n\n    Parameters\n    ----------\n    __k : str\n        Variable name\n\n    Returns\n    -------\n    FromFileVariables\n        Corresponding variable with name __k\n    \"\"\"\n    return self.get(__k)\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.__iter__","title":"<code>__iter__()</code>","text":"<p>Get an iterator of all variables.</p> <p>Yields:</p> Type Description <code>Iterator[FromFileVariables]</code> <p>Ietrator of all element in the storer.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __iter__(self) -&gt; Iterator[FromFileVariables]:\n\"\"\"Get an iterator of all variables.\n\n    Yields\n    ------\n    Iterator[FromFileVariables]\n        Ietrator of all element in the storer.\n    \"\"\"\n    return iter(self._elements)\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.__str__","title":"<code>__str__()</code>","text":"<p>Convert the object to string.</p> <p>Returns:</p> Type Description <code>str</code> <p>All variable as strings.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __str__(self) -&gt; str:\n\"\"\"Convert the object to string.\n\n    Returns\n    -------\n    str\n        All variable as strings.\n    \"\"\"\n    txt = \"\"\n    for var in self._elements:\n        if var.exist_in_dset is None:\n            here_txt = \"not attributed\"\n        elif var.exist_in_dset:\n            here_txt = var.aliases\n        else:\n            here_txt = \"not in file\"\n        txt += str(var) + f\": {here_txt}\\n\"\n    return txt\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of elements.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of elements.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Return the number of elements.\n\n    Returns\n    -------\n    int\n        Number of elements.\n    \"\"\"\n    return len(self._elements)\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.__eq__","title":"<code>__eq__(__o)</code>","text":"<p>Compare to object for equality.</p> <p>Parameters:</p> Name Type Description Default <code>__o</code> <code>object</code> <p>Object to compare with.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the objects have same types and all equal variables.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __eq__(self, __o: object) -&gt; bool:\n\"\"\"Compare to object for equality.\n\n    Parameters\n    ----------\n    __o : object\n        Object to compare with.\n\n    Returns\n    -------\n    bool\n        True if the objects have same types and all equal variables.\n    \"\"\"\n    if not isinstance(__o, VariableSet):\n        return False\n\n    has_wrong_len = len(self) != len(__o)\n    self_keys = set(self.mapper_by_name.keys())\n    other_keys = set(__o.mapper_by_name.keys())\n    has_wrong_keys = self_keys != other_keys\n\n    if has_wrong_len or has_wrong_keys:\n        return False\n\n    repr_eq = [repr(self[key]) == repr(__o[key]) for key in self.mapper_by_name]\n    return np.all(repr_eq)\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.get","title":"<code>get(var_name)</code>","text":"<p>Return the variable which name corresponds to var_name.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>Name of the variable to get.</p> required <p>Returns:</p> Type Description <code>FromFileVariables</code> <p>Variable with corresponding name in self._elements.</p> <p>Raises:</p> Type Description <code>IncorrectVariableNameError</code> <p>If var_name doesn't correspond to any name.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def get(self, var_name: str) -&gt; FromFileVariables:\n\"\"\"Return the variable which name corresponds to var_name.\n\n    Parameters\n    ----------\n    var_name : str\n        Name of the variable to get.\n\n    Returns\n    -------\n    FromFileVariables\n        Variable with corresponding name in self._elements.\n\n    Raises\n    ------\n    IncorrectVariableNameError\n        If var_name doesn't correspond to any name.\n    \"\"\"\n    if self.has_name(var_name=var_name):\n        return self.mapper_by_name[var_name]\n    error_msg = (\n        f\"{var_name} is not a valid variable name.Valid names are: \"\n        f\"{list(self.mapper_by_name.keys())}\"\n    )\n    raise IncorrectVariableNameError(error_msg)\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.add_var","title":"<code>add_var(var)</code>","text":"<p>Add a new variable to self._elements.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Var</code> <p>Variable to add</p> required <p>Raises:</p> Type Description <code>DuplicatedVariableNameError</code> <p>If the variable name is already in the set.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def add_var(self, var: FromFileVariables) -&gt; None:\n\"\"\"Add a new variable to self._elements.\n\n    Parameters\n    ----------\n    var : Var\n        Variable to add\n\n    Raises\n    ------\n    DuplicatedVariableNameError\n        If the variable name is already in the set.\n    \"\"\"\n    if var.name in self.keys():  # noqa: SIM118\n        error_msg = \"A variable already exists with his name\"\n        raise DuplicatedVariableNameError(error_msg)\n    self._elements.append(var)\n    self._instantiate_from_elements(self._elements)\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.pop","title":"<code>pop(var_name)</code>","text":"<p>Remove and return the variable with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>Name of the variable to remove from the ensemble.</p> required <p>Returns:</p> Type Description <code>AllVariablesTypes</code> <p>Removed variable.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def pop(self, var_name: str) -&gt; AllVariablesTypes:\n\"\"\"Remove and return the variable with the given name.\n\n    Parameters\n    ----------\n    var_name : str\n        Name of the variable to remove from the ensemble.\n\n    Returns\n    -------\n    AllVariablesTypes\n        Removed variable.\n    \"\"\"\n    var_to_suppress = self.get(var_name)\n    elements = [e for e in self._elements if e.name != var_name]\n    self._instantiate_from_elements(elements)\n    return var_to_suppress\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.has_name","title":"<code>has_name(var_name)</code>","text":"<p>Check if a variable name is the nam eof one of the variables.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>Name to test.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the name is in self.keys(), False otherwise.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def has_name(self, var_name: str) -&gt; bool:\n\"\"\"Check if a variable name is the nam eof one of the variables.\n\n    Parameters\n    ----------\n    var_name : str\n        Name to test.\n\n    Returns\n    -------\n    bool\n        True if the name is in self.keys(), False otherwise.\n    \"\"\"\n    return var_name in self.keys()  # noqa: SIM118\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.VariableSet.keys","title":"<code>keys()</code>","text":"<p>Keys to use when calling self[key].</p> <p>Returns:</p> Type Description <code>dict_keys</code> <p>View of self.mapper_by_name keys.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def keys(self) -&gt; dict_keys:\n\"\"\"Keys to use when calling self[key].\n\n    Returns\n    -------\n    dict_keys\n        View of self.mapper_by_name keys.\n    \"\"\"\n    return self.mapper_by_name.keys()\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.FeatureVariablesSet","title":"<code>FeatureVariablesSet(*args, **kwargs)</code>","text":"<p>             Bases: <code>VariableSet</code></p> <p>Ensemble of features.</p> <p>This class represents the set of both variables present     in the file and variables to take in consideration     (therefore to add even if empty) when loading the data.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>FeatureVar</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods.</p> <code>()</code> <code>*kwargs</code> <code>FeatureVar</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods. The parameter name has no importance.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError:</code> <p>If multiple var object have the same name.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __init__(self, *args: \"FeatureVar\", **kwargs: \"FeatureVar\") -&gt; None:\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.FeatureVariablesSet.iter_constructables_features","title":"<code>iter_constructables_features(available_vars)</code>","text":"<p>Create an iterator returning constructables features.</p> <p>The Iterator considers that all constructed features are available to construct the following ones.</p> <p>Parameters:</p> Name Type Description Default <code>available_vars</code> <code>list[AllVariablesTypes]</code> <p>Variable available to build the features.</p> required <p>Yields:</p> Type Description <code>Iterator[FeatureVar]</code> <p>Iterator.</p> <p>Raises:</p> Type Description <code>FeatureConstructionError</code> <p>If all features can not be constructed.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def iter_constructables_features(\n    self,\n    available_vars: list[AllVariablesTypes],\n) -&gt; Iterator[FeatureVar]:\n\"\"\"Create an iterator returning constructables features.\n\n    The Iterator considers that all constructed features\n    are available to construct the following ones.\n\n    Parameters\n    ----------\n    available_vars : list[AllVariablesTypes]\n        Variable available to build the features.\n\n    Yields\n    ------\n    Iterator[FeatureVar]\n        Iterator.\n\n    Raises\n    ------\n    FeatureConstructionError\n        If all features can not be constructed.\n    \"\"\"\n    available = available_vars.copy()\n    features = self._elements.copy()\n    constructables = self._get_constructable_features(features, available)\n    while constructables:\n        for feature in constructables:\n            available.append(feature)\n            yield feature\n        features = [f for f in features if all(f != a for a in available)]\n        constructables = self._get_constructable_features(features, available)\n    if features:\n        error_msg = (\n            f\"The following features can not be loaded: {features}. \"\n            \"They probably depend on non loaded variables.\"\n        )\n        raise FeatureConstructionError(error_msg)\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet","title":"<code>BaseRequiredVarsSet(expocode, date, year, month, day, latitude, longitude, depth, hour=None, provider=None, *args, **kwargs)</code>","text":"<p>             Bases: <code>VariableSet</code></p> <p>Storer for Variable objects with some required variables.</p> <p>This class represents the set of both variables present     in the file and variables to take in consideration     (therefore to add even if empty) when loading the data.</p> <p>Parameters:</p> Name Type Description Default <code>expocode</code> <code>FromFileVariables</code> <p>Expocode related variable.</p> required <code>date</code> <code>FromFileVariables</code> <p>Date related variable.</p> required <code>year</code> <code>FromFileVariables</code> <p>Year related variable.</p> required <code>month</code> <code>FromFileVariables</code> <p>Month related variable.</p> required <code>day</code> <code>FromFileVariables</code> <p>Day related variable.</p> required <code>latitude</code> <code>FromFileVariables</code> <p>Latitude related variable.</p> required <code>longitude</code> <code>FromFileVariables</code> <p>Longitude related variable.</p> required <code>depth</code> <code>FromFileVariables</code> <p>Depth related variable.</p> required <code>provider</code> <code>FromFileVariables</code> <p>Provider related variable. Can be set to None to be ignored., by default None</p> <code>None</code> <code>hour</code> <code>FromFileVariables</code> <p>Hour related variable. Can be set to None to be ignored., by default None</p> <code>None</code> <code>*args</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods.</p> <code>()</code> <code>*kwargs</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods. The parameter name has no importance.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError:</code> <p>If multiple var object have the same name.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __init__(\n    self,\n    expocode: FromFileVariables,\n    date: FromFileVariables,\n    year: FromFileVariables,\n    month: FromFileVariables,\n    day: FromFileVariables,\n    latitude: FromFileVariables,\n    longitude: FromFileVariables,\n    depth: FromFileVariables,\n    hour: FromFileVariables | None = None,\n    provider: FromFileVariables | None = None,\n    *args: FromFileVariables,\n    **kwargs: FromFileVariables,\n) -&gt; None:\n    if len(args) != len({var.name for var in args}):\n        error_msg = (\n            \"To set multiple alias for the same variable, \"\n            \"use Var.in_file_as([alias1, alias2])\"\n        )\n        raise ValueError(error_msg)\n    mandatory_variables = []\n    if provider is None:\n        self.has_provider = False\n        self.provider_var_name = None\n    else:\n        self.has_provider = True\n        self.provider_var_name = provider.name\n        mandatory_variables.append(provider)\n    self.expocode_var_name = expocode.name\n    mandatory_variables.append(expocode)\n    self.date_var_name = date.name\n    mandatory_variables.append(date)\n    self.year_var_name = year.name\n    mandatory_variables.append(year)\n    self.month_var_name = month.name\n    mandatory_variables.append(month)\n    self.day_var_name = day.name\n    mandatory_variables.append(day)\n    if hour is None:\n        self.has_hour = False\n        self.hour_var_name = None\n    else:\n        self.has_hour = True\n        self.hour_var_name = hour.name\n        mandatory_variables.append(hour)\n    self.latitude_var_name = latitude.name\n    mandatory_variables.append(latitude)\n    self.longitude_var_name = longitude.name\n    mandatory_variables.append(longitude)\n    self.depth_var_name = depth.name\n    mandatory_variables.append(depth)\n    self._elements: list[FromFileVariables | ParsedVar] = (\n        mandatory_variables + list(args) + list(kwargs.values())\n    )\n    self._save = [var.name for var in self._elements]\n    self._in_dset = [var for var in self._elements if var.exist_in_dset]\n    self._not_in_dset = [var for var in self._elements if not var.exist_in_dset]\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.has_provider","title":"<code>has_provider = False</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.provider_var_name","title":"<code>provider_var_name = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.expocode_var_name","title":"<code>expocode_var_name = expocode.name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.date_var_name","title":"<code>date_var_name = date.name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.year_var_name","title":"<code>year_var_name = year.name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.month_var_name","title":"<code>month_var_name = month.name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.day_var_name","title":"<code>day_var_name = day.name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.has_hour","title":"<code>has_hour = False</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.hour_var_name","title":"<code>hour_var_name = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.latitude_var_name","title":"<code>latitude_var_name = latitude.name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.longitude_var_name","title":"<code>longitude_var_name = longitude.name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.depth_var_name","title":"<code>depth_var_name = depth.name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.BaseRequiredVarsSet.pop","title":"<code>pop(var_name)</code>","text":"<p>Remove and return the variable with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>Name of the variable to remove from the ensemble.</p> required <p>Returns:</p> Type Description <code>AllVariablesTypes</code> <p>Removed variable.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the variable is mandatory.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def pop(self, var_name: str) -&gt; AllVariablesTypes:\n\"\"\"Remove and return the variable with the given name.\n\n    Parameters\n    ----------\n    var_name : str\n        Name of the variable to remove from the ensemble.\n\n    Returns\n    -------\n    AllVariablesTypes\n        Removed variable.\n\n    Raises\n    ------\n    KeyError\n        If the variable is mandatory.\n    \"\"\"\n    mandatory_variables_names = [\n        self.expocode_var_name,\n        self.provider_var_name,\n        self.date_var_name,\n        self.year_var_name,\n        self.month_var_name,\n        self.day_var_name,\n        self.hour_var_name,\n        self.latitude_var_name,\n        self.longitude_var_name,\n        self.depth_var_name,\n    ]\n    if var_name in mandatory_variables_names:\n        error_msg = (\n            f\"Variable {var_name} can not be removed since \"\n            \"it is a mandatory variable.\"\n        )\n        raise IncorrectVariableNameError(error_msg)\n    return super().pop(var_name)\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.LoadingVariablesSet","title":"<code>LoadingVariablesSet(expocode, date, year, month, day, latitude, longitude, depth, hour=None, provider=None, *args, **kwargs)</code>","text":"<p>             Bases: <code>BaseRequiredVarsSet</code></p> <p>Storer for Var object which are going to be loaded.</p> <p>This class represents the set of both variables present     in the file and variables to take in consideration     (therefore to add even if empty) when loading the data.</p> <p>Parameters:</p> Name Type Description Default <code>expocode</code> <code>FromFileVariables</code> <p>Expocode related variable.</p> required <code>date</code> <code>FromFileVariables</code> <p>Date related variable.</p> required <code>year</code> <code>FromFileVariables</code> <p>Year related variable.</p> required <code>month</code> <code>FromFileVariables</code> <p>Month related variable.</p> required <code>day</code> <code>FromFileVariables</code> <p>Day related variable.</p> required <code>latitude</code> <code>FromFileVariables</code> <p>Latitude related variable.</p> required <code>longitude</code> <code>FromFileVariables</code> <p>Longitude related variable.</p> required <code>depth</code> <code>FromFileVariables</code> <p>Depth related variable.</p> required <code>provider</code> <code>FromFileVariables</code> <p>Provider related variable. Can be set to None to be ignored., by default None</p> <code>None</code> <code>hour</code> <code>FromFileVariables</code> <p>Hour related variable. Can be set to None to be ignored., by default None</p> <code>None</code> <code>*args</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods.</p> <code>()</code> <code>*kwargs</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods. The parameter name has no importance.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError:</code> <p>If multiple var object have the same name.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __init__(\n    self,\n    expocode: FromFileVariables,\n    date: FromFileVariables,\n    year: FromFileVariables,\n    month: FromFileVariables,\n    day: FromFileVariables,\n    latitude: FromFileVariables,\n    longitude: FromFileVariables,\n    depth: FromFileVariables,\n    hour: FromFileVariables | None = None,\n    provider: FromFileVariables | None = None,\n    *args: FromFileVariables,\n    **kwargs: FromFileVariables,\n) -&gt; None:\n    super().__init__(\n        expocode,\n        date,\n        year,\n        month,\n        day,\n        latitude,\n        longitude,\n        depth,\n        hour,\n        provider,\n        *args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.LoadingVariablesSet.in_dset","title":"<code>in_dset: list[ExistingVar]</code>  <code>property</code>","text":"<p>List of Var object supposedly present in the dataset.</p> <p>Returns:</p> Type Description <code>list[Var]</code> <p>Var objects in the dataset.</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.LoadingVariablesSet.corrections","title":"<code>corrections: dict[str, Callable]</code>  <code>property</code>","text":"<p>Mapping between variables keys and correcting functions.</p> <p>Returns:</p> Type Description <code>dict[str, Callable]</code> <p>Mapping.</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.LoadingVariablesSet.to_remove_if_all_nan","title":"<code>to_remove_if_all_nan: list[str]</code>  <code>property</code>","text":"<p>Return the list of keys to inspect when removing rows.</p> <p>This is suited when seeking for rows to delete when many given variables are NaN.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keys to use.</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.LoadingVariablesSet.to_remove_if_any_nan","title":"<code>to_remove_if_any_nan: list[str]</code>  <code>property</code>","text":"<p>Return the list of keys to inspect when removing rows.</p> <p>This is suited when seeking for rows to delete when at least one given variable is NaN.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keys to use.</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.StoringVariablesSet","title":"<code>StoringVariablesSet(expocode, date, year, month, day, latitude, longitude, depth, hour=None, provider=None, *args, **kwargs)</code>","text":"<p>             Bases: <code>BaseRequiredVarsSet</code></p> <p>Storer for Var object which are going to be stored.</p> <p>This class represents the set of both variables present     in the file and variables to take in consideration     (therefore to add even if empty) when loading the data.</p> <p>Parameters:</p> Name Type Description Default <code>expocode</code> <code>FromFileVariables</code> <p>Expocode related variable.</p> required <code>date</code> <code>FromFileVariables</code> <p>Date related variable.</p> required <code>year</code> <code>FromFileVariables</code> <p>Year related variable.</p> required <code>month</code> <code>FromFileVariables</code> <p>Month related variable.</p> required <code>day</code> <code>FromFileVariables</code> <p>Day related variable.</p> required <code>latitude</code> <code>FromFileVariables</code> <p>Latitude related variable.</p> required <code>longitude</code> <code>FromFileVariables</code> <p>Longitude related variable.</p> required <code>depth</code> <code>FromFileVariables</code> <p>Depth related variable.</p> required <code>provider</code> <code>FromFileVariables</code> <p>Provider related variable. Can be set to None to be ignored., by default None</p> <code>None</code> <code>hour</code> <code>FromFileVariables</code> <p>Hour related variable. Can be set to None to be ignored., by default None</p> <code>None</code> <code>*args</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods.</p> <code>()</code> <code>*kwargs</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods. The parameter name has no importance.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError:</code> <p>If multiple var object have the same name.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __init__(\n    self,\n    expocode: FromFileVariables,\n    date: FromFileVariables,\n    year: FromFileVariables,\n    month: FromFileVariables,\n    day: FromFileVariables,\n    latitude: FromFileVariables,\n    longitude: FromFileVariables,\n    depth: FromFileVariables,\n    hour: FromFileVariables | None = None,\n    provider: FromFileVariables | None = None,\n    *args: FromFileVariables,\n    **kwargs: FromFileVariables,\n) -&gt; None:\n    super().__init__(\n        expocode,\n        date,\n        year,\n        month,\n        day,\n        latitude,\n        longitude,\n        depth,\n        hour,\n        provider,\n        *args,\n        **kwargs,\n    )\n    self._save = [var.name for var in self._elements]\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.StoringVariablesSet.saving_variables","title":"<code>saving_variables: SavingVariablesSet</code>  <code>property</code>","text":"<p>Order of the variables to save.</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.StoringVariablesSet.set_saving_order","title":"<code>set_saving_order(var_names=None)</code>","text":"<p>Set the saving order for the variables.</p> <p>Parameters:</p> Name Type Description Default <code>var_names</code> <code>list[str] | None</code> <p>List of variable names =&gt; saving variables sorted., by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a variable name is not one of the variables'.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def set_saving_order(self, var_names: list[str] | None = None) -&gt; None:\n\"\"\"Set the saving order for the variables.\n\n    Parameters\n    ----------\n    var_names : list[str] | None, optional\n        List of variable names =&gt; saving variables sorted., by default None\n\n    Raises\n    ------\n    ValueError\n        If a variable name is not one of the variables'.\n    \"\"\"\n    if var_names is None:\n        return\n    new_save = deepcopy(var_names)\n    self._save = new_save\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.SavingVariablesSet","title":"<code>SavingVariablesSet(expocode, date, year, month, day, latitude, longitude, depth, hour=None, provider=None, save_order=None, *args, **kwargs)</code>","text":"<p>             Bases: <code>BaseRequiredVarsSet</code></p> <p>Storer for Var object which are going to be saved.</p> <p>This class represents the set of both variables present     in the file and variables to take in consideration     (therefore to add even if empty) when loading the data.</p> <p>Parameters:</p> Name Type Description Default <code>expocode</code> <code>FromFileVariables</code> <p>Expocode related variable.</p> required <code>date</code> <code>FromFileVariables</code> <p>Date related variable.</p> required <code>year</code> <code>FromFileVariables</code> <p>Year related variable.</p> required <code>month</code> <code>FromFileVariables</code> <p>Month related variable.</p> required <code>day</code> <code>FromFileVariables</code> <p>Day related variable.</p> required <code>latitude</code> <code>FromFileVariables</code> <p>Latitude related variable.</p> required <code>longitude</code> <code>FromFileVariables</code> <p>Longitude related variable.</p> required <code>depth</code> <code>FromFileVariables</code> <p>Depth related variable.</p> required <code>hour</code> <code>FromFileVariables</code> <p>Hour related variable. Can be set to None to be ignored., by default None</p> <code>None</code> <code>provider</code> <code>FromFileVariables</code> <p>Provider related variable. Can be set to None to be ignored., by default None</p> <code>None</code> <code>save_order</code> <code>list[AllVariablesTypes] | None</code> <p>Default saving order. Order of the variables if None., by default None</p> <code>None</code> <code>*args</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods.</p> <code>()</code> <code>*kwargs</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods. The parameter name has no importance.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError:</code> <p>If multiple var object have the same name.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __init__(\n    self,\n    expocode: FromFileVariables,\n    date: FromFileVariables,\n    year: FromFileVariables,\n    month: FromFileVariables,\n    day: FromFileVariables,\n    latitude: FromFileVariables,\n    longitude: FromFileVariables,\n    depth: FromFileVariables,\n    hour: FromFileVariables | None = None,\n    provider: FromFileVariables | None = None,\n    save_order: list[AllVariablesTypes] | None = None,\n    *args: FromFileVariables,\n    **kwargs: FromFileVariables,\n) -&gt; None:\n    super().__init__(\n        expocode,\n        date,\n        year,\n        month,\n        day,\n        latitude,\n        longitude,\n        depth,\n        hour,\n        provider,\n        *args,\n        **kwargs,\n    )\n    if save_order is None:\n        self._save = [var.name for var in self._elements.copy()]\n    else:\n        self._save = save_order\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.SavingVariablesSet.save_labels","title":"<code>save_labels: list[str | tuple[str]]</code>  <code>property</code>","text":"<p>Sorting order to use when saving data.</p> <p>Returns:</p> Type Description <code>list[str | tuple[str]]</code> <p>List of columns keys to pass as df[self.save_sort] to sort data.</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.SavingVariablesSet.save_names","title":"<code>save_names: list[str | tuple[str]]</code>  <code>property</code>","text":"<p>Sorted names of variables to use for saving.</p> <p>Returns:</p> Type Description <code>list[str | tuple[str]]</code> <p>List of columns keys to pass as df[self.save_sort] to sort data.</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.SavingVariablesSet.name_save_format","title":"<code>name_save_format: str</code>  <code>property</code>","text":"<p>String line to use as formatting for name and unit rows.</p> <p>Returns:</p> Type Description <code>str</code> <p>Format string</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.SavingVariablesSet.value_save_format","title":"<code>value_save_format: str</code>  <code>property</code>","text":"<p>String line to use as formatting for value rows.</p> <p>Returns:</p> Type Description <code>str</code> <p>Format string\"</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.SavingVariablesSet.set_saving_order","title":"<code>set_saving_order(var_names=None)</code>","text":"<p>Set the saving order for the variables.</p> <p>Parameters:</p> Name Type Description Default <code>var_names</code> <code>list[str] | None</code> <p>List of variable names =&gt; saving variables sorted., by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a variable name is not one of the variables'.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def set_saving_order(self, var_names: list[str] | None = None) -&gt; None:\n\"\"\"Set the saving order for the variables.\n\n    Parameters\n    ----------\n    var_names : list[str] | None, optional\n        List of variable names =&gt; saving variables sorted., by default None\n\n    Raises\n    ------\n    ValueError\n        If a variable name is not one of the variables'.\n    \"\"\"\n    if var_names is None:\n        return\n    self._save = deepcopy(var_names)\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.SourceVariableSet","title":"<code>SourceVariableSet(expocode, date, year, month, day, latitude, longitude, depth, hour=None, provider=None, *args, **kwargs)</code>","text":"<p>             Bases: <code>BaseRequiredVarsSet</code></p> <p>Ensemble of variables.</p> <p>This class represents the set of both variables present     in the file and variables to take in consideration     (therefore to add even if empty) when loading the data.</p> <p>Parameters:</p> Name Type Description Default <code>expocode</code> <code>FromFileVariables</code> <p>Expocode related variable.</p> required <code>date</code> <code>FromFileVariables</code> <p>Date related variable.</p> required <code>year</code> <code>FromFileVariables</code> <p>Year related variable.</p> required <code>month</code> <code>FromFileVariables</code> <p>Month related variable.</p> required <code>day</code> <code>FromFileVariables</code> <p>Day related variable.</p> required <code>latitude</code> <code>FromFileVariables</code> <p>Latitude related variable.</p> required <code>longitude</code> <code>FromFileVariables</code> <p>Longitude related variable.</p> required <code>depth</code> <code>FromFileVariables</code> <p>Depth related variable.</p> required <code>provider</code> <code>FromFileVariables</code> <p>Provider related variable. Can be set to None to be ignored., by default None</p> <code>None</code> <code>hour</code> <code>FromFileVariables</code> <p>Hour related variable. Can be set to None to be ignored., by default None</p> <code>None</code> <code>*args</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods.</p> <code>()</code> <code>*kwargs</code> <code>FromFileVariables</code> <p>Var objects to represent the variables stored by the object. It is better if these Var object have been instanciated using .not_here or .here_as methods. The parameter name has no importance.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError:</code> <p>If multiple var object have the same name.</p> Source code in <code>src/bgc_data_processing/core/variables/sets.py</code> <pre><code>def __init__(\n    self,\n    expocode: FromFileVariables,\n    date: FromFileVariables,\n    year: FromFileVariables,\n    month: FromFileVariables,\n    day: FromFileVariables,\n    latitude: FromFileVariables,\n    longitude: FromFileVariables,\n    depth: FromFileVariables,\n    hour: FromFileVariables | None = None,\n    provider: FromFileVariables | None = None,\n    *args: FromFileVariables,\n    **kwargs: FromFileVariables,\n) -&gt; None:\n    super().__init__(\n        expocode,\n        date,\n        year,\n        month,\n        day,\n        latitude,\n        longitude,\n        depth,\n        hour,\n        provider,\n        *args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.SourceVariableSet.features","title":"<code>features: FeatureVariablesSet</code>  <code>property</code>","text":"<p>FeatureVar list.</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.SourceVariableSet.loading_variables","title":"<code>loading_variables: LoadingVariablesSet</code>  <code>property</code>","text":"<p>Ensembles of variables to load.</p>"},{"location":"reference/core/variables/sets/#bgc_data_processing.core.variables.sets.SourceVariableSet.storing_variables","title":"<code>storing_variables: StoringVariablesSet</code>  <code>property</code>","text":"<p>Ensemble of variables to store.</p>"},{"location":"reference/core/variables/vars/","title":"<code>bgc_data_processing.core.variables.vars</code>","text":"<p>Variables.</p>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar","title":"<code>BaseVar(name, unit, var_type, default=np.nan, name_format='%-15s', value_format='%15s')</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Class to store Meta data on a variable of interest.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>'Official' name for the variable : name to use when displaying the variable.</p> required <code>unit</code> <code>str</code> <p>Variable unit (written using the following format: [deg_C] for Celsius degree of [kg] for kilograms).</p> required <code>var_type</code> <code>str</code> <p>Variable type (str, int, datetime...). It will be used to convert the data using df[variable].astype(type)</p> required <code>default</code> <code>Any</code> <p>Default value to set instead of nan., by default np.nan</p> <code>nan</code> <code>name_format</code> <code>str</code> <p>Format to use to save the data name and unit in a csv of txt file. , by default \"%-15s\"</p> <code>'%-15s'</code> <code>value_format</code> <code>str</code> <p>Format to use to save the data value in a csv of txt file., by default \"%15s\"</p> <code>'%15s'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; var_lat = BaseVar(\"LATITUDE\", \"[deg_N]\", float, 7, 6, \"%-12s\", \"%12.6f\")\n</code></pre> <p>Class to store Meta data on a variable of interest.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>'Official' name for the variable : name to use when displaying the variable.</p> required <code>unit</code> <code>str</code> <p>Variable unit (written using the following format: [deg_C] for Celsius degree of [kg] for kilograms).</p> required <code>var_type</code> <code>str</code> <p>Variable type (str, int, datetime...). It will be used to convert the data using df[variable].astype(type)</p> required <code>default</code> <code>Any</code> <p>Default value to set instead of nan., by default np.nan</p> <code>nan</code> <code>name_format</code> <code>str</code> <p>Format to use to save the data name and unit in a csv of txt file. , by default \"%-15s\"</p> <code>'%-15s'</code> <code>value_format</code> <code>str</code> <p>Format to use to save the data value in a csv/txt file., by default \"%15s\"</p> <code>'%15s'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; var_lat = BaseVar(\"LATITUDE\", \"[deg_N]\", float, 7, 6, \"%-12s\", \"%12.6f\")\n</code></pre> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    unit: str,\n    var_type: str,\n    default: Any = np.nan,\n    name_format: str = \"%-15s\",\n    value_format: str = \"%15s\",\n):\n\"\"\"Class to store Meta data on a variable of interest.\n\n    Parameters\n    ----------\n    name : str\n        'Official' name for the variable : name to use when displaying the variable.\n    unit : str\n        Variable unit (written using the following format:\n        [deg_C] for Celsius degree of [kg] for kilograms).\n    var_type : str\n        Variable type (str, int, datetime...).\n        It will be used to convert the data using df[variable].astype(type)\n    default: Any\n        Default value to set instead of nan., by default np.nan\n    name_format: str\n        Format to use to save the data name and unit in a csv of txt file.\n        , by default \"%-15s\"\n    value_format: str\n        Format to use to save the data value in a csv/txt file., by default \"%15s\"\n\n    Examples\n    --------\n    &gt;&gt;&gt; var_lat = BaseVar(\"LATITUDE\", \"[deg_N]\", float, 7, 6, \"%-12s\", \"%12.6f\")\n    \"\"\"\n    self.name = name\n    self.unit = unit\n    self.type = var_type\n    self.default = default\n    self.name_format = name_format\n    self.value_format = value_format\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.is_feature","title":"<code>is_feature = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.exist_in_dset","title":"<code>exist_in_dset: bool = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.name","title":"<code>name = name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.unit","title":"<code>unit = unit</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.type","title":"<code>type = var_type</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.default","title":"<code>default = default</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.name_format","title":"<code>name_format = name_format</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.value_format","title":"<code>value_format = value_format</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.label","title":"<code>label: str</code>  <code>property</code>","text":"<p>Returns the label to use to find the variable data in a dataframe.</p> <p>Returns:</p> Type Description <code>str</code> <p>label.</p>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.__hash__","title":"<code>__hash__()</code>","text":"<p>Hashing method.</p> <p>Returns:</p> Type Description <code>int</code> <p>Hashed object.</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def __hash__(self) -&gt; int:\n\"\"\"Hashing method.\n\n    Returns\n    -------\n    int\n        Hashed object.\n    \"\"\"\n    return hash(self.__repr__())\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.__str__","title":"<code>__str__()</code>","text":"<p>Convert the variable to a string.</p> <p>Returns:</p> Type Description <code>str</code> <p>name - unit (type)</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def __str__(self) -&gt; str:\n\"\"\"Convert the variable to a string.\n\n    Returns\n    -------\n    str\n        name - unit (type)\n    \"\"\"\n    return f\"{self.name} - {self.unit} ({self.type})\"\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.__repr__","title":"<code>__repr__()</code>","text":"<p>Represent the variable as string.</p> <p>Returns:</p> Type Description <code>str</code> <p>name_unit_type</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def __repr__(self) -&gt; str:\n\"\"\"Represent the variable as string.\n\n    Returns\n    -------\n    str\n        name_unit_type\n    \"\"\"\n    return f\"{self.name}_{self.unit}\"\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.BaseVar.__eq__","title":"<code>__eq__(__o)</code>","text":"<p>Test variable equality.</p> <p>Parameters:</p> Name Type Description Default <code>__o</code> <code>object</code> <p>Object to test equality with.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if both are instance of basevar with same representation.</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def __eq__(self, __o: object) -&gt; bool:\n\"\"\"Test variable equality.\n\n    Parameters\n    ----------\n    __o : object\n        Object to test equality with.\n\n    Returns\n    -------\n    bool\n        True if both are instance of basevar with same representation.\n    \"\"\"\n    if isinstance(__o, BaseVar):\n        return repr(self) == repr(__o)\n    return False\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.TemplateVar","title":"<code>TemplateVar</code>","text":"<p>             Bases: <code>BaseVar</code></p> <p>Class to define default variable as a template to ease variable instantiation.</p>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.TemplateVar.building_informations","title":"<code>building_informations()</code>","text":"<p>Self's informations to instanciate object with same informations as self.</p> <p>Returns:</p> Type Description <code>dict</code> <p>arguments to use when initiating an instance of BaseVar.</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def building_informations(self) -&gt; dict:\n\"\"\"Self's informations to instanciate object with same informations as self.\n\n    Returns\n    -------\n    dict\n        arguments to use when initiating an instance of BaseVar.\n    \"\"\"\n    return {\n        \"name\": self.name,\n        \"unit\": self.unit,\n        \"var_type\": self.type,\n        \"default\": self.default,\n        \"name_format\": self.name_format,\n        \"value_format\": self.value_format,\n    }\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.TemplateVar.in_file_as","title":"<code>in_file_as(*args)</code>","text":"<p>Return an ExistingVar.</p> <p>New object has same attributes as self and         the property 'aliases' correctly set up using ExistingVar._set_aliases method.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>str | tuple[str, str, list]</code> <p>Name(s) of the variable in the dataset and the corresponding flags. Aliases are ranked: first will be the only one used if present in dataset. If not second will be checked, and so on.. Aliases are supposed to be formatted as : (alias, flag_alias, flag_values), where alias (str) is the name of the column storing the variable in the dataset, flag_alias (str) is the name of the column storing the variable's flag in the dataset and flag_values (list) is the list of correct values for the flag. If there is no flag columns, flag_alias and flag_values can be set to None, or the argument can be reduced to the variable column name only.</p> <code>()</code> <p>Returns:</p> Type Description <code>ExistingVar</code> <p>Variable with correct loading informations.</p> <p>Examples:</p> <p>To instantiate a variable specifying a flag column to use:</p> <pre><code>&gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n&gt;&gt;&gt; instanciated_var = default_var.in_file_as((\"CTDSAL\", \"CTDSAL_FLAG_W\", [2]))\n</code></pre> <p>To instantiate a variable without flag columns to use:</p> <pre><code>&gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n&gt;&gt;&gt; instanciated_var = default_var.in_file_as((\"CTDSAL\",None,None))\n# or equivalently:\n&gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n&gt;&gt;&gt; instanciated_var = default_var.in_file_as(\"CTDSAL\")\n</code></pre> <p>To instantiate a variable with multiple possible aliases and flags:</p> <pre><code>&gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n&gt;&gt;&gt; instanciated_var = default_var.in_file_as(\n&gt;&gt;&gt;     (\"CTDSAL1\", \"CTDSAL1_FLAG_W\", [2]),\n&gt;&gt;&gt;     (\"CTDSAL2\", \"CTDSAL2_FLAG_W\", [2]),\n&gt;&gt;&gt; )\n</code></pre> <p>To instantiate a variable with multiple possible aliases and some flags:</p> <pre><code>&gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n&gt;&gt;&gt; instanciated_var = default_var.in_file_as(\n&gt;&gt;&gt;     (\"CTDSAL1\", \"CTDSAL1_FLAG_W\", [2]),\n&gt;&gt;&gt;     (\"CTDSAL2\", None, None),\n&gt;&gt;&gt; )\n# or equivalently:\nTo instantiate a variable with multiple possible aliases and some flags:\n&gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n&gt;&gt;&gt; instanciated_var = default_var.in_file_as(\n&gt;&gt;&gt;     (\"CTDSAL1\", \"CTDSAL1_FLAG_W\", [2]),\n&gt;&gt;&gt;     \"CTDSAL2\",\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def in_file_as(self, *args: str | tuple[str, str, list]) -&gt; \"ExistingVar\":\n\"\"\"Return an ExistingVar.\n\n    New object has same attributes as self and \\\n    the property 'aliases' correctly set up using ExistingVar._set_aliases method.\n\n    Parameters\n    ----------\n    args : str | tuple[str, str, list]\n        Name(s) of the variable in the dataset and the corresponding flags.\n        Aliases are ranked: first will be the only one used if present in dataset.\n        If not second will be checked, and so on..\n        Aliases are supposed to be formatted as : (alias, flag_alias, flag_values),\n        where alias (str) is the name of the column storing the variable\n        in the dataset, flag_alias (str) is the name of the column storing\n        the variable's flag in the dataset and flag_values (list) is\n        the list of correct values for the flag.\n        If there is no flag columns, flag_alias and flag_values can be set to None,\n        or the argument can be reduced to the variable column name only.\n\n    Returns\n    -------\n    ExistingVar\n        Variable with correct loading informations.\n\n    Examples\n    --------\n    To instantiate a variable specifying a flag column to use:\n    &gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n    &gt;&gt;&gt; instanciated_var = default_var.in_file_as((\"CTDSAL\", \"CTDSAL_FLAG_W\", [2]))\n\n    To instantiate a variable without flag columns to use:\n    &gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n    &gt;&gt;&gt; instanciated_var = default_var.in_file_as((\"CTDSAL\",None,None))\n    # or equivalently:\n    &gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n    &gt;&gt;&gt; instanciated_var = default_var.in_file_as(\"CTDSAL\")\n\n    To instantiate a variable with multiple possible aliases and flags:\n    &gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n    &gt;&gt;&gt; instanciated_var = default_var.in_file_as(\n    &gt;&gt;&gt;     (\"CTDSAL1\", \"CTDSAL1_FLAG_W\", [2]),\n    &gt;&gt;&gt;     (\"CTDSAL2\", \"CTDSAL2_FLAG_W\", [2]),\n    &gt;&gt;&gt; )\n\n    To instantiate a variable with multiple possible aliases and some flags:\n    &gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n    &gt;&gt;&gt; instanciated_var = default_var.in_file_as(\n    &gt;&gt;&gt;     (\"CTDSAL1\", \"CTDSAL1_FLAG_W\", [2]),\n    &gt;&gt;&gt;     (\"CTDSAL2\", None, None),\n    &gt;&gt;&gt; )\n    # or equivalently:\n    To instantiate a variable with multiple possible aliases and some flags:\n    &gt;&gt;&gt; default_var = TemplateVar(\"PSAL\", \"[psu]\", float, 10, 9, \"%-10s\", \"%10.3f\")\n    &gt;&gt;&gt; instanciated_var = default_var.in_file_as(\n    &gt;&gt;&gt;     (\"CTDSAL1\", \"CTDSAL1_FLAG_W\", [2]),\n    &gt;&gt;&gt;     \"CTDSAL2\",\n    &gt;&gt;&gt; )\n    \"\"\"\n    return ExistingVar.from_template(self).set_aliases(*args)\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.TemplateVar.not_in_file","title":"<code>not_in_file()</code>","text":"<p>Return a NotExistingVar object with same attributes as self.</p> <p>Returns:</p> Type Description <code>NotExistingVar</code> <p>Instanciated variable.</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def not_in_file(self) -&gt; \"NotExistingVar\":\n\"\"\"Return a NotExistingVar object with same attributes as self.\n\n    Returns\n    -------\n    NotExistingVar\n        Instanciated variable.\n    \"\"\"\n    return NotExistingVar.from_template(self)\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.NotExistingVar","title":"<code>NotExistingVar(name, unit, var_type, default=np.nan, name_format='%-15s', value_format='%15s')</code>","text":"<p>             Bases: <code>BaseVar</code></p> <p>Class to represent variables which don't exist in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>'Official' name for the variable : name to use when displaying the variable.</p> required <code>unit</code> <code>str</code> <p>Variable unit (written using the following format: [deg_C] for Celsius degree of [kg] for kilograms).</p> required <code>var_type</code> <code>str</code> <p>Variable type (str, int, datetime...). It will be used to convert the data using df[variable].astype(type)</p> required <code>default</code> <code>Any</code> <p>Default value to set instead of nan., by default np.nan</p> <code>nan</code> <code>name_format</code> <code>str</code> <p>Format to use to save the data name and unit in a csv of txt file. , by default \"%-15s\"</p> <code>'%-15s'</code> <code>value_format</code> <code>str</code> <p>Format to use to save the data value in a csv of txt file., by default \"%15s\"</p> <code>'%15s'</code> <p>Class to represent variables which don't exist in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>'Official' name for the variable : name to use when displaying the variable.</p> required <code>unit</code> <code>str</code> <p>Variable unit (written using the following format: [deg_C] for Celsius degree of [kg] for kilograms).</p> required <code>var_type</code> <code>str</code> <p>Variable type (str, int, datetime...). It will be used to convert the data using df[variable].astype(type)</p> required <code>default</code> <code>Any</code> <p>Default value to set instead of nan., by default np.nan</p> <code>nan</code> <code>name_format</code> <code>str</code> <p>Format to use to save the data name and unit in a csv of txt file. , by default \"%-15s\"</p> <code>'%-15s'</code> <code>value_format</code> <code>str</code> <p>Format to use to save the data value in a csv/txt file., by default \"%15s\"</p> <code>'%15s'</code> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    unit: str,\n    var_type: str,\n    default: Any = np.nan,\n    name_format: str = \"%-15s\",\n    value_format: str = \"%15s\",\n):\n\"\"\"Class to represent variables which don't exist in the dataset.\n\n    Parameters\n    ----------\n    name : str\n        'Official' name for the variable : name to use when displaying the variable.\n    unit : str\n        Variable unit (written using the following format:\n        [deg_C] for Celsius degree of [kg] for kilograms).\n    var_type : str\n        Variable type (str, int, datetime...).\n        It will be used to convert the data using df[variable].astype(type)\n    default: Any\n        Default value to set instead of nan., by default np.nan\n    name_format: str\n        Format to use to save the data name and unit in a csv of txt file.\n        , by default \"%-15s\"\n    value_format: str\n        Format to use to save the data value in a csv/txt file., by default \"%15s\"\n    \"\"\"\n    super().__init__(name, unit, var_type, default, name_format, value_format)\n    self.exist_in_dset = self.__default_exist_in_dset\n    self._remove_if_nan = self.__default_remove_if_nan\n    self._remove_if_all_nan = self.__default_remove_if_all_nan\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.NotExistingVar.__default_exist_in_dset","title":"<code>__default_exist_in_dset: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.NotExistingVar.__default_remove_if_nan","title":"<code>__default_remove_if_nan: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.NotExistingVar.__default_remove_if_all_nan","title":"<code>__default_remove_if_all_nan: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.NotExistingVar.exist_in_dset","title":"<code>exist_in_dset = self.__default_exist_in_dset</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.NotExistingVar.remove_if_nan","title":"<code>remove_if_nan: bool</code>  <code>property</code>","text":"<p>True if the variable must be removed if NaN.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the variable must be removed if NaN.</p>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.NotExistingVar.remove_if_all_nan","title":"<code>remove_if_all_nan: bool</code>  <code>property</code>","text":"<p>Whether the variable must be removed if all same are NaN.</p> <p>If True, then the variable must be removed when this variable and other 'remove if all nan' variables are NaN.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the variable must be removed when this variable and other 'remove if all nan' variables are NaN.</p>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.NotExistingVar.from_template","title":"<code>from_template(template)</code>  <code>classmethod</code>","text":"<p>Instantiate a NotExistingVar from a TemplateVar.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>TemplateVar</code> <p>Template variable to build from.</p> required <p>Returns:</p> Type Description <code>NotExistingVar</code> <p>NotExistingVar from template.</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>@classmethod\ndef from_template(cls, template: \"TemplateVar\") -&gt; \"NotExistingVar\":\n\"\"\"Instantiate a NotExistingVar from a TemplateVar.\n\n    Parameters\n    ----------\n    template : TemplateVar\n        Template variable to build from.\n\n    Returns\n    -------\n    NotExistingVar\n        NotExistingVar from template.\n    \"\"\"\n    return cls(**template.building_informations())\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.NotExistingVar.set_default","title":"<code>set_default(default)</code>","text":"<p>Set the default value for the variable column.</p> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>Any</code> <p>Value to use as default</p> required <p>Returns:</p> Type Description <code>NotExistingVar</code> <p>Self.</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def set_default(self, default: Any) -&gt; \"NotExistingVar\":\n\"\"\"Set the default value for the variable column.\n\n    Parameters\n    ----------\n    default : Any\n        Value to use as default\n\n    Returns\n    -------\n    NotExistingVar\n        Self.\n    \"\"\"\n    self.default = default\n    return self\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.NotExistingVar.remove_when_all_nan","title":"<code>remove_when_all_nan()</code>","text":"<p>Set self._remove_if_all_nan to True.</p> <p>Returns:</p> Type Description <code>NotExistingVar</code> <p>self</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def remove_when_all_nan(self) -&gt; \"NotExistingVar\":\n\"\"\"Set self._remove_if_all_nan to True.\n\n    Returns\n    -------\n    NotExistingVar\n        self\n    \"\"\"\n    self._remove_if_all_nan = True\n    return self\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.NotExistingVar.remove_when_nan","title":"<code>remove_when_nan()</code>","text":"<p>Set self._remove_if_nan to True.</p> <p>Returns:</p> Type Description <code>NotExistingVar</code> <p>self</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def remove_when_nan(self) -&gt; \"NotExistingVar\":\n\"\"\"Set self._remove_if_nan to True.\n\n    Returns\n    -------\n    NotExistingVar\n        self\n    \"\"\"\n    self._remove_if_nan = True\n    return self\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar","title":"<code>ExistingVar(name, unit, var_type, default=np.nan, name_format='%-15s', value_format='%15s')</code>","text":"<p>             Bases: <code>NotExistingVar</code></p> <p>Class to represent variables existing in the dataset.</p> <p>This class allows to specify flag columns, correction functions...</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>'Official' name for the variable : name to use when displaying the variable.</p> required <code>unit</code> <code>str</code> <p>Variable unit (written using the following format: [deg_C] for Celsius degree of [kg] for kilograms).</p> required <code>var_type</code> <code>str</code> <p>Variable type (str, int, datetime...). It will be used to convert the data using df[variable].astype(type)</p> required <code>default</code> <code>Any</code> <p>Default value to set instead of nan., by default np.nan</p> <code>nan</code> <code>name_format</code> <code>str</code> <p>Format to use to save the data name and unit in a csv of txt file. , by default \"%-15s\"</p> <code>'%-15s'</code> <code>value_format</code> <code>str</code> <p>Format to use to save the data value in a csv of txt file., by default \"%15s\"</p> <code>'%15s'</code> <p>Class to represent variables existing in the dataset.</p> <p>This class allows to specify flag columns, correction functions...</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>'Official' name for the variable : name to use when displaying the variable.</p> required <code>unit</code> <code>str</code> <p>Variable unit (written using the following format: [deg_C] for Celsius degree of [kg] for kilograms).</p> required <code>var_type</code> <code>str</code> <p>Variable type (str, int, datetime...). It will be used to convert the data using df[variable].astype(type)</p> required <code>default</code> <code>Any</code> <p>Default value to set instead of nan., by default np.nan</p> <code>nan</code> <code>name_format</code> <code>str</code> <p>Format to use to save the data name and unit in a csv of txt file. , by default \"%-15s\"</p> <code>'%-15s'</code> <code>value_format</code> <code>str</code> <p>Format to use to save the data value in a csv/txt file., by default \"%15s\"</p> <code>'%15s'</code> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    unit: str,\n    var_type: str,\n    default: Any = np.nan,\n    name_format: str = \"%-15s\",\n    value_format: str = \"%15s\",\n):\n\"\"\"Class to represent variables existing in the dataset.\n\n    This class allows to specify flag columns, correction functions...\n\n    Parameters\n    ----------\n    name : str\n        'Official' name for the variable : name to use when displaying the variable.\n    unit : str\n        Variable unit (written using the following format:\n        [deg_C] for Celsius degree of [kg] for kilograms).\n    var_type : str\n        Variable type (str, int, datetime...).\n        It will be used to convert the data using df[variable].astype(type)\n    default: Any\n        Default value to set instead of nan., by default np.nan\n    name_format: str\n        Format to use to save the data name and unit in a csv of txt file.\n        , by default \"%-15s\"\n    value_format: str\n        Format to use to save the data value in a csv/txt file., by default \"%15s\"\n    \"\"\"\n    super().__init__(name, unit, var_type, default, name_format, value_format)\n    self.exist_in_dset = self.__default_exist_in_dset\n    self.correction = self.__default_correction\n    self._aliases = self.__default_aliases.copy()\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar.__default_exist_in_dset","title":"<code>__default_exist_in_dset: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar.__default_correction","title":"<code>__default_correction: callable = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar.__default_aliases","title":"<code>__default_aliases: list[tuple[str, str, list]] = []</code>  <code>class-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar.exist_in_dset","title":"<code>exist_in_dset = self.__default_exist_in_dset</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar.correction","title":"<code>correction = self.__default_correction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar.aliases","title":"<code>aliases: list[tuple[str, str, list]]</code>  <code>property</code>","text":"<p>Getter for aliases.</p> <p>Returns:</p> Type Description <code>list[tuple[str, str, list]]</code> <p>alias, flag column alias (None if not), values to keep from flag column (None if not)</p>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar.remove_if_all_nan","title":"<code>remove_if_all_nan: bool</code>  <code>property</code>","text":"<p>Whether or not to suppress the row when this an other variables are NaN.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if this variable must be included when removing where some variables are all nan.</p>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar.remove_if_nan","title":"<code>remove_if_nan: bool</code>  <code>property</code>","text":"<p>Whether or not to suppress the row when the variable is np.nan.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if rows must be removed when this variable is nan.</p>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar.from_template","title":"<code>from_template(template)</code>  <code>classmethod</code>","text":"<p>Instantiate a ExistingVar from a TemplateVar.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>TemplateVar</code> <p>Template variable to build from.</p> required <p>Returns:</p> Type Description <code>ExistingVar</code> <p>ExistingVar from template.</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>@classmethod\ndef from_template(cls, template: \"TemplateVar\") -&gt; \"ExistingVar\":\n\"\"\"Instantiate a ExistingVar from a TemplateVar.\n\n    Parameters\n    ----------\n    template : TemplateVar\n        Template variable to build from.\n\n    Returns\n    -------\n    ExistingVar\n        ExistingVar from template.\n    \"\"\"\n    return super().from_template(template)\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar.set_aliases","title":"<code>set_aliases(*args)</code>","text":"<p>Set aliases for the variable.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>str | tuple[str, str, list]</code> <p>Name(s) of the variable in the dataset and the corresponding flags. Aliases are ranked: first alias will be the only one considered if present in dataset. If not second will be checked, and so on.. Aliases are supposed to be formatted as : (alias, flag_alias, flag_values), where alias (str) is the name of the column storing the variable in the dataset, flag_alias (str) is the name of the column storing the variable's flag in the dataset and flag_values (list) is the list of correct values for the flag. If there is no flag columns, flag_alias and flag_values can be set to None, or the argument can be reduced to the variable column name only.</p> <code>()</code> <p>Returns:</p> Type Description <code>ExistingVar</code> <p>Updated version of self</p> <p>Raises:</p> Type Description <code>VariableInstantiationError</code> <p>If one of the arguments length is different than 1 and 3.</p> <code>ValueError</code> <p>If one of the arguments is not an instance of string or Iterable.</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def set_aliases(self, *args: str | tuple[str, str, list]) -&gt; \"ExistingVar\":\n\"\"\"Set aliases for the variable.\n\n    Parameters\n    ----------\n    args : str | tuple[str, str, list]\n        Name(s) of the variable in the dataset and the corresponding flags.\n        Aliases are ranked: first alias will be the only one considered if present\n        in dataset. If not second will be checked, and so on..\n        Aliases are supposed to be formatted as : (alias, flag_alias, flag_values),\n        where alias (str) is the name of the column storing the variable\n        in the dataset, flag_alias (str) is the name of the column storing\n        the variable's flag in the dataset and flag_values (list) is the list\n        of correct values for the flag.\n        If there is no flag columns, flag_alias and flag_values can be set to None,\n        or the argument can be reduced to the variable column name only.\n\n    Returns\n    -------\n    \"ExistingVar\"\n        Updated version of self\n\n    Raises\n    ------\n    VariableInstantiationError\n        If one of the arguments length is different than 1 and 3.\n    ValueError\n        If one of the arguments is not an instance of string or Iterable.\n    \"\"\"\n    aliases = []\n    for arg in args:\n        if isinstance(arg, str):\n            alias = arg\n            flag_alias = None\n            flag_value = None\n        elif isinstance(arg, Iterable):\n            if len(arg) == 1:\n                alias = arg[0]\n                flag_alias = None\n                flag_value = None\n            elif len(arg) == 3:\n                alias = arg[0]\n                flag_alias = arg[1]\n                flag_value = arg[2]\n            else:\n                msg = f\"{arg} can't be of length {len(arg)}\"\n                raise VariableInstantiationError(msg)\n        else:\n            msg = f\"{arg} must be str or Iterable\"\n            raise TypeError(msg)\n        aliases.append((alias, flag_alias, flag_value))\n    self._aliases = aliases\n    return self\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ExistingVar.correct_with","title":"<code>correct_with(function)</code>","text":"<p>Correction function definition.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>Function to apply to the dataframe row storing this variable's values.</p> required <p>Returns:</p> Type Description <code>ExistingVar</code> <p>self.</p> <p>Raises:</p> Type Description <code>VariableInstantiationError</code> <p>If the given object is not callable.</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def correct_with(self, function: Callable) -&gt; \"ExistingVar\":\n\"\"\"Correction function definition.\n\n    Parameters\n    ----------\n    function : Callable\n        Function to apply to the dataframe row storing this variable's values.\n\n    Returns\n    -------\n    ExistingVar\n        self.\n\n    Raises\n    ------\n    VariableInstantiationError\n        If the given object is not callable.\n    \"\"\"\n    if not isinstance(function, Callable):\n        msg = \"Correcting function must be callable.\"\n        raise VariableInstantiationError(msg)\n    self.correction = function\n    self._has_correction = True\n    return self\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ParsedVar","title":"<code>ParsedVar</code>","text":"<p>             Bases: <code>BaseVar</code></p> <p>Variables parsed from a csv file.</p>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.ParsedVar.__repr__","title":"<code>__repr__()</code>","text":"<p>Represent the parsed variable as a string.</p> <p>Returns:</p> Type Description <code>str</code> <p>name_unit</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def __repr__(self) -&gt; str:\n\"\"\"Represent the parsed variable as a string.\n\n    Returns\n    -------\n    str\n        name_unit\n    \"\"\"\n    return f\"{self.name}_{self.unit}\"\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.FeatureVar","title":"<code>FeatureVar(feature)</code>","text":"<p>             Bases: <code>BaseVar</code></p> <p>Variable resulting of an operation between variables.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>BaseFeature</code> <p>Feature the variable comes from.</p> required Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def __init__(self, feature: \"BaseFeature\"):\n    super().__init__(\n        feature.variable.name,\n        feature.variable.unit,\n        feature.variable.type,\n        feature.variable.default,\n        feature.variable.name_format,\n        feature.variable.value_format,\n    )\n    self._feature = feature\n    self.required_vars = feature.required_variables\n</code></pre>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.FeatureVar.is_feature","title":"<code>is_feature = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.FeatureVar.exist_in_dset","title":"<code>exist_in_dset: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.FeatureVar.required_vars","title":"<code>required_vars = feature.required_variables</code>  <code>instance-attribute</code>","text":""},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.FeatureVar.feature","title":"<code>feature: BaseFeature</code>  <code>property</code>","text":"<p>Feature for the variable.</p>"},{"location":"reference/core/variables/vars/#bgc_data_processing.core.variables.vars.FeatureVar.is_loadable","title":"<code>is_loadable(loaded_list)</code>","text":"<p>Find if the variable can be made using given some loaded variables.</p> <p>Parameters:</p> Name Type Description Default <code>loaded_list</code> <code>list[ExistingVar | NotExistingVar]</code> <p>List of available variables.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the variable is loadable.</p> Source code in <code>src/bgc_data_processing/core/variables/vars.py</code> <pre><code>def is_loadable(self, loaded_list: list[ExistingVar | NotExistingVar]) -&gt; bool:\n\"\"\"Find if the variable can be made using given some loaded variables.\n\n    Parameters\n    ----------\n    loaded_list : list[ExistingVar  |  NotExistingVar]\n        List of available variables.\n\n    Returns\n    -------\n    bool\n        True if the variable is loadable.\n    \"\"\"\n    for var in self.required_vars:\n        if not any(x.name == var.name for x in loaded_list):\n            return False\n    return True\n</code></pre>"},{"location":"reference/defaults/","title":"<code>bgc_data_processing.defaults</code>","text":"<p>Defaults object built using the configuration files</p> <p>The default objects include:</p> <ul> <li>PROVIDERS_CONFIG -&gt; providers configuration as detailled in config/providers.toml</li> <li>VARS -&gt; default variables defined in config/variables.toml</li> <li>WATER_MASS -&gt; default water masses defined in config/water_masses.toml</li> </ul> <p>To access the defaults objects: <pre><code>&gt;&gt;&gt; import bgc_data_processing as bgc_dp\n&gt;&gt;&gt; bgc_dp.defaults.PROVIDERS_CONFIG    # Providers configuration\n&gt;&gt;&gt; bgc_dp.defaults.VARS                # Default variables\n&gt;&gt;&gt; bgc_dp.defaults.WATER_MASS          # default water masses\n</code></pre></p>"},{"location":"reference/providers/","title":"<code>bgc_data_processing.providers</code>","text":"<p>Build all data sources which are defined in src/bgc_data_processing/providers.</p> <p>From this namespace is accessible:</p> <ul> <li><code>PROVIDERS</code> -&gt; Providers mapping between: name -&gt; DataSource</li> </ul>"},{"location":"reference/providers/argo/","title":"<code>bgc_data_processing.providers.argo</code>","text":"<p>Specific parameters to load Argo-provided data.</p>"},{"location":"reference/providers/argo/#bgc_data_processing.providers.argo.loader","title":"<code>loader = DataSource(provider_name='ARGO', data_format='netcdf', dirin=Path(PROVIDERS_CONFIG['ARGO']['PATH']), data_category=PROVIDERS_CONFIG['ARGO']['CATEGORY'], excluded_files=PROVIDERS_CONFIG['ARGO']['EXCLUDE'], files_pattern=FileNamePattern('.*.nc'), variable_ensemble=SourceVariableSet(provider=VARS['provider'].not_in_file(), expocode=VARS['expocode'].not_in_file(), date=VARS['date'].in_file_as('TIME'), year=VARS['year'].not_in_file(), month=VARS['month'].not_in_file(), day=VARS['day'].not_in_file(), hour=VARS['hour'].not_in_file(), longitude=VARS['longitude'].in_file_as('LONGITUDE'), latitude=VARS['latitude'].in_file_as('LATITUDE'), depth=VARS['depth'].in_file_as('PRES_ADJUSTED').remove_when_nan().correct_with(lambda : -np.abs(x)), temperature=VARS['temperature'].in_file_as(('TEMP_ADJUSTED', 'TEMP_ADJUSTED_QC', [1]), ('TEMP', 'TEMP_QC', [1])), salinity=VARS['salinity'].in_file_as(('PSAL_ADJUSTED', 'PSAL_ADJUSTED_QC', [1]), ('PSAl', 'PSAl_QC', [1])), oxygen=VARS['oxygen'].in_file_as('DOX2_ADJUSTED', 'DOX2').correct_with(units.convert_umol_by_kg_to_mmol_by_m3), phosphate=VARS['phosphate'].not_in_file(), nitrate=VARS['nitrate'].not_in_file(), silicate=VARS['silicate'].not_in_file(), chlorophyll=VARS['chlorophyll'].in_file_as(('CPHL_ADJUSTED', 'CPHL_ADJUSTED_QC', [1]), ('CPHL', 'CPHL_QC', [1])).remove_when_all_nan().correct_with(lambda : np.nan if x &lt; 0.01 else x)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/providers/clivar/","title":"<code>bgc_data_processing.providers.clivar</code>","text":"<p>Specific parameters to load CLIVAR-provided data.</p>"},{"location":"reference/providers/clivar/#bgc_data_processing.providers.clivar.loader","title":"<code>loader = DataSource(provider_name='CLIVAR', data_format='csv', dirin=Path(PROVIDERS_CONFIG['CLIVAR']['PATH']), data_category=PROVIDERS_CONFIG['CLIVAR']['CATEGORY'], excluded_files=PROVIDERS_CONFIG['CLIVAR']['EXCLUDE'], files_pattern=FileNamePattern('clivar_({years})[0-9][0-9][0-9][0-9]_.*.csv'), variable_ensemble=SourceVariableSet(provider=VARS['provider'].not_in_file(), expocode=VARS['expocode'].in_file_as('EXPOCODE'), date=VARS['date'].in_file_as('DATE'), year=VARS['year'].not_in_file(), month=VARS['month'].not_in_file(), day=VARS['day'].not_in_file(), hour=VARS['hour'].not_in_file(), longitude=VARS['longitude'].in_file_as('LONGITUDE'), latitude=VARS['latitude'].in_file_as('LATITUDE'), depth=VARS['depth'].in_file_as('CTDPRS').remove_when_nan().correct_with(lambda : -x), temperature=VARS['temperature'].in_file_as('CTDTMP'), salinity=VARS['salinity'].in_file_as(('CTDSAL', 'CTDSAL_FLAG_W', [2])), oxygen=VARS['oxygen'].in_file_as(('OXYGEN', 'OXYGEN_FLAG_W', [2])).correct_with(units.convert_umol_by_kg_to_mmol_by_m3), phosphate=VARS['phosphate'].in_file_as(('PHSPHT', 'PHSPHT_FLAG_W', [2])).remove_when_all_nan(), nitrate=VARS['nitrate'].in_file_as(('NITRAT', 'NITRAT_FLAG_W', [2])).remove_when_all_nan(), silicate=VARS['silicate'].in_file_as(('SILCAT', 'SILCAT_FLAG_W', [2])).remove_when_all_nan(), chlorophyll=VARS['chlorophyll'].not_in_file()), read_params={'low_memory': False, 'skiprows': [1]})</code>  <code>module-attribute</code>","text":""},{"location":"reference/providers/cmems/","title":"<code>bgc_data_processing.providers.cmems</code>","text":"<p>Specific parameters to load CMEMS-provided data.</p>"},{"location":"reference/providers/cmems/#bgc_data_processing.providers.cmems.loader","title":"<code>loader = DataSource(provider_name='CMEMS', data_format='netcdf', dirin=Path(PROVIDERS_CONFIG['CMEMS']['PATH']), data_category=PROVIDERS_CONFIG['CMEMS']['CATEGORY'], excluded_files=PROVIDERS_CONFIG['CMEMS']['EXCLUDE'], files_pattern=FileNamePattern('.*.nc'), variable_ensemble=SourceVariableSet(provider=VARS['provider'].not_in_file(), expocode=VARS['expocode'].not_in_file(), date=VARS['date'].in_file_as('TIME'), year=VARS['year'].not_in_file(), month=VARS['month'].not_in_file(), day=VARS['day'].not_in_file(), hour=VARS['hour'].not_in_file(), longitude=VARS['longitude'].in_file_as('LONGITUDE'), latitude=VARS['latitude'].in_file_as('LATITUDE'), depth=VARS['depth'].in_file_as('DEPH', 'PRES').remove_when_nan().correct_with(lambda : -np.abs(x)), temperature=VARS['temperature'].in_file_as(('TEMP', 'TEMP_QC', [1])), salinity=VARS['salinity'].in_file_as(('PSAL', 'PSL_QC', [1])), oxygen=VARS['oxygen'].in_file_as('DOX1').correct_with(units.convert_doxy_ml_by_l_to_mmol_by_m3), phosphate=VARS['phosphate'].in_file_as(('PHOS', 'PHOS_QC', [1])).remove_when_all_nan(), nitrate=VARS['nitrate'].in_file_as(('NTRA', 'NTRA_QC', [1])).remove_when_all_nan(), silicate=VARS['silicate'].in_file_as(('SLCA', 'SLCA_QC', [1])).remove_when_all_nan(), chlorophyll=VARS['chlorophyll'].in_file_as(('CPHL', 'CPHL_QC', [1])).remove_when_all_nan()))</code>  <code>module-attribute</code>","text":""},{"location":"reference/providers/esacci_oc/","title":"<code>bgc_data_processing.providers.esacci_oc</code>","text":"<p>Specific parameters to load ESACCI-OC-provided data.</p>"},{"location":"reference/providers/esacci_oc/#bgc_data_processing.providers.esacci_oc.loader","title":"<code>loader = DataSource(provider_name='ESACCI-OC', data_format='netcdf', dirin=Path(PROVIDERS_CONFIG['ESACCI-OC']['PATH']), data_category=PROVIDERS_CONFIG['ESACCI-OC']['CATEGORY'], excluded_files=PROVIDERS_CONFIG['ESACCI-OC']['EXCLUDE'], files_pattern=FileNamePattern('{years}/.*-{years}{months}{days}-.*.nc'), variable_ensemble=SourceVariableSet(provider=VARS['provider'].not_in_file(), expocode=VARS['expocode'].not_in_file(), date=VARS['date'].in_file_as('time'), year=VARS['year'].not_in_file(), month=VARS['month'].not_in_file(), day=VARS['day'].not_in_file(), hour=VARS['hour'].not_in_file(), longitude=VARS['longitude'].in_file_as('lon'), latitude=VARS['latitude'].in_file_as('lat'), depth=VARS['depth'].not_in_file().set_default(0), temperature=VARS['temperature'].not_in_file(), salinity=VARS['salinity'].not_in_file(), oxygen=VARS['oxygen'].not_in_file(), phosphate=VARS['phosphate'].not_in_file(), nitrate=VARS['nitrate'].not_in_file(), silicate=VARS['silicate'].not_in_file(), chlorophyll=VARS['chlorophyll'].in_file_as('chlor_a').remove_when_nan()))</code>  <code>module-attribute</code>","text":""},{"location":"reference/providers/glodap/","title":"<code>bgc_data_processing.providers.glodap</code>","text":"<p>Specific parameters to load GLODAPv2-provided data.</p>"},{"location":"reference/providers/glodap/#bgc_data_processing.providers.glodap.loader","title":"<code>loader = DataSource(provider_name='GLODAPv2', data_format='csv', dirin=Path(PROVIDERS_CONFIG['GLODAPv2']['PATH']), data_category=PROVIDERS_CONFIG['GLODAPv2']['CATEGORY'], excluded_files=PROVIDERS_CONFIG['GLODAPv2']['EXCLUDE'], files_pattern=FileNamePattern('glodapv2_{years}.csv'), variable_ensemble=SourceVariableSet(provider=VARS['provider'].not_in_file(), expocode=VARS['expocode'].in_file_as('cruise'), date=VARS['date'].not_in_file(), year=VARS['year'].in_file_as('YEAR'), month=VARS['month'].in_file_as('MONTH'), day=VARS['day'].in_file_as('DAY'), hour=VARS['hour'].in_file_as('hour'), longitude=VARS['longitude'].in_file_as('LONGITUDE'), latitude=VARS['latitude'].in_file_as('LATITUDE'), depth=VARS['depth'].in_file_as('DEPTH').remove_when_nan().correct_with(lambda : -x), temperature=VARS['temperature'].in_file_as('THETA'), salinity=VARS['salinity'].in_file_as('SALNTY'), oxygen=VARS['oxygen'].in_file_as('OXYGEN').correct_with(units.convert_umol_by_kg_to_mmol_by_m3), phosphate=VARS['phosphate'].in_file_as('PHSPHT').remove_when_all_nan(), nitrate=VARS['nitrate'].in_file_as('NITRAT').remove_when_all_nan(), silicate=VARS['silicate'].in_file_as('SILCAT').remove_when_all_nan(), chlorophyll=VARS['chlorophyll'].not_in_file().remove_when_all_nan()), read_params={'low_memory': False, 'index_col': False, 'skiprows': [1]})</code>  <code>module-attribute</code>","text":""},{"location":"reference/providers/glodap_2019/","title":"<code>bgc_data_processing.providers.glodap_2019</code>","text":"<p>Specific parameters to load GLODAPv2.2019-provided data.</p>"},{"location":"reference/providers/glodap_2019/#bgc_data_processing.providers.glodap_2019.loader","title":"<code>loader = DataSource(provider_name='GLODAP_2019', data_format='csv', dirin=Path(PROVIDERS_CONFIG['GLODAP_2019']['PATH']), data_category=PROVIDERS_CONFIG['GLODAP_2019']['CATEGORY'], excluded_files=PROVIDERS_CONFIG['GLODAP_2019']['EXCLUDE'], files_pattern=FileNamePattern('glodapv2_({years}).csv'), variable_ensemble=SourceVariableSet(provider=VARS['provider'].not_in_file(), expocode=VARS['expocode'].in_file_as('cruise'), date=VARS['date'].not_in_file(), year=VARS['year'].in_file_as('YEAR'), month=VARS['month'].in_file_as('MONTH'), day=VARS['day'].in_file_as('DAY'), hour=VARS['hour'].in_file_as('hour'), longitude=VARS['longitude'].in_file_as('LONGITUDE'), latitude=VARS['latitude'].in_file_as('LATITUDE'), depth=VARS['depth'].in_file_as('DEPTH').remove_when_nan().correct_with(lambda : -x), temperature=VARS['temperature'].in_file_as('THETA'), salinity=VARS['salinity'].in_file_as(('SALNTY', 'salinityf', [2])), oxygen=VARS['oxygen'].in_file_as(('OXYGEN', 'oxygenf', [2])).correct_with(units.convert_umol_by_kg_to_mmol_by_m3), phosphate=VARS['phosphate'].in_file_as(('PHSPHT', 'phosphatef', [2])).remove_when_all_nan(), nitrate=VARS['nitrate'].in_file_as(('NITRAT', 'nitratef', [2])).remove_when_all_nan(), silicate=VARS['silicate'].in_file_as(('SILCAT', 'silicatef', [2])).remove_when_all_nan(), chlorophyll=VARS['chlorophyll'].not_in_file().remove_when_all_nan()), read_params={'low_memory': False, 'index_col': False, 'skiprows': [1]})</code>  <code>module-attribute</code>","text":""},{"location":"reference/providers/glodap_2022/","title":"<code>bgc_data_processing.providers.glodap_2022</code>","text":"<p>Specific parameters to load GLODAPv2.2022-provided data.</p>"},{"location":"reference/providers/glodap_2022/#bgc_data_processing.providers.glodap_2022.loader","title":"<code>loader = DataSource(provider_name='GLODAP_2022', data_format='csv', dirin=Path(PROVIDERS_CONFIG['GLODAP_2022']['PATH']), data_category=PROVIDERS_CONFIG['GLODAP_2022']['CATEGORY'], excluded_files=PROVIDERS_CONFIG['GLODAP_2022']['EXCLUDE'], files_pattern=FileNamePattern('GLODAPv2.2022_all.csv'), variable_ensemble=SourceVariableSet(provider=VARS['provider'].not_in_file(), expocode=VARS['expocode'].in_file_as('G2expocode'), date=VARS['date'].not_in_file(), year=VARS['year'].in_file_as('G2year'), month=VARS['month'].in_file_as('G2month'), day=VARS['day'].in_file_as('G2day'), hour=VARS['hour'].in_file_as('G2hour'), longitude=VARS['longitude'].in_file_as('G2longitude'), latitude=VARS['latitude'].in_file_as('G2latitude'), depth=VARS['depth'].in_file_as('G2depth').remove_when_nan().correct_with(lambda : -x), temperature=VARS['temperature'].in_file_as('G2temperature'), salinity=VARS['salinity'].in_file_as(('G2salinity', 'G2salinityf', [2])), oxygen=VARS['oxygen'].in_file_as(('G2oxygen', 'G2oxygenf', [2])).correct_with(units.convert_umol_by_kg_to_mmol_by_m3), phosphate=VARS['phosphate'].in_file_as(('G2phosphate', 'G2phosphatef', [2])).remove_when_all_nan(), nitrate=VARS['nitrate'].in_file_as(('G2nitrate', 'G2nitratef', [2])).remove_when_all_nan(), silicate=VARS['silicate'].in_file_as(('G2silicate', 'G2silicatef', [2])).remove_when_all_nan(), chlorophyll=VARS['chlorophyll'].not_in_file().remove_when_all_nan()), read_params={'low_memory': False, 'index_col': False, 'na_values': -9999})</code>  <code>module-attribute</code>","text":""},{"location":"reference/providers/hycom/","title":"<code>bgc_data_processing.providers.hycom</code>","text":"<p>Specific parameters to load Argo-provided data.</p>"},{"location":"reference/providers/hycom/#bgc_data_processing.providers.hycom.loader","title":"<code>loader = DataSource(provider_name='HYCOM', data_format='abfiles', dirin=Path(PROVIDERS_CONFIG['HYCOM']['PATH']), data_category=PROVIDERS_CONFIG['HYCOM']['CATEGORY'], excluded_files=PROVIDERS_CONFIG['HYCOM']['EXCLUDE'], files_pattern=FileNamePattern('archm.{years}_[0-9]*_[0-9]*.a'), variable_ensemble=SourceVariableSet(provider=VARS['provider'].not_in_file().set_default('HYCOM'), expocode=VARS['expocode'].not_in_file(), date=VARS['date'].not_in_file(), year=VARS['year'].not_in_file(), month=VARS['month'].not_in_file(), day=VARS['day'].not_in_file(), hour=VARS['hour'].not_in_file(), longitude=VARS['longitude'].in_file_as('plon'), latitude=VARS['latitude'].in_file_as('plat'), depth=VARS['depth'].in_file_as('thknss'), temperature=VARS['temperature'].in_file_as('temp'), salinity=VARS['salinity'].in_file_as('salin'), oxygen=VARS['oxygen'].in_file_as('ECO_oxy'), phosphate=VARS['phosphate'].in_file_as('ECO_pho').correct_with(units.convert_phosphate_mgc_by_m3_to_umol_by_l), nitrate=VARS['nitrate'].in_file_as('ECO_no3').correct_with(units.convert_nitrate_mgc_by_m3_to_umol_by_l), silicate=VARS['silicate'].in_file_as('ECO_sil').correct_with(units.convert_silicate_mgc_by_m3_to_umol_by_l), chlorophyll=FeatureVar(ChlorophyllFromDiatomFlagellate.copy_var_infos_from_template(template=VARS['chlorophyll'], diatom_variable=VARS['diatom'].in_file_as('ECO_diac'), flagellate_variable=VARS['flagellate'].in_file_as('ECO_flac')))), grid_basename=PROVIDERS_CONFIG['HYCOM']['REGIONAL_GRID_BASENAME'])</code>  <code>module-attribute</code>","text":""},{"location":"reference/providers/ices/","title":"<code>bgc_data_processing.providers.ices</code>","text":"<p>Specific parameters to load ICES-provided data.</p>"},{"location":"reference/providers/ices/#bgc_data_processing.providers.ices.loader","title":"<code>loader = DataSource(provider_name='ICES', data_format='csv', dirin=Path(PROVIDERS_CONFIG['ICES']['PATH']), data_category=PROVIDERS_CONFIG['ICES']['CATEGORY'], excluded_files=PROVIDERS_CONFIG['ICES']['EXCLUDE'], files_pattern=FileNamePattern('ices_{years}.csv'), variable_ensemble=SourceVariableSet(provider=VARS['provider'].not_in_file(), expocode=VARS['expocode'].in_file_as('Cruise'), date=VARS['date'].in_file_as('DATE'), year=VARS['year'].not_in_file(), month=VARS['month'].not_in_file(), day=VARS['day'].not_in_file(), hour=VARS['hour'].not_in_file(), longitude=VARS['longitude'].in_file_as('LONGITUDE'), latitude=VARS['latitude'].in_file_as('LATITUDE'), depth=VARS['depth'].in_file_as('DEPTH').remove_when_nan().correct_with(lambda : -x), temperature=VARS['temperature'].in_file_as('CTDTMP'), salinity=VARS['salinity'].in_file_as('CTDSAL'), oxygen=VARS['oxygen'].in_file_as('DOXY').correct_with(units.convert_doxy_ml_by_l_to_mmol_by_m3), phosphate=VARS['phosphate'].in_file_as('PHOS').remove_when_all_nan(), nitrate=VARS['nitrate'].in_file_as('NTRA').remove_when_all_nan(), silicate=VARS['silicate'].in_file_as('SLCA').remove_when_all_nan(), chlorophyll=VARS['chlorophyll'].in_file_as('CPHL').remove_when_all_nan()), read_params={'low_memory': False, 'skiprows': [1]})</code>  <code>module-attribute</code>","text":""},{"location":"reference/providers/imr/","title":"<code>bgc_data_processing.providers.imr</code>","text":"<p>Specific parameters to load IMR-provided data.</p>"},{"location":"reference/providers/imr/#bgc_data_processing.providers.imr.loader","title":"<code>loader = DataSource(provider_name='IMR', data_format='csv', dirin=Path(PROVIDERS_CONFIG['IMR']['PATH']), data_category=PROVIDERS_CONFIG['IMR']['CATEGORY'], excluded_files=PROVIDERS_CONFIG['IMR']['EXCLUDE'], files_pattern=FileNamePattern('imr_{years}.csv'), variable_ensemble=SourceVariableSet(provider=VARS['provider'].not_in_file(), expocode=VARS['expocode'].not_in_file(), date=VARS['date'].not_in_file(), year=VARS['year'].in_file_as('Year'), month=VARS['month'].in_file_as('Month'), day=VARS['day'].in_file_as('Day'), hour=VARS['hour'].not_in_file(), longitude=VARS['longitude'].in_file_as('Long'), latitude=VARS['latitude'].in_file_as('Lati'), depth=VARS['depth'].in_file_as('Depth').remove_when_nan(), temperature=VARS['temperature'].in_file_as('Temp'), salinity=VARS['salinity'].in_file_as('Saln.'), oxygen=VARS['oxygen'].in_file_as('Oxygen', 'Doxy').correct_with(units.convert_doxy_ml_by_l_to_mmol_by_m3), phosphate=VARS['phosphate'].in_file_as('Phosphate').remove_when_all_nan(), nitrate=VARS['nitrate'].in_file_as('Nitrate').remove_when_all_nan(), silicate=VARS['silicate'].in_file_as('Silicate').remove_when_all_nan(), chlorophyll=VARS['chlorophyll'].in_file_as('Chl.').remove_when_all_nan()), read_params={'low_memory': False, 'delim_whitespace': True, 'skiprows': [1]})</code>  <code>module-attribute</code>","text":""},{"location":"reference/providers/nmdc/","title":"<code>bgc_data_processing.providers.nmdc</code>","text":"<p>Specific parameters to load NMDC-provided data.</p>"},{"location":"reference/providers/nmdc/#bgc_data_processing.providers.nmdc.loader","title":"<code>loader = DataSource(provider_name='NMDC', data_format='csv', dirin=Path(PROVIDERS_CONFIG['NMDC']['PATH']), data_category=PROVIDERS_CONFIG['NMDC']['CATEGORY'], excluded_files=PROVIDERS_CONFIG['NMDC']['EXCLUDE'], files_pattern=FileNamePattern('NMDC_1990-2019_all.csv'), variable_ensemble=SourceVariableSet(provider=VARS['provider'].not_in_file(), expocode=VARS['expocode'].in_file_as('SDN_CRUISE'), date=VARS['date'].in_file_as('Time'), year=VARS['year'].not_in_file(), month=VARS['month'].not_in_file(), day=VARS['day'].not_in_file(), hour=VARS['hour'].not_in_file(), longitude=VARS['longitude'].in_file_as('Longitude'), latitude=VARS['latitude'].in_file_as('Latitude'), depth=VARS['depth'].in_file_as('depth').remove_when_nan().correct_with(lambda : -x), temperature=VARS['temperature'].not_in_file(), salinity=VARS['salinity'].not_in_file(), oxygen=VARS['oxygen'].in_file_as('DOW').correct_with(units.convert_doxy_ml_by_l_to_mmol_by_m3), phosphate=VARS['phosphate'].in_file_as(('Phosphate', 'Phosphate_SEADATANET_QC', [1])).remove_when_all_nan(), nitrate=VARS['nitrate'].in_file_as(('Nitrate', 'Nitrate_SEADATANET_QC', [1])).remove_when_all_nan(), silicate=VARS['silicate'].in_file_as(('Silicate', 'Silicate_SEADATANET_QC', [1])).remove_when_all_nan(), chlorophyll=VARS['chlorophyll'].in_file_as(('ChlA', 'ChlA_SEADATANET_QC', [1])).remove_when_all_nan()), read_params={'low_memory': False, 'skiprows': [1]})</code>  <code>module-attribute</code>","text":""},{"location":"reference/utils/","title":"<code>bgc_data_processing.utils</code>","text":"<p>Various utilities.</p> <p>From this namespace are accessible:</p> <ul> <li><code>convert_polygons</code>    -&gt; polygon format conversion related objects</li> <li><code>dateranges</code>          -&gt; daterange generation related objects</li> <li><code>patterns</code>            -&gt; filepath parsing related objects</li> </ul>"},{"location":"reference/utils/convert_polygons/","title":"<code>bgc_data_processing.utils.convert_polygons</code>","text":"<p>Functions to convert a shaeply Polygon to list of nodes.</p>"},{"location":"reference/utils/convert_polygons/#bgc_data_processing.utils.convert_polygons.polygon_to_list","title":"<code>polygon_to_list(polygon)</code>","text":"<p>Convert polygon from shapely to list of coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Polygon to convert.</p> required <p>Returns:</p> Type Description <code>Tuple[list, list]</code> <p>List of longitude values, list of latitude values.</p> Source code in <code>src/bgc_data_processing/utils/convert_polygons.py</code> <pre><code>def polygon_to_list(polygon: \"Polygon\") -&gt; tuple[list, list]:\n\"\"\"Convert polygon from shapely to list of coordinates.\n\n    Parameters\n    ----------\n    polygon : Polygon\n        Polygon to convert.\n\n    Returns\n    -------\n    Tuple[list, list]\n        List of longitude values, list of latitude values.\n    \"\"\"\n    x_raw, y_raw = polygon.exterior.coords.xy\n    x = list(x_raw)\n    y = list(y_raw)\n    return x, y\n</code></pre>"},{"location":"reference/utils/dateranges/","title":"<code>bgc_data_processing.utils.dateranges</code>","text":"<p>Date range generating objects.</p>"},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRangeGenerator","title":"<code>DateRangeGenerator(start, end, interval, interval_length=1)</code>","text":"<p>Generate date ranges.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>datetime</code> <p>Starting date of the range.</p> required <code>end</code> <code>datetime</code> <p>Ending date of the range.</p> required <code>interval</code> <code>str</code> <p>Type of interval, 'day', 'week', 'month', 'year' or 'custom'.</p> required <code>interval_length</code> <code>int</code> <p>Length of custom interval, in days., by default 1</p> <code>1</code> <p>Generate date ranges.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>datetime</code> <p>Starting date of the range.</p> required <code>end</code> <code>datetime</code> <p>Ending date of the range.</p> required <code>interval</code> <code>str</code> <p>Type of interval, 'day', 'week', 'month', 'year' or 'custom'.</p> required <code>interval_length</code> <code>int</code> <p>Length of custom interval, in days., by default 1</p> <code>1</code> Source code in <code>src/bgc_data_processing/utils/dateranges.py</code> <pre><code>def __init__(\n    self,\n    start: dt.datetime,\n    end: dt.datetime,\n    interval: str,\n    interval_length: int = 1,\n) -&gt; None:\n\"\"\"Generate date ranges.\n\n    Parameters\n    ----------\n    start : dt.datetime\n        Starting date of the range.\n    end : dt.datetime\n        Ending date of the range.\n    interval : str\n        Type of interval, 'day', 'week', 'month', 'year' or 'custom'.\n    interval_length : int, optional\n        Length of custom interval, in days., by default 1\n    \"\"\"\n    self.start = pd.to_datetime(start).normalize()\n    self.end = pd.to_datetime(end).normalize()\n    self.interval = interval\n    self.interval_length = interval_length\n</code></pre>"},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRangeGenerator.freqs","title":"<code>freqs: dict[str, str] = {'day': 'D', 'week': 'W', 'month': 'M', 'year': 'Y'}</code>  <code>class-attribute</code>","text":""},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRangeGenerator.start_column_name","title":"<code>start_column_name: str = 'start_date'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRangeGenerator.end_column_name","title":"<code>end_column_name: str = 'end_date'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRangeGenerator.start","title":"<code>start = pd.to_datetime(start).normalize()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRangeGenerator.end","title":"<code>end = pd.to_datetime(end).normalize()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRangeGenerator.interval","title":"<code>interval = interval</code>  <code>instance-attribute</code>","text":""},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRangeGenerator.interval_length","title":"<code>interval_length = interval_length</code>  <code>instance-attribute</code>","text":""},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRangeGenerator.__call__","title":"<code>__call__()</code>","text":"<p>Load the date ranges.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Range DataFrame with self.start_column_name and             self.end_column_name columns. Both start and end dates             are supposed to be included in the final range.</p> Source code in <code>src/bgc_data_processing/utils/dateranges.py</code> <pre><code>def __call__(self) -&gt; \"DateRange\":\n\"\"\"Load the date ranges.\n\n    Returns\n    -------\n    pd.DataFrame\n        Range DataFrame with self.start_column_name and \\\n        self.end_column_name columns. Both start and end dates \\\n        are supposed to be included in the final range.\n    \"\"\"\n    if self.interval == \"custom\":\n        return self._make_custom_range()\n    return self._make_range()\n</code></pre>"},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRange","title":"<code>DateRange(data, start_col, end_col)</code>","text":"<p>Class to hold dateranges dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The actual daterange dataframe.</p> required <code>start_col</code> <code>str</code> <p>Name of the column with starting dates.</p> required <code>end_col</code> <code>str</code> <p>Name of the column with ending dates.</p> required Source code in <code>src/bgc_data_processing/utils/dateranges.py</code> <pre><code>def __init__(self, data: pd.DataFrame, start_col: str, end_col: str) -&gt; None:\n    self._data = data\n    self._start = start_col\n    self._end = end_col\n</code></pre>"},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRange.start_field","title":"<code>start_field: str</code>  <code>property</code>","text":"<p>Satrting date column name.</p>"},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRange.end_field","title":"<code>end_field: str</code>  <code>property</code>","text":"<p>Ending date column name.</p>"},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRange.start_dates","title":"<code>start_dates: pd.Series</code>  <code>property</code>","text":"<p>Starting date column.</p>"},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRange.end_dates","title":"<code>end_dates: pd.Series</code>  <code>property</code>","text":"<p>Ending date column.</p>"},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRange.as_dataframe","title":"<code>as_dataframe()</code>","text":"<p>Return the dateranges as a DataFrame, ie self._data.</p> Source code in <code>src/bgc_data_processing/utils/dateranges.py</code> <pre><code>def as_dataframe(self) -&gt; pd.DataFrame:\n\"\"\"Return the dateranges as a DataFrame, ie self._data.\"\"\"\n    return self._data\n</code></pre>"},{"location":"reference/utils/dateranges/#bgc_data_processing.utils.dateranges.DateRange.as_str","title":"<code>as_str()</code>","text":"<p>Return a Series of dateranges as strings with format: 'YYYYMMDD-YYYYMMDD'.</p> Source code in <code>src/bgc_data_processing/utils/dateranges.py</code> <pre><code>def as_str(self) -&gt; pd.Series:\n\"\"\"Return a Series of dateranges as strings with format: 'YYYYMMDD-YYYYMMDD'.\"\"\"\n    str_start = pd.to_datetime(self.start_dates).dt.strftime(\"%Y%m%d\")\n    str_end = pd.to_datetime(self.end_dates).dt.strftime(\"%Y%m%d\")\n    return str_start + \"-\" + str_end\n</code></pre>"},{"location":"reference/utils/patterns/","title":"<code>bgc_data_processing.utils.patterns</code>","text":"<p>Filenames Patterns building.</p>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.FileNamePattern","title":"<code>FileNamePattern(str_pattern, year_field='years', month_field='months', day_field='days')</code>","text":"<p>Create the pattern to use to select filenames.</p> <p>Parameters:</p> Name Type Description Default <code>str_pattern</code> <code>str</code> <p>Raw string pattern, with field to infer written in brackets ('{'&amp;'}').</p> required <code>year_field</code> <code>str</code> <p>Name of the field supposed to contain the year informations. , by default \"years\"</p> <code>'years'</code> <code>month_field</code> <code>str</code> <p>Name of the field supposed to contain the month informations. , by default \"months\"</p> <code>'months'</code> <code>day_field</code> <code>str</code> <p>Name of the field supposed to contain the day informations. , by default \"days\"</p> <code>'days'</code> Source code in <code>src/bgc_data_processing/utils/patterns.py</code> <pre><code>def __init__(\n    self,\n    str_pattern: str,\n    year_field: str = \"years\",\n    month_field: str = \"months\",\n    day_field: str = \"days\",\n) -&gt; None:\n    self._base = str_pattern\n    self._year = year_field\n    self._month = month_field\n    self._day = day_field\n</code></pre>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.FileNamePattern.build_from_constraint","title":"<code>build_from_constraint(date_constraint)</code>","text":"<p>Build the pattern from date constraints.</p> <p>Parameters:</p> Name Type Description Default <code>date_constraint</code> <code>dict</code> <p>Date constraint.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Final pattern.</p> Source code in <code>src/bgc_data_processing/utils/patterns.py</code> <pre><code>def build_from_constraint(self, date_constraint: dict) -&gt; \"PatternMatcher\":\n\"\"\"Build the pattern from date constraints.\n\n    Parameters\n    ----------\n    date_constraint : dict\n        Date constraint.\n\n    Returns\n    -------\n    str\n        Final pattern.\n    \"\"\"\n    date_min, date_max = self._parse_dates_from_constraints(date_constraint)\n    return self.build(date_min=date_min, date_max=date_max)\n</code></pre>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.FileNamePattern.build","title":"<code>build(date_min, date_max)</code>","text":"<p>Build the pattern from the starting and ending dates.</p> <p>Parameters:</p> Name Type Description Default <code>date_min</code> <code>date | None</code> <p>Starting date.</p> required <code>date_max</code> <code>date | None</code> <p>Ending date.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Final pattern.</p> Source code in <code>src/bgc_data_processing/utils/patterns.py</code> <pre><code>def build(\n    self,\n    date_min: dt.date | None,\n    date_max: dt.date | None,\n) -&gt; \"PatternMatcher\":\n\"\"\"Build the pattern from the starting and ending dates.\n\n    Parameters\n    ----------\n    date_min : dt.date | None\n        Starting date.\n    date_max : dt.date | None\n        Ending date.\n\n    Returns\n    -------\n    str\n        Final pattern.\n    \"\"\"\n    interval_patterns = self._slice(date_min=date_min, date_max=date_max)\n    pattern = \"|\".join(map(self._create_pattern, interval_patterns))\n    return PatternMatcher(pattern=pattern)\n</code></pre>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.DateIntervalPattern","title":"<code>DateIntervalPattern(date_min, date_max, precision='day')</code>","text":"<p>Create the patterns values for a given date interval.</p> <p>Parameters:</p> Name Type Description Default <code>date_min</code> <code>date | None</code> <p>Starting date of the interval.</p> required <code>date_max</code> <code>date | None</code> <p>Ending date of the interval.</p> required <code>precision</code> <code>str</code> <p>Precision of the interval. A \"day\" precision means that year and month are constant, therefore the required days are contained between the minimum day number and the maximum day number. A \"month\" precision means that year is constant but months are different but totally included (from first day to last day.) in the interval., therefore the required months are contained between the minimum months number and the maximum months number. A \"year\" precision means the years to include are different but totally included (from first day to last day.) in the interval, therefore the required years are contained between the minimum year number and the maximum year number., by default \"day\"</p> <code>'day'</code> Source code in <code>src/bgc_data_processing/utils/patterns.py</code> <pre><code>def __init__(\n    self,\n    date_min: dt.date | None,\n    date_max: dt.date | None,\n    precision: str = \"day\",\n) -&gt; None:\n    if not (date_min is None and date_max is None):\n        self._validate_inputs(\n            date_min=date_min,\n            date_max=date_max,\n            precision=precision,\n        )\n\n    self._precision = precision\n    self._min = date_min\n    self._max = date_max\n</code></pre>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.DateIntervalPattern.years","title":"<code>years: str</code>  <code>property</code>","text":"<p>'year' part of the pattern.</p>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.DateIntervalPattern.months","title":"<code>months: str</code>  <code>property</code>","text":"<p>'month' part of the pattern.</p>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.DateIntervalPattern.days","title":"<code>days: str</code>  <code>property</code>","text":"<p>'day' part of the pattern.</p>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.DateIntervalPattern.__repr__","title":"<code>__repr__()</code>","text":"<p>Object representation string.</p> <p>Returns:</p> Type Description <code>str</code> <p>Starting date - Ending date ; precision: interval precision.</p> Source code in <code>src/bgc_data_processing/utils/patterns.py</code> <pre><code>def __repr__(self) -&gt; str:\n\"\"\"Object representation string.\n\n    Returns\n    -------\n    str\n        Starting date - Ending date ; precision: interval precision.\n    \"\"\"\n    return f\"{self._min} - {self._max} ; precision: {self._precision}\"\n</code></pre>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.DateIntervalPattern.with_day_precision","title":"<code>with_day_precision(date_min, date_max)</code>  <code>classmethod</code>","text":"<p>Create a DateIntervalPattern with day precision.</p> <p>Parameters:</p> Name Type Description Default <code>date_min</code> <code>date</code> <p>Starting date.</p> required <code>date_max</code> <code>date</code> <p>Ending date.</p> required <p>Returns:</p> Type Description <code>DateIntervalPattern</code> <p>Date interval pattern.</p> Source code in <code>src/bgc_data_processing/utils/patterns.py</code> <pre><code>@classmethod\ndef with_day_precision(\n    cls,\n    date_min: dt.date,\n    date_max: dt.date,\n) -&gt; \"DateIntervalPattern\":\n\"\"\"Create a DateIntervalPattern with day precision.\n\n    Parameters\n    ----------\n    date_min : dt.date\n        Starting date.\n    date_max : dt.date\n        Ending date.\n\n    Returns\n    -------\n    DateIntervalPattern\n        Date interval pattern.\n    \"\"\"\n    return cls(\n        date_min=date_min,\n        date_max=date_max,\n        precision=cls._day_precision_label,\n    )\n</code></pre>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.DateIntervalPattern.with_month_precision","title":"<code>with_month_precision(date_min, date_max)</code>  <code>classmethod</code>","text":"<p>Create a DateIntervalPattern with month precision.</p> <p>Parameters:</p> Name Type Description Default <code>date_min</code> <code>date</code> <p>Starting date.</p> required <code>date_max</code> <code>date</code> <p>Ending date.</p> required <p>Returns:</p> Type Description <code>DateIntervalPattern</code> <p>Date interval pattern.</p> Source code in <code>src/bgc_data_processing/utils/patterns.py</code> <pre><code>@classmethod\ndef with_month_precision(\n    cls,\n    date_min: dt.date,\n    date_max: dt.date,\n) -&gt; \"DateIntervalPattern\":\n\"\"\"Create a DateIntervalPattern with month precision.\n\n    Parameters\n    ----------\n    date_min : dt.date\n        Starting date.\n    date_max : dt.date\n        Ending date.\n\n    Returns\n    -------\n    DateIntervalPattern\n        Date interval pattern.\n    \"\"\"\n    return cls(\n        date_min=date_min,\n        date_max=date_max,\n        precision=cls._month_precision_label,\n    )\n</code></pre>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.DateIntervalPattern.with_year_precision","title":"<code>with_year_precision(date_min, date_max)</code>  <code>classmethod</code>","text":"<p>Create a DateIntervalPattern with year precision.</p> <p>Parameters:</p> Name Type Description Default <code>date_min</code> <code>date</code> <p>Starting date.</p> required <code>date_max</code> <code>date</code> <p>Ending date.</p> required <p>Returns:</p> Type Description <code>DateIntervalPattern</code> <p>Date interval pattern.</p> Source code in <code>src/bgc_data_processing/utils/patterns.py</code> <pre><code>@classmethod\ndef with_year_precision(\n    cls,\n    date_min: dt.date,\n    date_max: dt.date,\n) -&gt; \"DateIntervalPattern\":\n\"\"\"Create a DateIntervalPattern with year precision.\n\n    Parameters\n    ----------\n    date_min : dt.date\n        Starting date.\n    date_max : dt.date\n        Ending date.\n\n    Returns\n    -------\n    DateIntervalPattern\n        Date interval pattern.\n    \"\"\"\n    return cls(\n        date_min=date_min,\n        date_max=date_max,\n        precision=cls._year_precision_label,\n    )\n</code></pre>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.PatternMatcher","title":"<code>PatternMatcher(pattern, validation_function=lambda : True)</code>","text":"<p>Matcher to find files which match agiven pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Pattern to match.</p> required <code>validation_function</code> <code>Callable</code> <p>Function to use to validate that files are to be loaded., by default lambdafilepath</p> <code>lambda : True</code> Source code in <code>src/bgc_data_processing/utils/patterns.py</code> <pre><code>def __init__(\n    self,\n    pattern: str,\n    validation_function: Callable = lambda filepath: True,  # noqa: ARG005\n) -&gt; None:\n    self._pattern = pattern\n    self.validate = validation_function\n</code></pre>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.PatternMatcher.validate","title":"<code>validate: Callable</code>  <code>property</code> <code>writable</code>","text":"<p>Validation function.</p>"},{"location":"reference/utils/patterns/#bgc_data_processing.utils.patterns.PatternMatcher.select_matching_filepath","title":"<code>select_matching_filepath(research_directory)</code>","text":"<p>Select the filepaths matching the pattern.</p> <p>Parameters:</p> Name Type Description Default <code>research_directory</code> <code>Path | str</code> <p>Directory to serach for files.</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of correct paths.</p> Source code in <code>src/bgc_data_processing/utils/patterns.py</code> <pre><code>def select_matching_filepath(\n    self,\n    research_directory: Path | str,\n) -&gt; list[Path]:\n\"\"\"Select the filepaths matching the pattern.\n\n    Parameters\n    ----------\n    research_directory : Path | str\n        Directory to serach for files.\n\n    Returns\n    -------\n    list[Path]\n        List of correct paths.\n    \"\"\"\n    return self._recursive_match(\n        research_dir=Path(research_directory),\n        pattern=self._pattern,\n    )\n</code></pre>"},{"location":"scripts/","title":"Python Scripts","text":"<p>The following sections all relate to the python scripts defined here.</p>"},{"location":"scripts/#loading-data","title":"Loading Data","text":"How to load and save new data? <p>Command: <code>make run-save-data</code></p> <p>More informations here: Data Saving script</p> <p>(Source)</p>"},{"location":"scripts/#data-extraction","title":"Data Extraction","text":"How to extract data from a given geographic area ? <p>Command: <code>make run-extract-data</code></p> <p>More informations here: Data Extraction script</p> <p>(Source)</p> How to extract data from a given water mass ? <p>Command: <code>make run-extract-water-mass</code></p> <p>More informations here: Data Extraction script</p> <p>(Source)</p>"},{"location":"scripts/#overall-data-plotting","title":"Overall Data Plotting","text":"How to plot an interactive map of the data? <p>Command: <code>make run-plot-interactive</code></p> <p>More informations here: Interactive Map Script</p> <p>(Source)</p> How to show the data density on a map? <p>Command: <code>make run-plot-data-density</code></p> <p>More informations here: Mesh Plotting Script</p> <p>(Source)</p> How to show a profile with data density evolution? <p>Command: <code>make run-plot-profile</code></p> <p>More informations here: Profile Plotting Script</p> <p>(Source)</p>"},{"location":"scripts/#water-mass-related-plotting","title":"Water Mass Related Plotting","text":"How to show the Temperature-Salinity diagram of a water mass ? <p>Command: <code>make run-plot-ts-diagram</code></p> <p>More informations here: Temperature-Salinity Diagram Script</p> <p>(Source)</p> How to show the Boxplots of different water masses for a given variable? <p>Command: <code>make run-plot-var-boxplot</code></p> <p>More informations here: Variable Boxplot Plotting Script</p> <p>(Source)</p> How to show the Histograms of different water masses for a given variable? <p>Command: <code>make run-plot-var-histogram</code></p> <p>More informations here: Variable Histogram Plotting Script</p> <p>(Source)</p> How to compare a variable to pressure for given water masses? <p>Command: <code>make run-plot-var-pressure</code></p> <p>More informations here: Variable vs Pressure Plotting Script</p> <p>(Source)</p>"},{"location":"scripts/#compare-simulations-to-observations","title":"Compare Simulations to Observations","text":"How to compare simulations to observations? <p>Command: <code>make run-compare-sims-obs</code></p> <p>More informations here: Simulation-Observation Comparison Script</p> <p>(Source)</p>"},{"location":"scripts/compare_sims_obs/","title":"Simulations-Observations Comparison","text":"<p><code>make run-compare-sims-obs</code></p>"},{"location":"scripts/compare_sims_obs/#summary","title":"Summary","text":"<p>This scripts loads observation data over given periode and area. It then loads the closest simulation data to these obersvation and saves both value in files. Afterward, those dataframes are compared through RMSE and Bias computation. The comparison output is saved into a file.</p>"},{"location":"scripts/compare_sims_obs/#configuration","title":"Configuration","text":"<p>The configuration file for this script is <code>config/compare_sims_obs.toml</code> (based on <code>config/default/compare_sims_obs.toml</code>). All the parameters and their functionality are listed below:</p>"},{"location":"scripts/compare_sims_obs/#inputoutput","title":"Input/Output","text":"LOADING_DIR <p>Directory from which to load data.</p> <p>default: <code>\"bgc_data\"</code></p> <p>Expected type: <code>str</code></p> SAVING_DIR <p>Directory in which to save Observations and Simulations corresponding DataFrames.</p> <p>default: <code>\"output\"</code></p> <p>Expected type: <code>str</code></p> SIM_PROVIDER <p>Name of the simualted data provider.</p> <p>default: <code>\"HYCOM\"</code></p> <p>Expected type: <code>str</code></p> RESULT_OUTPUT_FILE <p>Output filepath. In this file will be saved the result of the comparison metrics.</p> <p>default: <code>\"comparison_output.txt\"</code></p> <p>Expected type:  <code>str</code></p>"},{"location":"scripts/compare_sims_obs/#interpolation-parameters","title":"Interpolation Parameters","text":"TO_INTERPOLATE <p>Names of the variables to interpolate. The names are supposed to be the ones defined in <code>config/variables.toml</code> (<code>config/default/variables.toml</code>) by default.)</p> <p>default: <code>[\"TEMP\", \"PSAL\", \"DOXY\", \"PHOS\", \"NTRA\", \"SLCA\", \"CPHL\"]</code></p> <p>Expected type: <code>list[str]</code></p> INTERPOLATION_KIND <p>Kind of interpolation for scipy.interpolate.interp1d.</p> <p>default: <code>\"linear\"</code></p> <p>Expected type: <code>str</code></p>"},{"location":"scripts/compare_sims_obs/#comparison-parameters","title":"Comparison Parameters","text":"VARIABLES_TO_COMPARE <p>Names of the variables to compare.</p> <p>default: <code>[\"PTEMP\", \"DOXY\", \"PSAL\", \"NTRA\", \"SLCA\", \"PHOS\", \"CPHL\"</code></p> <p>Expected type:  <code>list[str]</code></p>"},{"location":"scripts/compare_sims_obs/#data-selection","title":"Data Selection","text":"DATE_MIN <p>Beginning of the data to load (included).</p> <p>default: <code>\"20150101\"</code></p> <p>Expected type: <code>str</code> (respecting the <code>YYYYMMDD</code> format)</p> DATE_MAX <p>End of the data to load (included).</p> <p>default: <code>\"20151231\"</code></p> <p>Expected type: <code>str</code> (respecting the <code>YYYYMMDD</code> format)</p> LATITUDE_MIN <p>Minimum latitude boundary for the loaded data (included).</p> <p>default: <code>50</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LATITUDE_MAX <p>Maximum latitude boundary for the loaded data (included).</p> <p>default: <code>90</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LONGITUDE_MIN <p>Minimum longitude boundary for the loaded data (included).</p> <p>default: <code>-180</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LONGITUDE_MAX <p>Maximum longitude boundary for the loaded data (included).</p> <p>default: <code>180</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> DEPTH_MIN <p>Minimum depth boundary for the loaded data (included).</p> <p>default: <code>nan</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> DEPTH_MAX <p>Maximum depth boundary for the loaded data (included).</p> <p>default: <code>0</code></p> <p>Expected type: <code>int</code> or <code>float</code></p>"},{"location":"scripts/compare_sims_obs/#others","title":"Others","text":"SHOW_MAP <p>Whether to show the map of all compared points or not</p> <p>default: <code>false</code></p> <p>Expected type:  <code>bool</code></p> VERBOSE <p>Verbose value, the higher, the more informations. If set to 0 or below: no information displayed. If set to 1: minimal informations displayed. If set to 2: very complete informations displayed. If set to 3 or higher: exhaustive informations displayed.</p> <p>default: <code>3</code></p> <p>Expected type: <code>int</code></p>"},{"location":"scripts/compare_sims_obs/#script-output","title":"Script Output","text":"<p>When executed, the script will finally output a file named <code>RESULT_OUTPUT_FILE</code>.</p> <p>This is an example of what this file could look like (values are only for illustration purpose and do not reprersent anything):</p> <p></p> <p>If <code>SHOW_MAP</code> is set to <code>true</code>, a visualisation of the matched point is also displayed.</p> <p>This is an example of what this image could look like:</p> <p></p> <p>Source code: </p>"},{"location":"scripts/extract_data/","title":"Data Extraction","text":"<p><code>make run-extract-data</code></p>"},{"location":"scripts/extract_data/#summary","title":"Summary","text":"<p>This script extract data from an already loaded dataset and saves it into a given folder.</p>"},{"location":"scripts/extract_data/#configuration","title":"Configuration","text":"<p>The configuration file for this script is <code>config/extract_data.toml</code> (based on <code>config/default/extract_data.toml</code>). All the parameters and their functionality are listed below:</p>"},{"location":"scripts/extract_data/#inputoutput","title":"Input/Output","text":"LOADING_DIR <p>Directory from which to load data.</p> <p>default: <code>\"bgc_data\"</code></p> <p>Expected type: <code>str</code></p> SAVING_DIR <p>Directory in which to save extracted data.</p> <p>default: <code>\"output\"</code></p> <p>Expected type: <code>str</code></p>"},{"location":"scripts/extract_data/#data-selection","title":"Data Selection","text":"FROM_POLYGON <p>If true, the extracted data is the data within POLYGON_DOMAIN, otherwise, the data within the [LONGITUDE_MIN, LONGITUDE_MAX] and [LATITUDE_MIN, LATITUDE_MAX] boundaries.</p> <p>default: <code>false</code></p> <p>Expected type: <code>bool</code></p> DATE_MIN <p>Beginning of the data to load (included).</p> <p>default: <code>\"20070101\"</code></p> <p>Expected type: <code>str</code> (respecting the <code>YYYYMMDD</code> format)</p> DATE_MAX <p>End of the data to load (included).</p> <p>default: <code>\"20071231\"</code></p> <p>Expected type: <code>str</code> (respecting the <code>YYYYMMDD</code> format)</p> LATITUDE_MIN <p>Minimum latitude boundary for the loaded data (included). Only used if <code>FROM_POLYGON</code> is set to <code>false</code>.</p> <p>default: <code>60</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LATITUDE_MAX <p>Maximum latitude boundary for the loaded data (included). Only used if <code>FROM_POLYGON</code> is set to <code>false</code>.</p> <p>default: <code>80</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LONGITUDE_MIN <p>Minimum longitude boundary for the loaded data (included). Only used if <code>FROM_POLYGON</code> is set to <code>false</code>.</p> <p>default: <code>-50</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LONGITUDE_MAX <p>Maximum longitude boundary for the loaded data (included).    Only used if <code>FROM_POLYGON</code> is set to <code>false</code>.</p> <p>default: <code>50</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> POLYGON_DOMAIN <p>Polygon to use for data extraction. Only used if <code>FROM_POLYGON</code> is set to <code>true</code>.</p> <p>default: <code>\"POLYGON((-50 60, 50 60, 50 80, -50 80, -50 60))\"</code></p> PRIORITY <p>Providers priority list to use when removing duplicates.</p> <p>default: <code>[\"GLODAP_2022\", \"CMEMS\", \"ARGO\", \"NMDC\", \"CLIVAR\", \"IMR\", \"ICES\"]</code></p> <p>Expected type: <code>list[str]</code></p>"},{"location":"scripts/extract_data/#others","title":"Others","text":"VERBOSE <p>Verbose value, the higher, the more informations. If set to 0 or below: no information displayed. If set to 1: minimal informations displayed. If set to 2: very complete informations displayed. If set to 3 or higher: exhaustive informations displayed.</p> <p>default: <code>2</code></p> <p>Expected type: <code>int</code></p>"},{"location":"scripts/extract_data/#script-output","title":"Script Output","text":"<p>When executed, this script will create a file in the <code>SAVING_DIR</code> folder with the extracted data.</p> <p>This is an example of what this data could look like:</p> <p></p> <p>Source code: </p>"},{"location":"scripts/extract_water_mass/","title":"Water Mass Extraction","text":"<p><code>make run-extract-water-mass</code></p>"},{"location":"scripts/extract_water_mass/#summary","title":"Summary","text":"<p>This script extracts data from a given watermass and saves it into a given folder.</p>"},{"location":"scripts/extract_water_mass/#configuration","title":"Configuration","text":"<p>The configuration file for this script is <code>config/extract_water_mass.toml</code> (based on <code>config/default/extract_water_mass.toml</code>). All the parameters and their functionality are listed below:</p>"},{"location":"scripts/extract_water_mass/#inputoutput","title":"Input/Output","text":"LOADING_DIR <p>Directory from which to load data.</p> <p>default: <code>\"bgc_data\"</code></p> <p>Expected type: <code>str</code></p> SAVING_DIR <p>Directory in which to save extracted data.</p> <p>default: <code>\"extracted_watermass\"</code></p> <p>Expected type: <code>str</code></p>"},{"location":"scripts/extract_water_mass/#data-selection","title":"Data Selection","text":"WATER_MASS_ACRONYM <p>Acronym of the water mass to extract. The water mass acronyms are the acronyms defined in config/water_masses.toml (based on config/default/water_masses.toml). The acronym is the value defined under the <code>ACRONYM</code> parameter.</p> <p>default: <code>ALL</code></p> <p>Expected type: <code>str</code></p> DATE_MIN <p>Beginning of the data to load (included).</p> <p>default: <code>\"20070101\"</code></p> <p>Expected type: <code>str</code> (respecting the <code>YYYYMMDD</code> format)</p> DATE_MAX <p>End of the data to load (included).</p> <p>default: <code>\"20071231\"</code></p> <p>Expected type: <code>str</code> (respecting the <code>YYYYMMDD</code> format)</p> LATITUDE_MIN <p>Minimum latitude boundary for the loaded data (included).</p> <p>default: <code>60</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LATITUDE_MAX <p>Maximum latitude boundary for the loaded data (included).</p> <p>default: <code>80</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LONGITUDE_MIN <p>Minimum longitude boundary for the loaded data (included).</p> <p>default: <code>-50</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LONGITUDE_MAX <p>Maximum longitude boundary for the loaded data (included).    Only used if <code>FROM_POLYGON</code> is set to <code>false</code>.</p> <p>default: <code>50</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> PRIORITY <p>Providers priority list to use when removing duplicates.</p> <p>default: <code>[\"GLODAP_2022\", \"CMEMS\", \"ARGO\", \"NMDC\", \"CLIVAR\", \"IMR\", \"ICES\"]</code></p> <p>Expected type: <code>list[str]</code></p>"},{"location":"scripts/extract_water_mass/#others","title":"Others","text":"VERBOSE <p>Verbose value, the higher, the more informations. If set to 0 or below: no information displayed. If set to 1: minimal informations displayed. If set to 2: very complete informations displayed. If set to 3 or higher: exhaustive informations displayed.</p> <p>default: <code>2</code></p> <p>Expected type: <code>int</code></p>"},{"location":"scripts/extract_water_mass/#script-output","title":"Script Output","text":"<p>When executed, this script will create a file in the <code>SAVING_DIR</code> folder with the extracted data.</p> <p>This is an example of what this data could look like:</p> <p></p> <p>Source code: </p>"},{"location":"scripts/plot_data_density/","title":"Density Plotting","text":"<p><code>make run-plot-data-density</code></p>"},{"location":"scripts/plot_data_density/#summary","title":"Summary","text":"<p>This scripts reads data from a folder and displays the data density on a map.</p>"},{"location":"scripts/plot_data_density/#configuration","title":"Configuration","text":"<p>The configuration file for this script is <code>config/plot_data_density.toml</code> (based on <code>config/default_plot_data_density.toml</code>). All the parameters and their functionality are listed below:</p>"},{"location":"scripts/plot_data_density/#inputoutput","title":"Input/output","text":"LOADING_DIR <p>Directory from which to load data.</p> <p>default: <code>\"bgc_data\"</code></p> <p>Expected type: <code>str</code></p> SAVE <p>Whether to save the figure or not.</p> <p>default: <code>true</code></p> <p>Expected type: <code>bool</code></p> SAVING_DIR <p>Directory in which to save the figure.</p> <p>default: <code>\"bgc_figs\"</code></p> <p>Expected type: <code>str</code></p> SHOW <p>Whether to show the figure or not.</p> <p>default: <code>true</code></p> <p>Expected type: <code>bool</code></p> VARIABLE <p>Name of the variable to map. The names are supposed to be the ones defined in <code>config/variables.toml</code> (<code>config/default/variables.toml</code>) by default.). If 'all': will map density of datapoints, regardless of their variables.</p> <p>default: <code>\"all\"</code></p> <p>Expected type: <code>str</code></p>"},{"location":"scripts/plot_data_density/#data-selection","title":"Data Selection","text":"DATE_MIN <p>First date to map (included).</p> <p>default: <code>\"20070101\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code> format)</p> DATE_MAX <p>Last date to map (included).</p> <p>default: <code>\"20201231\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code> format)</p> LATITUDE_MIN <p>Minimum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>50</code></p> <p>Expected type: <code>int or float</code></p> LATITUDE_MAX <p>Maximum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>90</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MIN <p>Minimum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>-180</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MAX <p>Maximum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>180</code></p> <p>Expected type: <code>int or float</code></p> LATITUDE_MAP_MIN <p>Minimum latitude boundary displayed on the map (included). If set to nan, the map boundaries will match the extremum of the dataframe.</p> <p>default: <code>nan</code></p> <p>Expected type: <code>int or float</code></p> DEPTH_MIN <p>Minimum depth boundary to consider for the loaded data (included).</p> <p>default: <code>nan</code></p> <p>Expected type: <code>int or float</code></p> DEPTH_MAX <p>Maximum depth boundary to consider for the loaded data (included).</p> <p>default: <code>0</code></p> <p>Expected type: <code>int or float</code></p> EXPOCODES_TO_LOAD <p>Precise expocode to load alone. If empty, no discrimination on expocode will be conducted.</p> <p>default: <code>[]</code></p> <p>Expected type: <code>list[str]</code></p> PRIORITY <p>Providers priority list to use when removing duplicates. Every provider takes priority over the following ones.</p> <p>default: <code>[\"GLODAP_2022\", \"CMEMS\", \"ARGO\", \"NMDC\", \"CLIVAR\", \"IMR\", \"ICES\"]</code></p> <p>Expected type: <code>list[str]</code></p>"},{"location":"scripts/plot_data_density/#mapping-options","title":"Mapping Options","text":"LATITUDE_MAP_MAX <p>Maximum latitude boundary displayed on the map (included). If set to nan, the map boundaries will match the extremum of the dataframe.</p> <p>default: <code>nan</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MAP_MIN <p>Minimum longitude boundary displayed on the map (included). If set to nan, the map boundaries will match the extremum of the dataframe.</p> <p>default: <code>nan</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MAP_MAX <p>Maximum longitude boundary displayed on the map (included). If set to nan, the map boundaries will match the extremum of the dataframe.</p> <p>default: <code>nan</code></p> <p>Expected type: <code>int or float</code></p> BIN_SIZE <p>Bins size. If list, first component is latitude size, second is longitude size. If int or float, represents both latitude and longitude size.</p> <p>default: <code>[0.5, 1.5]</code></p> <p>Expected type: <code>list[float] or list[int] or float or int</code></p> CONSIDER_DEPTH <p>If <code>true</code>: the plotted density will consider all data points (even the ones in the water column). If <code>false</code>: the plotted density will only consider one data point per location and date.</p> <p>default: <code>false</code></p> <p>Expected type: <code>bool</code></p>"},{"location":"scripts/plot_data_density/#others","title":"Others","text":"VERBOSE <p>Verbose value, the higher, the more informations. If set to 0 or below: no information displayed. If set to 1: minimal informations displayed. If set to 2: very complete informations displayed. If set to 3 or higher: exhaustive informations displayed.</p> <p>default: <code>2</code></p> <p>Expected type: <code>int</code></p>"},{"location":"scripts/plot_data_density/#script-output","title":"Script Output","text":"<p>When executed, this script display a map with the density of the data points (regrouped in bins).</p> <p>This is an example of what this map could look like:</p> <p></p> <p>Source code: </p>"},{"location":"scripts/plot_interactive/","title":"Interactive Map","text":"<p><code>make run-plot_interactive</code></p>"},{"location":"scripts/plot_interactive/#summary","title":"Summary","text":"<p>This scripts shows an interactive map on which it is possible to draw polygon in order to view data density and profile on a given area.</p> <p>The following keys are used to interact with this map:</p> <ul> <li>D to start drawing a polygon, then:<ul> <li>Left Click to draw polygon (keep the button clicked).</li> <li>Middle Click to close the polygon shape.</li> </ul> </li> <li>Enter key to confirm the drawn polygon as area to zoom in.</li> <li>Z key to remove the polygon.</li> <li>S to save the data within the polygon (then you must enter a filename in the terminal).</li> <li>P to save the polygon (then you must enter a filename in the terminal).</li> <li>L to load a polygon (then you must enter the name of the file to load).</li> </ul>"},{"location":"scripts/plot_interactive/#configuration","title":"Configuration","text":"<p>The configuration file for this script is <code>config/plot_interactive.toml</code> (based on <code>config/default_plot_interactive.toml</code>). All the parameters and their functionality are listed below:</p>"},{"location":"scripts/plot_interactive/#inputoutput","title":"Input/Output","text":"LOADING_DIR <p>Directory from which to load the data.</p> <p>default: <code>\"bgc_data\"</code></p> <p>Expected type: <code>str</code></p> POLYGONS_FOLDER <p>Directory from which to load already existing polygons and in which to save the drawn polygons.</p> <p>default: <code>\"polygons\"</code></p> <p>Expected type: <code>str</code></p>"},{"location":"scripts/plot_interactive/#data-selection","title":"Data Selection","text":"VARIABLE <p>Name of the variable to map. The names are supposed to be the ones defined in <code>config/variables.toml</code> (<code>config/default/variables.toml</code>) by default.). If 'all': will map density of datapoints, regardless of their variables.</p> <p>default: <code>\"all\"</code></p> <p>Expected type: <code>str</code></p> DATE_MIN <p>First date to map (included).</p> <p>default: <code>\"20070101\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code> format)</p> DATE_MAX <p>Last date to map (included).</p> <p>default: <code>\"20071231\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code> format)</p> LATITUDE_MIN <p>Minimum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>50</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> LATITUDE_MAX <p>Maximum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>90</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> LONGITUDE_MIN <p>Minimum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>-180</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> LONGITUDE_MAX <p>Maximum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>180</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> DEPTH_MIN <p>Minimum depth boundary to consider for the loaded data (included).</p> <p>default: <code>nan</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> DEPTH_MAX <p>Maximum depth boundary to consider for the loaded data (included)</p> <p>default: <code>0</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> EXPOCODES_TO_LOAD <p>Precise expocode to load alone. If empty, no discrimination on expocode will be conducted.</p> <p>default: <code>[]</code></p> <p>Expected type: <code>list[str]</code></p> PRIORITY <p>Providers priority list to use when removing duplicates</p> <p>default: <code>[\"GLODAP_2022\", \"CMEMS\", \"ARGO\", \"NMDC\", \"CLIVAR\", \"IMR\", \"ICES\"]</code></p> <p>Expected type: <code>list[str]</code></p>"},{"location":"scripts/plot_interactive/#mapping-options","title":"Mapping Options","text":"LATITUDE_MAP_MIN <p>Minimum latitude boundary displayed on the map (included). If set to nan, the map boundaries will match the extremum of the dataframe</p> <p>default: <code>50</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> LATITUDE_MAP_MAX <p>Maximum latitude boundary displayed on the map (included). If set to nan, the map boundaries will match the extremum of the dataframe</p> <p>default: <code>90</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> LONGITUDE_MAP_MIN <p>Minimum longitude boundary displayed on the map (included). If set to nan, the map boundaries will match the extremum of the dataframe</p> <p>default: <code>-180</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> LONGITUDE_MAP_MAX <p>Maximum longitude boundary displayed on the map (included). If set to nan, the map boundaries will match the extremum of the dataframe</p> <p>default: <code>180</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> BIN_SIZE <p>Bins size. If list, first component is latitude size, second is longitude size. If int or float, represents both latitude and longitude size.</p> <p>default: <code>[0.5, 1.5]</code></p> <p>Expected type: <code>list[float]</code> or <code>list[int]</code> or <code>float</code> or <code>int</code></p> CONSIDER_DEPTH <p>If <code>true</code>: the plotted density will consider all data points (even the ones in the water column). If <code>false</code>: the plotted density will only consider one data point per location and date</p> <p>default: <code>true</code></p> <p>Expected type: <code>bool</code></p>"},{"location":"scripts/plot_interactive/#script-output","title":"Script Output","text":"<p>When executed, this script will display an interactive map with three panels:</p> <ol> <li>The general map of the loaded data, on the left panel.</li> <li>A zoomed view of any selected data, on the top right panel.</li> <li>A depth vs date density view of any selected data, on the bottom right panel.</li> </ol> <p>This is an example of what the displayed figure could look like:</p> <p></p> <p>Source code: </p>"},{"location":"scripts/plot_profile/","title":"Profile Density","text":"<p><code>make run-plot-profile</code></p>"},{"location":"scripts/plot_profile/#summary","title":"Summary","text":"<p>This scripts reads data from a folder and plot the density profile (depth as vertical axis and date as horizontal) over a given area.</p>"},{"location":"scripts/plot_profile/#configuration","title":"Configuration","text":"<p>The configuration file for this script is <code>config/plot_profile.toml</code> (based on <code>config/default_plot_profile.toml</code>). All the parameters and their functionality are listed below:</p>"},{"location":"scripts/plot_profile/#inputoutput","title":"Input/Output","text":"LOADING_DIR <p>Directory from which to load data.</p> <p>default: <code>\"bgc_data\"</code></p> <p>Expected type: <code>str</code></p> SAVE <p>Whether to save the figure or not.</p> <p>default: <code>true</code></p> <p>Expected type: <code>bool</code></p> SAVING_DIR <p>Directory in which to save the figure.</p> <p>default: <code>\"bgc_figs\"</code></p> <p>Expected type: <code>str</code></p> SHOW <p>Whether to show the figure or not.</p> <p>default: <code>true</code></p> <p>Expected type: <code>bool</code></p>"},{"location":"scripts/plot_profile/#data-selection","title":"Data Selection","text":"VARIABLE <p>Name of the variable to map. The names are supposed to be the ones defined in <code>config/variables.toml</code> (<code>config/default/variables.toml</code>) by default.). If 'all': will map density of datapoints, regardless of their variables.</p> <p>default: <code>\"NTRA\"</code></p> <p>Expected type: <code>str</code></p> DATE_MIN <p>First date to map (included).</p> <p>default: <code>\"20070101\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code> format)</p> DATE_MAX <p>Last date to map (included).</p> <p>default: <code>\"20201231\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code> format)</p> LATITUDE_MIN <p>Minimum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>50</code></p> <p>Expected type: <code>int or float</code></p> LATITUDE_MAX <p>Maximum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>90</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MIN <p>Minimum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>-180</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MAX <p>Maximum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>180</code></p> <p>Expected type: <code>int or float</code></p> DEPTH_MIN <p>Minimum depth boundary to consider for the loaded data (included), 'nan' indicate not boundary.</p> <p>default: <code>nan</code></p> <p>Expected type: <code>int or float</code></p> DEPTH_MAX <p>Maximum depth boundary to consider for the loaded data (included), 'nan' indicate not boundary.</p> <p>default: <code>0</code></p> <p>Expected type: <code>int or float</code></p> EXPOCODES_TO_LOAD <p>Precise expocode to load alone. If empty, no discrimination on expocode will be conducted.</p> <p>default: <code>[]</code></p> <p>Expected type: <code>list[str]</code></p> PRIORITY <p>Providers priority list to use when removing duplicates.</p> <p>default: <code>[\"GLODAP_2022\", \"CMEMS\", \"ARGO\", \"NMDC\", \"CLIVAR\", \"IMR\", \"ICES\"]</code></p> <p>Expected type: <code>list[str]</code></p>"},{"location":"scripts/plot_profile/#plotting-options","title":"Plotting Options","text":"INTERVAL <p>Horizontal resolution of the plot. If set to <code>day</code>: will group datapoint by day. If set to <code>week</code>: will group datapoints by their week number. If set to <code>month</code>: will group datapoints by month. If set to <code>year</code>: will grou datapoints by year. If set to <code>custom</code>: will group datapoints based on a custom interval.</p> <p>default: <code>\"month\"</code></p> <p>Expected type: <code>str</code></p> CUSTOM_INTERVAL <p>If interval is 'custom', length of the custom interval (in days).</p> <p>default: <code>8</code></p> <p>Expected type: <code>int</code></p> DEPTH_INTERVAL <p>Vertical resolution of the figure. If of type int: vertical axis will be divided in equally sized bins of size depth_interval. If of type list[int]: vertical axis will be divided according to the given levels (levls value ar supposed to negative).</p> <p>default: <code>10</code></p> <p>Expected type: <code>int or list[int]</code></p>"},{"location":"scripts/plot_profile/#others","title":"Others","text":"VERBOSE <p>Verbose value, the higher, the more informations. If set to 0 or below: no information displayed. If set to 1: minimal informations displayed. If set to 2: very complete informations displayed. If set to 3 or higher: exhaustive informations displayed.</p> <p>default: <code>2</code></p> <p>Expected type: <code>int</code></p>"},{"location":"scripts/plot_profile/#script-output","title":"Script Output","text":"<p>When executed, this script displays a density profile plot. This plot shows the depth vs date density of the data.</p> <p>This is an example of what this plot could look like:</p> <p></p> <p>Source code: </p>"},{"location":"scripts/plot_ts_diagram/","title":"Temperature-Salinity Diagram","text":"<p><code>make run-plot-ts-diagram</code></p>"},{"location":"scripts/plot_ts_diagram/#summary","title":"Summary","text":"<p>This scripts reads data from a folder and plot the Temperature-Salinity Diagram for the loaded data.</p>"},{"location":"scripts/plot_ts_diagram/#configuration","title":"Configuration","text":"<p>The configuration file for this script is <code>config/plot_ts_diagram.toml</code> (based on <code>config/default_plot_ts_diagram.toml</code>). All the parameters and their functionality are listed below:</p>"},{"location":"scripts/plot_ts_diagram/#inputoutput","title":"Input/Output","text":"LOADING_DIR <p>Directory from which to load data.</p> <p>default: <code>\"bgc_data\"</code></p> <p>Expected type: <code>str</code></p> SHOW <p>Whether to show the figure or not.</p> <p>default: <code>true</code></p> <p>Expected type: <code>bool</code></p> SAVE <p>Whether to save the figure or not.</p> <p>default: <code>false</code></p> <p>Expected type: <code>bool</code></p> SAVING_DIR <p>Directory in which to save the figure.</p> <p>default: <code>\"bgc_figs\"</code></p> <p>Expected type: <code>str</code></p>"},{"location":"scripts/plot_ts_diagram/#data-selection","title":"Data Selection","text":"DATE_MIN <p>First date to map (included).</p> <p>default: <code>\"20070101\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code> format)</p> DATE_MAX <p>Last date to map (included).</p> <p>default: <code>\"20070331\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code> format)</p> LATITUDE_MIN <p>Minimum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>69</code></p> <p>Expected type: <code>int or float</code></p> LATITUDE_MAX <p>Maximum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>76</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MIN <p>Minimum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>0</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MAX <p>Maximum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>40</code></p> <p>Expected type: <code>int or float</code></p> DEPTH_MIN <p>Minimum depth boundary to consider for the loaded data (included).</p> <p>default: <code>nan</code></p> <p>Expected type: <code>int or float</code></p> DEPTH_MAX <p>Maximum depth boundary to consider for the loaded data (included).</p> <p>default: <code>0</code></p> <p>Expected type: <code>int or float</code></p> EXPOCODES_TO_LOAD <p>Precise expocode to load alone. If empty, no discrimination on expocode will be conducted.</p> <p>default: <code>[]</code></p> <p>Expected type: <code>list[str]</code></p> PRIORITY <p>Providers priority list to use when removing duplicates.</p> <p>default: <code>[\"GLODAP_2022\", \"CMEMS\", \"ARGO\", \"NMDC\", \"CLIVAR\", \"IMR\", \"ICES\"]</code></p> <p>Expected type: <code>list[str]</code></p>"},{"location":"scripts/plot_ts_diagram/#others","title":"Others","text":"VERBOSE <p>Verbose value, the higher, the more informations. If set to 0 or below: no information displayed. If set to 1: minimal informations displayed. If set to 2: very complete informations displayed. If set to 3 or higher: exhaustive informations displayed.</p> <p>default: <code>2</code></p> <p>Expected type: <code>int</code></p>"},{"location":"scripts/plot_ts_diagram/#script-output","title":"Script Output","text":"<p>When executed, this script displays the Temperature-Salinity diagram for the selected data.</p> <p>This is an example of what this diagram could look like:</p> <p></p> <p>Source code: </p>"},{"location":"scripts/plot_var_boxplot/","title":"Variable Boxplots","text":"<p><code>make run-plot-var-boxplot</code></p>"},{"location":"scripts/plot_var_boxplot/#summary","title":"Summary","text":"<p>This scripts reads data from a folder and show the box plots of a given variable for different given water masses.</p>"},{"location":"scripts/plot_var_boxplot/#configuration","title":"Configuration","text":"<p>The configuration file for this script is <code>config/plot_var_boxplot.toml</code> (based on <code>config/default_plot_var_boxplot.toml</code>). All the parameters and their functionality are listed below:</p>"},{"location":"scripts/plot_var_boxplot/#inputoutput","title":"Input/Output","text":"LOADING_DIR <p>Directory from which to load data.</p> <p>default: <code>\"bgc_data\"</code></p> <p>Expected type: <code>str</code></p> SHOW <p>Whether to show the figure or not.</p> <p>default: <code>true</code></p> <p>Expected type: <code>bool</code></p> SAVE <p>Whether to save the figure or not.</p> <p>default: <code>false</code></p> <p>Expected type: <code>bool</code></p> SAVING_DIR <p>Directory in which to save the figure.</p> <p>default: <code>\"bgc_figs\"</code></p> <p>Expected type: <code>str</code></p>"},{"location":"scripts/plot_var_boxplot/#data-selection","title":"Data Selection","text":"PLOT_VARIABLE <p>Variable to plot the box-whisker figure of.</p> <p>default: <code>\"NTRA\"</code></p> <p>Expected type: <code>str</code></p> DATE_MIN <p>First date to map (included).</p> <p>default: <code>\"20070101\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code> format)</p> DATE_MAX <p>Last date to map (included).</p> <p>default: <code>\"20121231\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code> format)</p> LATITUDE_MIN <p>Minimum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>50</code></p> <p>Expected type: <code>int or float</code></p> LATITUDE_MAX <p>Maximum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>90</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MIN <p>Minimum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>-180</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MAX <p>Maximum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>180</code></p> <p>Expected type: <code>int or float</code></p> WATER_MASS_ACRONYMS <p>List of the acronyms of the water masses to load. The acronyms are the ones defined in <code>config/water_masses.toml</code>, in the <code>ACRONYM</code> field.</p> <p>default: <code>[\"AW\", \"PSWw\"]</code></p> <p>Expected type: <code>list[str]</code></p> EXPOCODES_TO_LOAD <p>Precise expocode to load alone. If empty, no discrimination on expocode will be conducted.</p> <p>default: <code>[]</code></p> <p>Expected type: <code>list[str]</code></p> PRIORITY <p>Providers priority list to use when removing duplicates.</p> <p>default: <code>[\"GLODAP_2022\", \"CMEMS\", \"ARGO\", \"NMDC\", \"CLIVAR\", \"IMR\", \"ICES\"]</code></p> <p>Expected type: <code>list[str]</code></p>"},{"location":"scripts/plot_var_boxplot/#plotting-options","title":"Plotting Options","text":"BOXPLOT_PERIOD <p>Period to use for each boxplot. Can be one of <code>year</code>, <code>month</code>, <code>week</code> or <code>day</code>.</p> <p>default: <code>\"year\"</code></p> <p>Expected type: <code>str</code></p>"},{"location":"scripts/plot_var_boxplot/#others","title":"Others","text":"VERBOSE <p>Verbose value, the higher, the more informations. If set to 0 or below: no information displayed. If set to 1: minimal informations displayed. If set to 2: very complete informations displayed. If set to 3 or higher: exhaustive informations displayed.</p> <p>default: <code>2</code></p> <p>Expected type: <code>int</code></p>"},{"location":"scripts/plot_var_boxplot/#script-output","title":"Script output","text":"<p>When executed, this script displays Box plots for a given variable, on multiple given water masses.</p> <p>This is an example of what this plot could look like:</p> <p></p> <p>Source code: </p>"},{"location":"scripts/plot_var_histogram/","title":"Variable Histograms","text":"<p><code>make run-plot-var-histogram</code></p>"},{"location":"scripts/plot_var_histogram/#summary","title":"Summary","text":"<p>This scripts reads data from a folder and show the histograms of a given variable for different given water masses.</p>"},{"location":"scripts/plot_var_histogram/#configuration","title":"Configuration","text":"<p>The configuration file for this script is <code>config/plot_var_histogram.toml</code> (based on <code>config/default_plot_var_histogram.toml</code>). All the parameters and their functionality are listed below:</p>"},{"location":"scripts/plot_var_histogram/#inputoutput","title":"Input/Output","text":"LOADING_DIR <p>Directory from which to load data.</p> <p>default: <code>\"bgc_data\"</code></p> <p>Expected type: <code>str</code></p> SHOW <p>Whether to show the figure or not.</p> <p>default: <code>true</code></p> <p>Expected type: <code>bool</code></p> SAVE <p>Whether to save the figure or not.</p> <p>default: <code>false</code></p> <p>Expected type: <code>bool</code></p> SAVING_DIR <p>Directory in which to save the figure.</p> <p>default: <code>\"bgc_figs\"</code></p> <p>Expected type: <code>str</code></p>"},{"location":"scripts/plot_var_histogram/#data-selection","title":"Data Selection","text":"PLOT_VARIABLE <p>Name of the variable to plot the value of. The names are supposed to be the ones defined in <code>config/variables.toml</code> (<code>config/default/variables.toml</code>) by default.)</p> <p>default: <code>\"TEMP\"</code></p> <p>Expected type: <code>str</code></p> DATE_MIN <p>First date to map (included).</p> <p>default: <code>\"20070101\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code>format)</p> DATE_MAX <p>Last date to map (included).</p> <p>default: <code>\"20121231\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code>format)</p> LATITUDE_MIN <p>Minimum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>50</code></p> <p>Expected type: <code>int or float</code></p> LATITUDE_MAX <p>Maximum latitude boundary to consider for the loaded data (included).</p> <p>default: <code>90</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MIN <p>Minimum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>-180</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MAX <p>Maximum longitude boundary to consider for the loaded data (included).</p> <p>default: <code>180</code></p> <p>Expected type: <code>int or float</code></p> WATER_MASS_ACRONYMS <p>List of the acronyms of the water masses to load. The acronyms are the ones defined in <code>config/water_masses.toml</code>, in the <code>ACRONYM</code> field.</p> <p>default: <code>[\"AW\", \"PSWw\"]</code></p> <p>Expected type: <code>list[str]</code></p> EXPOCODES_TO_LOAD <p>Precise expocode to load alone. If empty, no discrimination on expocode will be conducted.</p> <p>default: <code>[]</code></p> <p>Expected type: <code>list[str]</code></p> PRIORITY <p>Providers priority list to use when removing duplicates.</p> <p>default: <code>[\"GLODAP_2022\", \"CMEMS\", \"ARGO\", \"NMDC\", \"CLIVAR\", \"IMR\", \"ICES\"]</code></p> <p>Expected type: <code>list[str]</code></p>"},{"location":"scripts/plot_var_histogram/#others","title":"Others","text":"VERBOSE <p>Verbose value, the higher, the more informations. If set to 0 or below: no information displayed. If set to 1: minimal informations displayed. If set to 2: very complete informations displayed. If set to 3 or higher: exhaustive informations displayed.</p> <p>default: <code>2</code></p> <p>Expected type: <code>int</code></p>"},{"location":"scripts/plot_var_histogram/#script-output","title":"Script Output","text":"<p>When executed, this script displays histograms for a given variable, on multiple given water masses.</p> <p>This is an example of what this plot could look like:</p> <p></p> <p>Source code: </p>"},{"location":"scripts/plot_var_pressure/","title":"Variable vs Pressure Plot","text":"<p><code>make run-plot-var-pressure</code></p>"},{"location":"scripts/plot_var_pressure/#summary","title":"Summary","text":"<p>This scripts reads data from a folder and show a given variable value against pressure value, for different water masses.</p>"},{"location":"scripts/plot_var_pressure/#configuration","title":"Configuration","text":"<p>The configuration file for this script is <code>config/plot_var_pressure.toml</code> (based on <code>config/default_plot_var_pressure.toml</code>). All the parameters and their functionality are listed below:</p>"},{"location":"scripts/plot_var_pressure/#inputoutput","title":"Input/Output","text":"LOADING_DIR <p>Directory from which to load data.</p> <p>default: <code>\"bgc_data\"</code></p> <p>Expected type: <code>str</code></p> SHOW <p>Whether to show the figure or not.</p> <p>default: <code>true</code></p> <p>Expected type: <code>bool</code></p> SAVE <p>Whether to save the figure or not.</p> <p>default: <code>false</code></p> <p>Expected type: <code>bool</code></p> SAVING_DIR <p>Directory in which to save the figure.</p> <p>default: <code>\"bgc_figs\"</code></p> <p>Expected type: <code>str</code></p>"},{"location":"scripts/plot_var_pressure/#data-selection","title":"Data Selection","text":"PLOT_VARIABLE <p>Name of the variable to plot the value of. The names are supposed to be the ones defined in <code>config/variables.toml</code> (<code>config/default/variables.toml</code>) by default.)</p> <p>default: <code>\"NTRA\"</code></p> <p>Expected type: <code>str</code></p> DATE_MIN <p>First date to map (included).</p> <p>default: <code>\"20070101\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code>format)</p> DATE_MAX <p>Last date to map (included).</p> <p>default: <code>\"20121231\"</code></p> <p>Expected type: <code>str</code> (must match the <code>YYYYMMDD</code>format)</p> LATITUDE_MIN <p>Minimum latitude boundary for the loaded data (included).</p> <p>default: <code>50</code></p> <p>Expected type: <code>int or float</code></p> LATITUDE_MAX <p>Maximum latitude boundary for the loaded data (included).</p> <p>default: <code>90</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MIN <p>Minimum longitude boundary for the loaded data (included).</p> <p>default: <code>-180</code></p> <p>Expected type: <code>int or float</code></p> LONGITUDE_MAX <p>Maximum longitude boundary for the loaded data (included).</p> <p>default: <code>180</code></p> <p>Expected type: <code>int or float</code></p> WATER_MASS_ACRONYMS <p>List of the acronyms of the water masses to load. The acronyms are the ones defined in <code>config/water_masses.toml</code>, in the <code>ACRONYM</code> field.</p> <p>default: <code>[\"ALL\", \"AW\", \"MAW\"]</code></p> <p>Expected type: <code>list[str]</code></p> EXPOCODES_TO_LOAD <p>Precise expocode to load alone. If empty, no discrimination on expocode will be conducted.</p> <p>default: <code>[]</code></p> <p>Expected type: <code>list[str]</code></p> PRIORITY <p>Providers priority list to use when removing duplicates.</p> <p>default: <code>[\"GLODAP_2022\", \"CMEMS\", \"ARGO\", \"NMDC\", \"CLIVAR\", \"IMR\", \"ICES\"]</code></p> <p>Expected type: <code>list[str]</code></p>"},{"location":"scripts/plot_var_pressure/#others","title":"Others","text":"VERBOSE <p>Verbose value, the higher, the more informations. If set to 0 or below: no information displayed. If set to 1: minimal informations displayed. If set to 2: very complete informations displayed. If set to 3 or higher: exhaustive informations displayed.</p> <p>default: <code>2</code></p> <p>Expected type: <code>int</code></p>"},{"location":"scripts/plot_var_pressure/#script-output","title":"Script Output","text":"<p>When executed, this script display a value vs pressure view of a given variable, for different given water masses.</p> <p>This is an example of what this plot could look like:</p> <p></p> <p>Source code: </p>"},{"location":"scripts/save_data/","title":"Data Saving","text":"<p><code>make run-save-data</code></p>"},{"location":"scripts/save_data/#summary","title":"Summary","text":"<p>This scripts loads data from differents providers, aggregate the data to a standardized format and save the data in <code>.txt</code> files.</p>"},{"location":"scripts/save_data/#configuration","title":"Configuration","text":"<p>The configuration file for this script is <code>config/save_data.toml</code> (based on <code>config/default_save_data.toml</code>). All the parameters and their functionality are listed below:</p>"},{"location":"scripts/save_data/#inputoutput","title":"Input/Output","text":"SAVING_DIR <p>Directory in which to save the dataframes.</p> <p>default: <code>\"bgc_data\"</code></p> <p>Expected type: <code>str</code></p>"},{"location":"scripts/save_data/#data-selection","title":"Data Selection","text":"PROVIDERS <p>List of providers to use data from.</p> <p>default: <code>[\"GLODAP_2022\", \"CMEMS\", \"ARGO\", \"NMDC\", \"CLIVAR\", \"IMR\", \"ICES\"]</code></p> <p>Expected type: <code>list[str]</code></p> VARIABLES <p>Variables to include in the output file. The name or the variables are the ones defined in <code>config/variables.toml</code>, in the <code>NAME</code> field.</p> <p>default: <code>[\"PROVIDER\", \"EXPOCODE\", \"YEAR\", \"MONTH\", \"DAY\", \"HOUR\", \"LONGITUDE\",</code>\"LATITUDE\", \"DEPH\", \"TEMP\", \"PSAL\", \"DOXY\", \"PHOS\", \"NTRA\", \"SLCA\", \"CPHL\"]</p> <p>Expected type: <code>list[str]</code></p> DATE_MIN <p>Beginning of the data to load (included).</p> <p>default: <code>\"20070101\"</code></p> <p>Expected type: <code>str</code> (respecting the <code>YYYYMMDD</code> format)</p> DATE_MAX <p>End of the data to load (included).</p> <p>default: <code>\"20071231\"</code></p> <p>Expected type: <code>str</code> (respecting the <code>YYYYMMDD</code> format)</p> INTERVAL <p>Horizontal resolution of the plot. If set to 'day': will group datapoint by day. If set to 'week': will group datapoints by their week number. If set to 'month': will group datapoints by month. If set to 'year': will grou datapoints by year. If set to 'custom': will group datapoints based on a custom interval.</p> <p>default: <code>\"month\"</code></p> <p>Expected type: <code>str</code></p> CUSTOM_INTERVAL <p>If parameter <code>INTERVAL</code> is set to <code>custom</code>, length of the custom interval (in days).</p> <p>default: <code>8</code></p> <p>Expected type: <code>int</code></p> LATITUDE_MIN <p>Minimum latitude boundary for the loaded data (included).</p> <p>default: <code>50</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LATITUDE_MAX <p>Maximum latitude boundary for the loaded data (included).</p> <p>default: <code>90</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LONGITUDE_MIN <p>Minimum longitude boundary for the loaded data (included).</p> <p>default: <code>-180</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> LONGITUDE_MAX <p>Maximum longitude boundary for the loaded data (included).</p> <p>default: <code>180</code></p> <p>Expected type: <code>int</code>or <code>float</code></p> DEPTH_MIN <p>Minimum depth boundary for the loaded data (included).</p> <p>default: <code>nan</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> DEPTH_MAX <p>Maximum depth boundary for the loaded data (included).</p> <p>default: <code>0</code></p> <p>Expected type: <code>int</code> or <code>float</code></p> EXPOCODES_TO_LOAD <p>Precise expocode to load alone. If empty, no discrimination on expocode will be conducted.</p> <p>default: <code>[]</code></p> <p>Expected type: <code>list[str]</code></p> PRIORITY <p>Providers priority list to use when removing duplicates.</p> <p>default: <code>[\"GLODAP_2022\", \"CMEMS\", \"ARGO\", \"NMDC\", \"CLIVAR\", \"IMR\", \"ICES\"]</code></p> <p>Expected type: <code>list[str]</code></p>"},{"location":"scripts/save_data/#others","title":"Others","text":"VERBOSE <p>Verbose value, the higher, the more informations. If set to 0 or below: no information displayed. If set to 1: minimal informations displayed. If set to 2: very complete informations displayed. If set to 3 or higher: exhaustive informations displayed.</p> <p>default: <code>2</code></p> <p>Expected type: <code>int</code></p>"},{"location":"scripts/save_data/#script-output","title":"Script Output","text":"<p>When executed, this script will create files in the <code>SAVING_DIR</code> folder with the data from all sources specified in <code>PROVIDERS</code> with a standardized format.</p> <p>This is an example of what this data could look like:</p> <p></p> <p>Source code: </p>"},{"location":"virtual_env/","title":"Virtual Environment","text":"<p>This project environment is build using Anaconda, while the package management is done using the Python tool Poetry. The only requirement is to have anaconda installed and to be able to run commands using the <code>conda</code> methods in a terminal. More informations on enabling the <code>conda</code> method in Visual Studio Code here.</p>"},{"location":"virtual_env/#conda","title":"Conda","text":"<p>The conda environment is defined by the environment.yml file. This file contains the bare minimum to build the environment :</p> <ul> <li>The Python version to use when creating the environment.</li> <li>The Poetry version to load when creating the environment.</li> <li>Any module/package which couldn't be installed using Poetry.</li> </ul>"},{"location":"virtual_env/#poetry","title":"Poetry","text":"<p>The dependencies versions are defined in the pyproject.toml file. This file contains informations on the packages/modules version to use when building the environment.</p>"},{"location":"virtual_env/#environment-setup","title":"Environment setup","text":"<ol> <li> <p>Create the virtual environment from the file <code>environment.yml</code>:</p> From a Bash terminalFrom a Bash script <pre><code>conda env create --file environment.yml --prefix ./.venv\n</code></pre> <pre><code>$CONDA_EXE env create --file environment.yml --prefix ./.venv\n</code></pre> $CONDA_EXE <p>Variable referring to the conda executable path. It should be already existing.</p> conda env create <p>https://docs.conda.io/projects/conda/en/latest/commands/create.html</p> </li> <li> <p>Activate the virtual environment located in <code>./.venv</code>: TODO : add line about where to find the environment's name</p> From a Bash terminalFrom a Bash script <pre><code>conda activate ./.venv\n</code></pre> <p><sup>Not needed. <li> <p>Build the environment:</p> From a Bash terminalFrom a Bash script <p>In the virtual environment: <pre><code>poetry install\n</code></pre></p> <pre><code>.venv/bin/poetry install\n</code></pre> poetry install <p>https://python-poetry.org/docs/cli/#install</p> Installing groups <p>In the dependencies file (pyproject.toml), dependencies are organized into groups, main (nameless), dev and docs. <code>poetry install</code> will install all dependencies, regardless of their group.</p> <p><code>poetry install --only docs</code> will only install the 'docs' group.</p> <p><code>poetry install --without dev,docs</code> will install without the docs and dev groups.</p> </li> <li> <p>Run the python script <code>hello_world.py</code>:</p> From a Bash terminalFrom a Bash script <p>In the virtual environment: <pre><code>python hello_world.py\n</code></pre></p> <pre><code>.venv/bin/python hello_world.py\n</code></pre> </li> <li> <p>Updating the environment:</p> From a Bash terminalFrom a Bash script <p>In the virtual environment: <pre><code>poetry update\n</code></pre></p> <pre><code>.venv/bin/poetry update\n</code></pre> poetry update <p>https://python-poetry.org/docs/cli/#update</p> </li> <li> <p>Deactivate the environment:</p> From a Bash terminalFrom a Bash script <p>In the virtual environment: <pre><code>conda deactivate\n</code></pre></p> <p><sup>Not needed."}]}